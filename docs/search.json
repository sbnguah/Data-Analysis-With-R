[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data analysis with R",
    "section": "",
    "text": "This book is dedicated to my family, Balqisu, Nyarko and Ekuba.\nTo all the young scientists who would benefit from this book and colleagues who, in every way, helped me write this, I say a big thank you. My motivation comes from the dire and urgent need for many, especially those in developing countries, to have and use freely available yet sophisticated statistical software for analysing clinical data. In regions where buying statistical software is often not affordable to most people, R comes as a great relief. There is a huge gap between theoretical and practical knowledge of statistical applications by many scientists. R, an open-source statistical software, offers a unique but vital opportunity to bridge this gap.\nThis book introduces data analysis and R to persons with little knowledge of both. The step-by-step introduction to data analysis and R is deliberately organised with limited text but with many practical examples.\nReaders will be briefly introduced to R and RStudio. Required packages will be used in chapters that require them. However, before using a function from any package it will be explained.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data analysis with R</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 The statistical data analyst\nStatistical data analysis is more than just using computer software to generate results. It involves a basic understanding of the data type and the best way to analyse and present such data to make meaning to the general population. Thus, the data analyst:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#the-statistical-data-analyst",
    "href": "introduction.html#the-statistical-data-analyst",
    "title": "2  Introduction",
    "section": "",
    "text": "Must understand the genesis (study methodology) involved in obtaining the data in the first place. The conclusion from the same data may differ depending on the study methodology used and the hypothesis being tested. It is very prudent, therefore, that the statistical data analyst be involved in the data collection process right from the beginning.\nShould be able to point out errors in the data collection process in the early stages. This avoids wasting valuable resources on data that may not answer the question.\nProvide valuable advice on the best method of analysing the data at hand.\nPerform the analysis scientifically and soundly by applying the most current and statistically appropriate principles.\nPresent the result of the analysis in a manner that makes it easy for all persons without statistical and analytical expertise to understand with the least effort. This requires the statistical data analyst to be in a position to explain the analysis in a common language.\nFinally, the data analyst must know his limit. There are often instances where the analyst should seek ”professional” help, even though he may feel he is on the right path. It never hurts to seek a second opinion from your peers. It, therefore, goes without saying from the prior discussion that the data analyst must have a firm understanding of statistical and research methodology.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#statistical-software",
    "href": "introduction.html#statistical-software",
    "title": "2  Introduction",
    "section": "2.2 Statistical software",
    "text": "2.2 Statistical software\nSome years back, statistical analysis was one of the most tedious processes done mainly by dedicated statisticians. With the advent of computers and statistical software, it has become rather handy, with many advantages but some disadvantages. The main advantages are: 1. The tremendous speed with which large data is processed and results obtained. 1. The accuracy of the statistical calculations performed. Computers do not make mistakes but one has to beware of rounding in some software. Some software can perform calculations to a specific number of decimal places. Therefore, when one is confronted with a figure such as 1.00377655432, the software may work with 1.0037765, leaving out the last four digits. Calculations using this truncated value are likely to have a different result from the non-truncated figure, thus affecting the accuracy of the final result. 1. Many modern statistical software can read data from varied sources and formats. This makes it easy to transfer data from one software to another without having to re-enter the data collected into the second computer or software. This transferability has enabled the use of other digital equipment such as smartphones, personal digital assistants and tablets for data collection. Data collected in this manner is said to be ready for cleaning and analysis, bypassing the data entry stage. 1. Plotting graphs is one of the most important uses of modern-day computerised data analysis. Statistical software tends to make this rather tedious process almost hassle-free and accords us the ability to redo the plot from scratch at the click of a button.\nDespite all these advantages, many disadvantages are also inherent in the use of computers and statistical software. Some include:\n\nMany people with very little or no statistical knowledge can manipulate data and come up with conclusions that often tend to be very spurious. The cliche ”Garbage In Garbage Out” could not apply better than in this situation.\nMany commonly used software tend to be very reliable and accurate. With a large number of often user-written statistical software freely available online, one needs to be cautious of the output generated. Some of these could be wrongly written codes or have errors, thus producing faulty results.\nUnfortunately, the most used, reliable and accurate statistical software tends to be expensive as well. This notwithstanding, there are few, such as R, that combine free and open source with versatility and reliability. This forms the basis for my choice of R for this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#obtaining-and-installing-r",
    "href": "introduction.html#obtaining-and-installing-r",
    "title": "2  Introduction",
    "section": "2.3 Obtaining and installing R",
    "text": "2.3 Obtaining and installing R\nR is a free software programming language and environment for data manipulation, calculation and graphical display. It can run on Windows, MacOS X and Unix systems. It has great applications in many academic fields, including mathematics, economics and epidemiology. This capability has been enhanced by the many packages written by individuals over the years. R has the great advantage of being able to handle many datasets simultaneously. However, this functionality comes at a cost, which will be discussed in the subsequent chapters. R also has great graphics functionality but requires practice.\nSeveral advanced statistical and mathematical functions, such as regression and survival analysis, are also implemented in R. R and its many packages are obtainable free from http://cran.r-project.org/. The most current version at the time of writing this book is R-4.2.1. The Windows operating system version can be installed on both 32 and 64-bit operating systems. Download the base file from https://cran.r-project.org/bin/windows/base/, save it on your computer and install it, preferably as an administrator, by following the on-screen instructions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#obtaining-and-installing-rstudio",
    "href": "introduction.html#obtaining-and-installing-rstudio",
    "title": "2  Introduction",
    "section": "2.4 Obtaining and installing RStudio",
    "text": "2.4 Obtaining and installing RStudio\nRStudio is an Integrated Development Environment for R and other software such as Python. It provides an interface that adds more functionality and automation to R. It is downloadable from https://posit.co/download/rstudio-desktop/. It has two forms, a desktop version and the Server version used within a web browser. For simplicity and easy following of the processes in this book, it will be preferable to download and install RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data-importation.html",
    "href": "data-importation.html",
    "title": "3  Data Importation",
    "section": "",
    "text": "3.1 Using data in R packages\nMany packages in R come with data that can be used for practice. To be able to use a dataset in a specific package, that package first has to be installed. For instance, to be able to use the data Oswego native to the package epiDisplay, we first ensure the package is installed. The next step will be to make the data available in the R session as below\nCode\ndata(\"Oswego\", package = \"epiDisplay\")\nNow that the data is available in the working environment, we can visualize the first 4 rows of the fist three variables below\nCode\nOswego %&gt;% \n    select(age, sex, timesupper, ill) %&gt;% \n    head()\n\n\n\n\nagesextimesupperill\n\n11M       FALSE\n\n52F2e+03       TRUE\n\n65M1.83e+03TRUE\n\n59F1.83e+03TRUE\n\n13F       FALSE\n\n63F1.93e+03TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#direct-entry-into-r",
    "href": "data-importation.html#direct-entry-into-r",
    "title": "3  Data Importation",
    "section": "3.2 Direct entry into R",
    "text": "3.2 Direct entry into R\nWe first use the data.frame() function from the base package to create the data frame.\n\n\nCode\ndata.frame(\n    name = c(\"Ama\", \"Yakubu\", \"John\"), \n    sex = c(\"Female\", \"Male\", \"Male\"),\n    age = c(12, 9, 4),\n    school = c(\"JHS\", \"Primary\", \"Creche\")\n    )\n\n\n\n\nnamesexageschool\n\nAmaFemale12JHS\n\nYakubuMale9Primary\n\nJohnMale4Creche\n\n\n\n\nBelow we first describe how to manually enter data into R. We aim to create a tibble by using the tibble function.\n\n\nCode\ntibble(\n    name = c(\"Ama\", \"Yakubu\", \"John\"), \n    sex = c(\"Female\", \"Male\", \"Male\"),\n    age = c(12, 9, 4),\n    school = c(\"JHS\", \"Primary\", \"Creche\")\n    )\n\n\n\n\nnamesexageschool\n\nAmaFemale12JHS\n\nYakubuMale9Primary\n\nJohnMale4Creche",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#r-data-file",
    "href": "data-importation.html#r-data-file",
    "title": "3  Data Importation",
    "section": "3.3 R data file",
    "text": "3.3 R data file\nWhen working in R, a frequent mode of storage of data is as an .Rdata file. This preserves the structure and environment of the data. Below we will read an already saved .Rdata file.\n\n\nCode\nload(file = \"C:/Dataset/data1.Rdata\")\nls()\n\n\n[1] \"data1_stata\" \"mytheme\"     \"Oswego\"     \n\n\nWe then visualise the first 4 rows of the single data within the loaded file called data1_stata\n\n\nCode\ndata1_stata %&gt;% head(n=4)\n\n\n\n\nidsexweightheight\n\n125Male764\n\n62Female1196\n\n112Female13115\n\n29Female20106",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#text-files",
    "href": "data-importation.html#text-files",
    "title": "3  Data Importation",
    "section": "3.4 Text files",
    "text": "3.4 Text files\nThe first file format that we are going to read from is a flat file or text file. These usually have the extension .txt. The data in these files could be separated by various delimiters. These include tabs, commas, spaces, etc. In this section, we will read in one with a tab delimiter as a prototype as the rest will be similar.\n\n\nCode\nread_delim(\n    file = \"C:/Dataset/bpA.txt\", \n    delim = \"\\t\", \n    col_types = c(\"c\", \"c\", \"d\", \"d\")\n    )\n\n\n\n\nidsexagesbp0\n\nB01M73145\n\nB02F47164\n\nB03M59153\n\n\n\n\nThe last file to be read in this subsection is a comma-delimited text file\n\n\nCode\nread_delim(\n    file = \"C:/Dataset/blood.txt\", \n    delim = \",\", \n    col_types = c(\"c\", \"d\", \"d\", \"d\", \"d\", \"d\", \"d\")\n    ) %&gt;% \n    head(n=4)\n\n\n\n\nstnoagewgthgthbwbchct\n\n100112018.41287.713.722.6\n\n10029624  1238  8.522.8\n\n100316838.51438.226.824.1\n\n10049620.41148.310.523.3\n\n\n\n\nComma-delimited files with extension .csv can also be imported with the commnands\n\n\nCode\nread_csv(\n    file = \"C:/Dataset/blood.txt\",\n    col_types = c(\"c\", \"d\", \"d\", \"d\", \"d\", \"d\", \"d\")) %&gt;% \n    head(n=4)\n\n\n\n\nstnoagewgthgthbwbchct\n\n100112018.41287.713.722.6\n\n10029624  1238  8.522.8\n\n100316838.51438.226.824.1\n\n10049620.41148.310.523.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#microsoft-excel",
    "href": "data-importation.html#microsoft-excel",
    "title": "3  Data Importation",
    "section": "3.5 Microsoft Excel",
    "text": "3.5 Microsoft Excel\nProbably the most common format for transferring data is Microsoft Excel. There are two versions of Excel with extensions .xls and .xlsx. Below reading in the .xlsx is demonstrated using the readxl package.\n\n\nCode\nreadxl::read_xlsx(path = \"C:/Dataset/data1.xlsx\") %&gt;% \n    head(n=4)\n\n\n\n\nidsexweightheight\n\n125Male764\n\n62Female1196\n\n112Female13115\n\n29Female20106",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#spss-files",
    "href": "data-importation.html#spss-files",
    "title": "3  Data Importation",
    "section": "3.6 SPSS files",
    "text": "3.6 SPSS files\nFiles from SPSS are usually saved with the extension of .sav. Below we read an SSPSS data file using the haven package\n\n\nCode\nhaven::read_sav(file = \"C:/Dataset/data1.sav\") %&gt;% \n    head(n=4)\n\n\n\n\nidsexweightheight\n\n125Male764\n\n62Female1196\n\n112Female13115\n\n29Female20106",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#stata-files",
    "href": "data-importation.html#stata-files",
    "title": "3  Data Importation",
    "section": "3.7 Stata files",
    "text": "3.7 Stata files\nStata files, similar to SPSS data files can be imported using the haven package. This is illustrated below\n\n\nCode\nhaven::read_dta(file = \"C:/Dataset/data1.dta\") %&gt;% \n    head(n=4)\n\n\n\n\nidsexweightheight\n\n125Male764\n\n62Female1196\n\n112Female13115\n\n29Female20106",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#sas-files",
    "href": "data-importation.html#sas-files",
    "title": "3  Data Importation",
    "section": "3.8 SAS files",
    "text": "3.8 SAS files\nThe haven package also offers the ability to read into R a SAS data file. This is illustrated below\n\n\nCode\nhaven::read_sas(data_file = \"C:/Dataset/data1.sas7bdat\") %&gt;% \n    head(n=4)\n\n\n\n\nIDSEXWEIGHTHEIGHT\n\n125Male764\n\n62Female1196\n\n112Female13115\n\n29Female20106",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-importation.html#conclusion",
    "href": "data-importation.html#conclusion",
    "title": "3  Data Importation",
    "section": "3.9 Conclusion",
    "text": "3.9 Conclusion\nIn this section, we have learned how to import data into R from various file formats and programs. In the next section, we will learn about how to export data in R to other file formats.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Importation</span>"
    ]
  },
  {
    "objectID": "data-types.html",
    "href": "data-types.html",
    "title": "4  Data Types",
    "section": "",
    "text": "4.1 Qualitative data\nIn this data type, the observations fall into distinctive categories. There is usually no scale applicable to qualitative data type. These are further divided into:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "data-types.html#qualitative-data",
    "href": "data-types.html#qualitative-data",
    "title": "4  Data Types",
    "section": "",
    "text": "4.1.1 Nominal\nThese are qualitative data types that have no order. The colors of a flag for instance can be ”red” ”yellow” and ”green”. None of these can be said to be coming after the other. Contrast this to the one immediately below.\n\n\n4.1.2 Binary\nA special type of nominal data type is binary data. It is widespread in statistical analysis. These are observations that can take only two values. A question that records the presence of a disease will only have a ”Yes” or ”No” answer. Sex is usually recorded as ”Male” or ”Female”.\n\n\n4.1.3 Ordinal\nAn ordinal qualitative data type has an order to it. A commonly used one is the socioeconomic status, often categorised as ”Low”, ”Middle” and ”High”. Although we cannot say that the interval between ”Low” and ”Middle” is the same as ”Middle” and ”High”, we know ”Low” is lower than ”Middle” which in turn is also lower than ”High”. The Likert scale, a well-known scale in many social science research is also an example of an ordinal scale. Ordinal variables are often created from quantitative (see below)variables. E.g. the ages of a group of men can be converted into age groups of any desired number of categories.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "data-types.html#quantitative-data",
    "href": "data-types.html#quantitative-data",
    "title": "4  Data Types",
    "section": "4.2 Quantitative data",
    "text": "4.2 Quantitative data\nQuantitative or numerical data are observations such as numbers that can be measured. There are two types:\n\n4.2.1 Discrete\nDiscrete quantitative data is one that only specific values can be obtained. The number of persons attending a theatre can only be a whole number. So is the number of votes obtained in an election. Although discrete quantitative variables are often analysed as continuous ones they can occasionally pose problems when analysed as such. We will be dealing with some of these in the subsequent chapters.\n\n\n4.2.2 Continuous\nContinuous quantitative variables on the other hand can be measured to any precision, thereby making the figures they present to be as precise as the experimenter desires. For instance, the distance between two towns can be measured in kilometres to as much precision as possible. Theoretically, this can be as 12.0kms to as much as 12.0234278kms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "data-types.html#other-specific-data-types",
    "href": "data-types.html#other-specific-data-types",
    "title": "4  Data Types",
    "section": "4.3 Other specific data types",
    "text": "4.3 Other specific data types\nThere are other specific subtypes of data encountered in statistical analysis. Some of these are:\n\n4.3.1 Ratios\nRatios are special continuous variables that are generated from two other variables. For example, the ratio of boys to girls in a sample can be determined by dividing the number of boys by that of girls. The figures obtained are similar to continuous variables but will require special techniques in analysis.\n\n\n4.3.2 Rates\nRates are population parameters often used in medicine and epidemiology. Examples include the population growth rate and mortality rate. It is also a statistic or parameter generated from two others. The mortality rate is generated for instance from the number of deaths and time interval. In the case of neonatal mortality rate, this is generated from the number of deaths and the number of live births in the same period. In analysis, this is often treated as indicated for the ratios above.\n\n\n4.3.3 Percentage\nPercentages are peculiar as they often have a definite maximum and a minimum of 0% and 100% respectively. However, percentage changes can take up any value. For instance, a change from 4 to 3 will yield a negative (-25%) change. To avoid the tedious nature of these percentages it is advisable to often retain and work with the specific values involved in determining the percentage rather than the percentages derived.\n\n\n4.3.4 Ranks\nRanks are often treated as continuous variables though they are not. A well-known example is the position of a student in a class test. The position is relative to others and differs from the actual mark scored. The ranks may give the impression of an equal space between consecutive ranks but the actual difference may be much smaller or bigger than the rank difference.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html",
    "href": "data-wrangling.html",
    "title": "5  Data Wrangling",
    "section": "",
    "text": "5.1 Renaming variables\nBelow we rename the variables hb to hemog and id to studyid using he rename function, and then show the first 5 records with the head function.\nCode\ndf_blood %&gt;% \n    rename(hemog = hb, studyid = id) %&gt;% \n    head(5)\n\n\n\n\nstudyidhemoghctsexbldgrppdonor\n\n110.531.8MaleO3\n\n211.937.2MaleAB0\n\n31  26  MaleA1\n\n48.926.8MaleA3\n\n57.824.2MaleA2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#sorting-data",
    "href": "data-wrangling.html#sorting-data",
    "title": "5  Data Wrangling",
    "section": "5.2 Sorting data",
    "text": "5.2 Sorting data\nBelow we use the arrange function to sort the bldgrp in ascending order and hb by descending order.\n\n\nCode\ndf_blood %&gt;% \n    arrange(bldgrp, desc(hb)) %&gt;% \n    head(10)\n\n\n\n\nidhbhctsexbldgrppdonor\n\n179.830.5FemaleA4\n\n219.128  A3\n\n48.926.8MaleA3\n\n57.824.2MaleA2\n\n31  26  MaleA1\n\n916.4  MaleAB1\n\n1014.443.6MaleAB1\n\n1612.799  FemaleAB0\n\n2412.338.2AB2\n\n1412.236.8FemaleAB1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#subsetting-data",
    "href": "data-wrangling.html#subsetting-data",
    "title": "5  Data Wrangling",
    "section": "5.3 Subsetting data",
    "text": "5.3 Subsetting data\nIn this subsection, we demonstrate the use of the filter and select function to select specific records and variables in a tibble. Below we filter to select all records with hb &gt; 12g/dl and keep only the id, hb and sex columns.\n\n\nCode\ndf_blood %&gt;% \n    filter(hb &gt; 12) %&gt;% \n    select(id, hb, sex)\n\n\n\n\nidhbsex\n\n916.4Male\n\n1014.4Male\n\n1412.2Female\n\n1416.4Female\n\n1612.7Female\n\n2412.3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#generating-new-variables",
    "href": "data-wrangling.html#generating-new-variables",
    "title": "5  Data Wrangling",
    "section": "5.4 Generating new variables",
    "text": "5.4 Generating new variables\nTo generate new variables we use the mutate function. Based on our knowledge that the hematocrit is approximately three times the haemoglobin we generate a new variable, hb_from_hct.\n\n\nCode\ndf_blood %&gt;% \n    mutate(hb_from_hct = hct/3) %&gt;% \n    head(10)\n\n\n\n\nidhbhctsexbldgrppdonorhb_from_hct\n\n110.531.8MaleO310.6 \n\n211.937.2MaleAB012.4 \n\n31  26  MaleA18.67\n\n48.926.8MaleA38.93\n\n57.824.2MaleA28.07\n\n610  30.9MaleB110.3 \n\n710.433.9MaleB011.3 \n\n811.335  MaleO111.7 \n\n916.4  MaleAB1   \n\n1014.443.6MaleAB114.5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#aggregating-data",
    "href": "data-wrangling.html#aggregating-data",
    "title": "5  Data Wrangling",
    "section": "5.5 Aggregating data",
    "text": "5.5 Aggregating data\nData can be aggregated in R using the summarize function. Below we determine the mean and standard deviation of the haemoglobin for the patient in the data.\n\n\nCode\ndf_blood %&gt;% \n    summarize(mean_hb = mean(hb), sd_hb = sd(hb))\n\n\n\n\nmean_hbsd_hb\n\n112.89\n\n\n\n\nGrouping the data by the “bldgrp” before the aggregation yields the aggregated means and standard deviations for the various blood groups.\n\n\nCode\ndf_blood %&gt;% \n    group_by(bldgrp) %&gt;% \n    summarize(mean_hb = mean(hb), sd_hb = sd(hb))\n\n\n\n\nbldgrpmean_hbsd_hb\n\nA7.323.61 \n\nAB13.1 1.69 \n\nB10.2 0.283\n\nO11   0.427\n\nP16.4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#reshaping-data",
    "href": "data-wrangling.html#reshaping-data",
    "title": "5  Data Wrangling",
    "section": "5.6 Reshaping data",
    "text": "5.6 Reshaping data\nIn longitudinal studies, data is captured from the same individual repeatedly. Such data is recorded either in long or wide formats. A typical example of a data frame in the long form is bpB below.\n\n\nCode\nbp_long &lt;- read_csv(\n    file = \"C:/Dataset/bp_long.txt\",\n    col_names = TRUE, \n    col_types = c(\"c\", \"c\", \"i\"))\n\nbp_long\n\n\n\n\nidmeasuresbp\n\nB01sbp1141\n\nB01sbp2137\n\nB02sbp1155\n\nB02sbp2153\n\nB03sbp1153\n\n\n\n\nIn this format, each visit or round of data taking is captured as a new row, but with the appropriate study ID and period of record, captured as the variable measure above. Measurement of systolic blood pressure on day 1 is indicated by sbp1 in the measure variable. Day 2 measurements are indicated as sbp2.\nThe wide format of the same data can be obtained as below.\n\n\nCode\nbp_wide &lt;- \n    bp_long %&gt;% \n    pivot_wider(\n        id_cols = id, \n        names_from = measure, \n        values_from = sbp)\n\nbp_wide\n\n\n\n\nidsbp1sbp2\n\nB01141137\n\nB02155153\n\nB03153\n\n\n\n\nHere, each study participant’s record for the whole study is on one row of the data and the different measurements of systolic blood pressure are captured as different variables. Next, we convert the wide back to the long format.\n\n\nCode\nbp_wide %&gt;% \n    pivot_longer(\n        cols = c(sbp1, sbp2),\n        names_to = \"time\",\n        values_to = \"syst_bp\")\n\n\n\n\nidtimesyst_bp\n\nB01sbp1141\n\nB01sbp2137\n\nB02sbp1155\n\nB02sbp2153\n\nB03sbp1153\n\nB03sbp2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#combining-data",
    "href": "data-wrangling.html#combining-data",
    "title": "5  Data Wrangling",
    "section": "5.7 Combining data",
    "text": "5.7 Combining data\nIn a study to determine the change in weight of athletes running a marathon, data about the athletes were obtained by the investigators. Since the marathon starts in town A and ends in town B, the investigators decided to weigh the athletes just before starting the race. Here they took records of the ID of the athlete’s sid, sex, age and weight at the start (wgtst). The records of five of these athletes are in the data marathonA. At the end point of the marathon, another member of the investigation team recorded their IDs (eid), weight upon completion (wgtend) and the time it took the athletes to complete the marathon (dura).\n\n\nCode\ndataA &lt;- \n    read_delim(\n        file = \"C:/Dataset/marathonA.txt\",\n        col_names = TRUE,\n        delim = \"\\t\",\n        col_types = c(\"c\",\"c\",\"i\",\"d\"))\n\ndataB &lt;- \n    read_delim(\n        file = \"C:/Dataset/marathonB.txt\",\n        col_names = TRUE,\n        delim = \"\\t\",\n        col_types = c(\"c\",\"c\",\"i\",\"d\"))\n\ndataA\n\n\n\n\nsidsexagewgtst\n\nC001M2357.1\n\nC002F2762.3\n\nC003M1954.5\n\nC004M2159.4\n\nC005F3253.4\n\n\n\n\nCode\ndataB\n\n\n\n\neidwgtenddura\n\nC00353.9189\n\nC00553  197\n\nC00262.2201\n\nC00156.8209\n\n\n\n\nWe can determine the weight change only by matching the before and after weight of each individual. This is where merging is very useful. Below, we merge the two data into one. This is done below.\n\n\nCode\ndataA %&gt;% \n    full_join(dataB, by = join_by(sid == eid))\n\n\n\n\nsidsexagewgtstwgtenddura\n\nC001M2357.156.8209\n\nC002F2762.362.2201\n\nC003M1954.553.9189\n\nC004M2159.4  \n\nC005F3253.453  197",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#reading-in-data",
    "href": "data-wrangling.html#reading-in-data",
    "title": "5  Data Wrangling",
    "section": "5.8 Reading in data",
    "text": "5.8 Reading in data\n\n\nCode\ndataF &lt;-\n    readxl::read_xlsx(\"C:/Dataset/SBPDATA.xlsx\") %&gt;% \n    janitor::clean_names() %&gt;% \n    rename(\n        ageyrs = a3_how_old_are_you_years,\n        dxs_class = disease_class,\n        gender = a1_gender\n        ) %&gt;% \n    mutate(\n        dxs_class = factor(dxs_class),\n        gender = factor(\n            gender, \n            levels = c(0, 1), \n            labels = c(\"Male\", \"Female\")))\n\ndataF %&gt;% select(1:5) %&gt;% head()\n\n\n\n\nsiddxs_classsbp_0sbp_2sbp_4\n\n1HPT139124130\n\n2DM+HPT155\n\n3HPT109123109\n\n4HPT130\n\n5HPT124120146\n\n6DM+HPT140114163\n\n\n\n\nCode\ndat &lt;- \n    tribble(\n        ~\"name\", ~\"day\", ~\"month\", ~\"year\", ~\"bp\",\n        \"Ama\", 12, 05, 2020, \"120/80\",\n        \"Kwame\", 14, 02, 2019, \"132/66\",\n        \"Akosua\", 21, 12, 2010, \"110/76\",\n        \"Yaw\", 13, 03, 1982, \"144/98\",\n        \"Yaa\", 19, 08, 2000, \"117/77\")\n\ndat\n\n\n\n\nnamedaymonthyearbp\n\nAma1252.02e+03120/80\n\nKwame1422.02e+03132/66\n\nAkosua21122.01e+03110/76\n\nYaw1331.98e+03144/98\n\nYaa1982e+03       117/77",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#arrange",
    "href": "data-wrangling.html#arrange",
    "title": "5  Data Wrangling",
    "section": "5.9 arrange",
    "text": "5.9 arrange\n\n\nCode\ndat %&gt;% arrange(name, desc(day))\n\n\n\n\nnamedaymonthyearbp\n\nAkosua21122.01e+03110/76\n\nAma1252.02e+03120/80\n\nKwame1422.02e+03132/66\n\nYaa1982e+03       117/77\n\nYaw1331.98e+03144/98",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#unite",
    "href": "data-wrangling.html#unite",
    "title": "5  Data Wrangling",
    "section": "5.10 unite",
    "text": "5.10 unite\n\n\nCode\ndat %&gt;% \n    unite(col = \"dob\", c(day, month, year), sep=\"/\") \n\n\n\n\nnamedobbp\n\nAma12/5/2020120/80\n\nKwame14/2/2019132/66\n\nAkosua21/12/2010110/76\n\nYaw13/3/1982144/98\n\nYaa19/8/2000117/77",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#seperate",
    "href": "data-wrangling.html#seperate",
    "title": "5  Data Wrangling",
    "section": "5.11 seperate",
    "text": "5.11 seperate\n\n\nCode\ndat %&gt;% \n    separate(col = bp, into = c(\"sbp\", \"dbp\"), sep = \"/\") \n\n\n\n\nnamedaymonthyearsbpdbp\n\nAma1252.02e+0312080\n\nKwame1422.02e+0313266\n\nAkosua21122.01e+0311076\n\nYaw1331.98e+0314498\n\nYaa1982e+03       11777\n\n\n\n\n\n\nCode\ndat %&gt;% \n    separate(col = bp, into = c(\"sbp\", \"dbp\"), sep = \"/\") %&gt;% \n    unite(col = \"dob\", c(day, month, year), sep=\"/\") %&gt;% \n    mutate(dob_new = lubridate::dmy(dob)) \n\n\n\n\nnamedobsbpdbpdob_new\n\nAma12/5/2020120802020-05-12\n\nKwame14/2/2019132662019-02-14\n\nAkosua21/12/2010110762010-12-21\n\nYaw13/3/1982144981982-03-13\n\nYaa19/8/2000117772000-08-19",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#relocate",
    "href": "data-wrangling.html#relocate",
    "title": "5  Data Wrangling",
    "section": "5.12 relocate",
    "text": "5.12 relocate\n\n\nCode\ndataF %&gt;% \n    relocate(ageyrs, gender, .before = sbp_0) %&gt;% \n    select(1:8) %&gt;% \n    slice_head(n=10) \n\n\n\n\nsiddxs_classageyrsgendersbp_0sbp_2sbp_4sbp_6\n\n1HPT75Male139124130130\n\n2DM+HPT60Male155\n\n3HPT62Male109123109126\n\n4HPT70Male130\n\n5HPT72Male124120146144\n\n6DM+HPT56Male140114163117\n\n7DM+HPT51Male137135132147\n\n8DM73Male160130\n\n9HPT61Female153218\n\n10HPT59Male135130118150\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    relocate(sid, .after = last_col()) %&gt;% \n    slice_head(n=10) \n\n\n\n\ndxs_classsbp_0sbp_2sbp_4sbp_6sbp_8sbp_10sbp_12sbp_14sbp_16sbp_18ageyrsgendersid\n\nHPT1391241301301041298012912613575Male1\n\nDM+HPT15560Male2\n\nHPT10912310912610811511512213110262Male3\n\nHPT13070Male4\n\nHPT12412014614415712312013112012372Male5\n\nDM+HPT14011416311712412112811910012756Male6\n\nDM+HPT13713513214713012414214412851Male7\n\nDM16013073Male8\n\nHPT15321821861Female9\n\nHPT13513011815012712314913259Male10\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    relocate(where(is.numeric)) %&gt;% \n    slice_head(n=10) \n\n\n\n\nsidsbp_0sbp_2sbp_4sbp_6sbp_8sbp_10sbp_12sbp_14sbp_16sbp_18ageyrsdxs_classgender\n\n11391241301301041298012912613575HPTMale\n\n215560DM+HPTMale\n\n310912310912610811511512213110262HPTMale\n\n413070HPTMale\n\n512412014614415712312013112012372HPTMale\n\n614011416311712412112811910012756DM+HPTMale\n\n713713513214713012414214412851DM+HPTMale\n\n816013073DMMale\n\n915321821861HPTFemale\n\n1013513011815012712314913259HPTMale\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    relocate(contains(\"sbp\")) %&gt;% \n    slice_head(n=10) \n\n\n\n\nsbp_0sbp_2sbp_4sbp_6sbp_8sbp_10sbp_12sbp_14sbp_16sbp_18siddxs_classageyrsgender\n\n139124130130104129801291261351HPT75Male\n\n1552DM+HPT60Male\n\n1091231091261081151151221311023HPT62Male\n\n1304HPT70Male\n\n1241201461441571231201311201235HPT72Male\n\n1401141631171241211281191001276DM+HPT56Male\n\n1371351321471301241421441287DM+HPT51Male\n\n1601308DM73Male\n\n1532182189HPT61Female\n\n13513011815012712314913210HPT59Male",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#summarize-across",
    "href": "data-wrangling.html#summarize-across",
    "title": "5  Data Wrangling",
    "section": "5.13 summarize & across",
    "text": "5.13 summarize & across\n\n\nCode\ndataF %&gt;% \n    summarize(across(sbp_0:sbp_8, .fns = mean, na.rm=T)) \n\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `across(sbp_0:sbp_8, .fns = mean, na.rm = T)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\nsbp_0sbp_2sbp_4sbp_6sbp_8\n\n141139138138137\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    na.omit() %&gt;% \n    group_by(dxs_class) %&gt;% \n    summarize(across(where(is.numeric), ~quantile(.x))) %&gt;% \n    ungroup() \n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'dxs_class'. You can override using the\n`.groups` argument.\n\n\n\n\ndxs_classsidsbp_0sbp_2sbp_4sbp_6sbp_8sbp_10sbp_12sbp_14sbp_16sbp_18ageyrs\n\nDM23       8170849487747185798627\n\nDM980       11411611311411211311311211211044\n\nDM1.65e+0312412512512612712412312212012150\n\nDM2.34e+0313413913814013813813613613013558\n\nDM3.28e+0318919419918718717518319618519775\n\nDM+HPT6       9881887082908387878325\n\nDM+HPT826       12912712512612712412512412412253\n\nDM+HPT1.57e+0314214114213914013914013814013760\n\nDM+HPT2.44e+0315815715815515715615315415215466\n\nDM+HPT3.28e+0321623123123427724023222022822689\n\nHPT1       9071788873878081798630\n\nHPT783       12612412012012012012012012012051\n\nHPT1.59e+0313813513213113113213013113013059\n\nHPT2.51e+0315115014714614614414414414514266\n\nHPT3.29e+0321922122020920221722821520121399",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#distinct-observations",
    "href": "data-wrangling.html#distinct-observations",
    "title": "5  Data Wrangling",
    "section": "5.14 Distinct observations",
    "text": "5.14 Distinct observations\n\n\nCode\ndataF %&gt;% \n    summarise(across(where(is.numeric), n_distinct)) \n\n\n\n\nsidsbp_0sbp_2sbp_4sbp_6sbp_8sbp_10sbp_12sbp_14sbp_16sbp_18ageyrs\n\n329613814114513813513413413313012777",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#list-of-functions",
    "href": "data-wrangling.html#list-of-functions",
    "title": "5  Data Wrangling",
    "section": "5.15 list of functions",
    "text": "5.15 list of functions\n\n\nCode\ndataF %&gt;% \n    filter(!is.na(dxs_class)&!is.na(gender)) %&gt;% \n    group_by(dxs_class, gender) %&gt;%\n    summarise(\n        across(\n            starts_with(\"sbp\"), \n            list(\n                AVG = mean, \n                SD = sd, \n                N_missing = ~sum(is.na(.x), na.rm=TRUE))\n            )\n        ) %&gt;%\n    ungroup() \n\n\n`summarise()` has grouped output by 'dxs_class'. You can override using the\n`.groups` argument.\n\n\n\n\ndxs_classgendersbp_0_AVGsbp_0_SDsbp_0_N_missingsbp_2_AVGsbp_2_SDsbp_2_N_missingsbp_4_AVGsbp_4_SDsbp_4_N_missingsbp_6_AVGsbp_6_SDsbp_6_N_missingsbp_8_AVGsbp_8_SDsbp_8_N_missingsbp_10_AVGsbp_10_SDsbp_10_N_missingsbp_12_AVGsbp_12_SDsbp_12_N_missingsbp_14_AVGsbp_14_SDsbp_14_N_missingsbp_16_AVGsbp_16_SDsbp_16_N_missingsbp_18_AVGsbp_18_SDsbp_18_N_missing\n\nDMMale  15770757479798594110\n\nDMFemale12819.80172329302932365055\n\nDM+HPTMale14523  084113119145153153186204247\n\nDM+HPTFemale14722  0263639454355576884\n\nHPTMale  3237309354401414473522639741\n\nHPTFemale14420.9074116131147145165173216241",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#summarizing-by-anonymous-functions",
    "href": "data-wrangling.html#summarizing-by-anonymous-functions",
    "title": "5  Data Wrangling",
    "section": "5.16 Summarizing by anonymous functions",
    "text": "5.16 Summarizing by anonymous functions\n\n\nCode\ndataF %&gt;% \n    filter(!is.na(dxs_class)) %&gt;% \n    group_by(dxs_class) %&gt;%\n    summarise(\n        across(\n            .cols = c(sbp_0, sbp_18), \n            .fns = list(\n                \"Mean\" = ~mean(.x, na.rm=T), \n                \"UpperCI\" = ~(mean(.x, na.rm=T) + \n                                  1.96*sd(.x, na.rm=T)/sqrt(n())) ,\n                \"LowerCI\" = ~(mean(.x, na.rm=T) - \n                                  1.96*sd(.x, na.rm=T)/sqrt(n()))))) %&gt;%\n    ungroup() \n\n\n\n\ndxs_classsbp_0_Meansbp_0_UpperCIsbp_0_LowerCIsbp_18_Meansbp_18_UpperCIsbp_18_LowerCI\n\nDM126128124124125122\n\nDM+HPT145147144139141138\n\nHPT142143141135135134",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#expand",
    "href": "data-wrangling.html#expand",
    "title": "5  Data Wrangling",
    "section": "5.17 expand",
    "text": "5.17 expand\n\n\nCode\ndataF %&gt;% \n    filter(!is.na(dxs_class) & !is.na(gender)) %&gt;% \n    expand(dxs_class, gender) \n\n\n\n\ndxs_classgender\n\nDMMale\n\nDMFemale\n\nDM+HPTMale\n\nDM+HPTFemale\n\nHPTMale\n\nHPTFemale",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#crossing",
    "href": "data-wrangling.html#crossing",
    "title": "5  Data Wrangling",
    "section": "5.18 crossing",
    "text": "5.18 crossing\n\n\nCode\ndataF %&gt;% \n    filter(!is.na(dxs_class) & !is.na(gender)) %&gt;% \n    select(dxs_class, gender) %&gt;% \n    crossing() \n\n\n\n\ndxs_classgender\n\nDMMale\n\nDMFemale\n\nDM+HPTMale\n\nDM+HPTFemale\n\nHPTMale\n\nHPTFemale",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#adding-a-running-id",
    "href": "data-wrangling.html#adding-a-running-id",
    "title": "5  Data Wrangling",
    "section": "5.19 Adding a running id",
    "text": "5.19 Adding a running id\n\n\nCode\ndataF %&gt;% \n    filter(!is.na(dxs_class) & !is.na(gender)) %&gt;% \n    select(dxs_class, gender) %&gt;%\n    mutate(running_id = row_number()) %&gt;% \n    slice_head(n=10) \n\n\n\n\ndxs_classgenderrunning_id\n\nHPTMale1\n\nDM+HPTMale2\n\nHPTMale3\n\nHPTMale4\n\nHPTMale5\n\nDM+HPTMale6\n\nDM+HPTMale7\n\nDMMale8\n\nHPTFemale9\n\nHPTMale10",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#pivot_longer-pivot_wider",
    "href": "data-wrangling.html#pivot_longer-pivot_wider",
    "title": "5  Data Wrangling",
    "section": "5.20 pivot_longer & pivot_wider",
    "text": "5.20 pivot_longer & pivot_wider\n\n\nCode\ndataF_long &lt;-\n    dataF %&gt;% \n    select(gender, dxs_class, sbp_0:sbp_18) %&gt;% \n    pivot_longer(\n        cols = starts_with(\"sbp\"),\n        names_to = \"measure\",\n        values_to = \"sbp\",\n        values_drop_na = TRUE)\n\ndataF_long %&gt;% \n    slice_head(n=10) \n\n\n\n\ngenderdxs_classmeasuresbp\n\nMaleHPTsbp_0139\n\nMaleHPTsbp_2124\n\nMaleHPTsbp_4130\n\nMaleHPTsbp_6130\n\nMaleHPTsbp_8104\n\nMaleHPTsbp_10129\n\nMaleHPTsbp_1280\n\nMaleHPTsbp_14129\n\nMaleHPTsbp_16126\n\nMaleHPTsbp_18135\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(dxs_class, gender, sbp_0, sbp_2, sbp_4) %&gt;%\n    na.omit() %&gt;% \n    group_by(dxs_class) %&gt;% \n    pivot_wider(\n        names_from = gender, \n        values_from = c(sbp_0, sbp_2, sbp_4), \n        values_fn = ~mean(.x, na.rm = TRUE)) %&gt;% \n    ungroup() %&gt;% \n    slice_head(n=10) \n\n\n\n\ndxs_classsbp_0_Malesbp_0_Femalesbp_2_Malesbp_2_Femalesbp_4_Malesbp_4_Female\n\nHPT141144139142136141\n\nDM+HPT145146144146143146\n\nDM126130127132125134",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#tidyquant-tabulation",
    "href": "data-wrangling.html#tidyquant-tabulation",
    "title": "5  Data Wrangling",
    "section": "5.21 tidyquant tabulation",
    "text": "5.21 tidyquant tabulation\n\n\nCode\ndataF %&gt;% \n    select(dxs_class, gender) %&gt;% \n    na.omit() %&gt;% \n    tidyquant::pivot_table(\n        .rows = gender, .columns = dxs_class, .values = ~n()\n    ) \n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n\ngenderDMDM+HPTHPT\n\nMale3087771434\n\nFemale114226430\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(dxs_class, gender, sbp_0, sbp_2) %&gt;% \n    na.omit() %&gt;% \n    tidyquant::pivot_table(.rows = gender, \n                           .columns = dxs_class, \n                           .values = ~quantile(sbp_0)) %&gt;% \n    unnest(cols = c(\"DM\",\"HPT\",\"DM+HPT\")) \n\n\nWarning: Values from `quantile(sbp_0)` are not uniquely identified; output will contain\nlist-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(gender, dxs_class)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\n\n\ngenderDMDM+HPTHPT\n\nMale909570\n\nMale113128126\n\nMale124142140\n\nMale134158154\n\nMale182228224\n\nFemale819898\n\nFemale117131129\n\nFemale127145141\n\nFemale138161159\n\nFemale208220210",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#rowwise-manipulations",
    "href": "data-wrangling.html#rowwise-manipulations",
    "title": "5  Data Wrangling",
    "section": "5.22 rowwise manipulations",
    "text": "5.22 rowwise manipulations\n\n\nCode\ndataF %&gt;% \n    rowwise() %&gt;% \n    mutate(\n        sbp_mean = mean(\n            c(sbp_0,sbp_2,sbp_4,sbp_6,sbp_8, sbp_10, sbp_12,\n                             sbp_14,sbp_16, sbp_18), na.rm=T),\n        sbp_sd = sd(\n            c(sbp_0,sbp_2,sbp_4,sbp_6,sbp_8, sbp_10, sbp_12,\n              sbp_14,sbp_16, sbp_18), na.rm=T),\n        n=n()) %&gt;% \n    ungroup() %&gt;% \n    relocate(\n        sid, dxs_class, ageyrs, gender, sbp_mean, sbp_sd, \n        contains(\"sbp\")) %&gt;% \n    slice_head(n=10) \n\n\n\n\nsiddxs_classageyrsgendersbp_meansbp_sdsbp_0sbp_2sbp_4sbp_6sbp_8sbp_10sbp_12sbp_14sbp_16sbp_18n\n\n1HPT75Male12317.6 139124130130104129801291261351\n\n2DM+HPT60Male155   1551\n\n3HPT62Male1169.251091231091261081151151221311021\n\n4HPT70Male130   1301\n\n5HPT72Male13113.4 1241201461441571231201311201231\n\n6DM+HPT56Male12516.8 1401141631171241211281191001271\n\n7DM+HPT51Male1357.751371351321471301241421441281\n\n8DM73Male14521.2 1601301\n\n9HPT61Female19637.5 1532182181\n\n10HPT59Male13311.5 1351301181501271231491321",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-wrangling.html#str_glue",
    "href": "data-wrangling.html#str_glue",
    "title": "5  Data Wrangling",
    "section": "5.23 str_glue",
    "text": "5.23 str_glue\n\n\nCode\nx &lt;- c(\"Ama\", \"is\", \"a\", \"Girl\")\ncat(x)\n\n\nAma is a Girl\n\n\n\n\nCode\nname &lt;- \"Fred\"\nstr_glue('My name is {name}.')\n\n\nMy name is Fred.\n\n\n\n\nCode\nstringr_fcn &lt;- \"`stringr::str_glue()`\"\nglue_fcn    &lt;- \"`glue::glue()`\"\n\nstr_glue('{stringr_fcn} is essentially an alias for {glue_fcn}.')\n\n\n`stringr::str_glue()` is essentially an alias for `glue::glue()`.\n\n\n\n\nCode\nname &lt;- \"Fred\"\nage &lt;- 50\nanniversary &lt;- as.Date(\"1991-10-12\")\nstr_glue('My name is {name},',\n  ' my age next year is {age + 1},',\n  ' my anniversary is {format(anniversary, \"%A, %B %d, %Y\")}.')\n\n\nMy name is Fred, my age next year is 51, my anniversary is Saturday, October 12, 1991.\n\n\n\n\nCode\nstr_glue('My name is {name},',\n  ' my age next year is {age + 1},',\n  ' my anniversary is {format(anniversary, \"%A, %B %d, %Y\")}.',\n  name = \"Joe\",\n  age = 40,\n  anniversary = as.Date(\"2001-10-12\"))\n\n\nMy name is Joe, my age next year is 41, my anniversary is Friday, October 12, 2001.\n\n\n\n\nCode\nmtcars %&gt;% \n    head() \n\n\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb\n\n21  61601103.9 2.6216.50144\n\n21  61601103.9 2.8817  0144\n\n22.84108933.852.3218.61141\n\n21.462581103.083.2119.41031\n\n18.783601753.153.4417  0032\n\n18.162251052.763.4620.21031\n\n\n\n\nCode\nhead(mtcars) %&gt;% \n    glue::glue_data(\"{rownames(.)} has {hp} hp\")\n\n\nMazda RX4 has 110 hp\nMazda RX4 Wag has 110 hp\nDatsun 710 has 93 hp\nHornet 4 Drive has 110 hp\nHornet Sportabout has 175 hp\nValiant has 105 hp\n\n\n\n\nCode\nhead(iris) %&gt;%\n  mutate(\n      description = str_glue(\n          \"This {Species} has a petal length of {Petal.Length}\"\n          )\n      ) \n\n\n\n\nSepal.LengthSepal.WidthPetal.LengthPetal.WidthSpeciesdescription\n\n5.13.51.40.2setosaThis setosa has a petal length of 1.4\n\n4.93  1.40.2setosaThis setosa has a petal length of 1.4\n\n4.73.21.30.2setosaThis setosa has a petal length of 1.3\n\n4.63.11.50.2setosaThis setosa has a petal length of 1.5\n\n5  3.61.40.2setosaThis setosa has a petal length of 1.4\n\n5.43.91.70.4setosaThis setosa has a petal length of 1.7\n\n\n\n\n\n\nCode\nstr_glue(\"\n    A formatted string\n    Can have multiple lines\n      with additional indention preserved\n    \")\n\n\nA formatted string\nCan have multiple lines\n  with additional indention preserved\n\n\n\n\nCode\nstr_glue(\"\n\n  leading or trailing newlines can be added explicitly\n\n  \")\n\n\n\nleading or trailing newlines can be added explicitly\n\n\n\n\nCode\nstr_glue(\"\n    A formatted string \\\\\n    can also be on a \\\\\n    single line\n    \")\n\n\nA formatted string can also be on a single line\n\n\n\n\nCode\nname &lt;- \"Fred\"\nstr_glue(\"My name is {name}, not {{name}}.\")\n\n\nMy name is Fred, not {name}.\n\n\n\n\nCode\none &lt;- \"1\"\nstr_glue(\n    \"The value of $e^{2\\\\pi i}$ is $&lt;&lt;one&gt;&gt;$.\", \n    .open = \"&lt;&lt;\", \n    .close = \"&gt;&gt;\")\n\n\nThe value of $e^{2\\pi i}$ is $1$.\n\n\n\n\nCode\ndataF %&gt;% \n    filter(!is.na(sbp_0)) %&gt;% \n    ggplot(aes(x=sbp_0)) +\n    geom_histogram(col = \"grey\", fill = \"wheat\") +\n    labs(title = str_glue(\n        \"Histogram with Mean = {mean_sbp0}mmHg and \\\\\n         Standard Deviation = {sd_sbp0}\",\n        mean_sbp0 = mean(dataF$sbp_0, na.rm=T) %&gt;% \n            round(1),\n        sd_sbp0 = sd(dataF$sbp_0,   na.rm=T) %&gt;% \n            round(1)),\n         x = \"Systolic Blood Pressure (mmHg)\",\n         y = \"Frequency\") +\n    theme_light(base_size = 12, base_family = \"serif\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "6  Data Cleaning",
    "section": "",
    "text": "6.1 Data dictionary or codebook\nFor well-collected and managed data, there should always be a dictionary. The dictionary outlines for every variable in the dataset its variable name, the meaning of the variable, the source of the variable (from the questionnaire, data collection sheet, etc.), the valid ranges or codes and the format. This is an invaluable tool for determining wrong and abnormal entries. It is also sometimes referred to as the codebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#importing-the-data-into-r",
    "href": "data-cleaning.html#importing-the-data-into-r",
    "title": "6  Data Cleaning",
    "section": "6.2 Importing the data into R",
    "text": "6.2 Importing the data into R\nThe first step in analysis is to read or import the data into the data analysis software where a general overview can be obtained. We begin by importing the blood_donors_3.dta into R and calling it blood3.\n\n\nCode\nblood3 &lt;- readxl::read_xls(\"C:/Dataset/blood_donors_2.xls\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#visualising-the-data-in-r",
    "href": "data-cleaning.html#visualising-the-data-in-r",
    "title": "6  Data Cleaning",
    "section": "6.3 Visualising the data in R",
    "text": "6.3 Visualising the data in R\nNext, we visualize the data\n\n\nCode\nblood3\n\n\n# A tibble: 25 × 6\n      id    hb   hct   sex bldgrp pdonor\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1     1 10.5   31.8     1      3      3\n 2     2 11.9   37.2     1      4      0\n 3     3  1     26       1      1      1\n 4     4  8.90  26.8     1      1      3\n 5     5  7.80  24.2     1      1      2\n 6     6 10     30.9     1      2      1\n 7     7 10.4   33.9     1      2      0\n 8     8 11.3   35       1      3      1\n 9     9 16.4   NA       1      4      1\n10    10 14.4   43.6     1      4      1\n# ℹ 15 more rows\n\n\nTo visualise all the data we can use the print() or View() functions. Note that this might not be the best if you have relatively big data. Below I use the as.data.frame() function to display the whole data.\n\n\nCode\nblood3 %&gt;% as.data.frame()\n\n\n   id   hb  hct sex bldgrp pdonor\n1   1 10.5 31.8   1      3      3\n2   2 11.9 37.2   1      4      0\n3   3  1.0 26.0   1      1      1\n4   4  8.9 26.8   1      1      3\n5   5  7.8 24.2   1      1      2\n6   6 10.0 30.9   1      2      1\n7   7 10.4 33.9   1      2      0\n8   8 11.3 35.0   1      3      1\n9   9 16.4   NA   1      4      1\n10 10 14.4 43.6   1      4      1\n11 11 11.2 33.2   0      3     99\n12 12 11.5 35.5   0      3      1\n13 13 10.5 33.7   0      3      2\n14 14 12.2 36.8   0      4      1\n15 14 16.4 48.8   0      5      2\n16 16 12.7 99.0   0      4      0\n17 17  9.8 30.5   0      1      4\n18 18 10.9 33.8   0      3      0\n19 19 11.6 35.4   0      3      3\n20 20 10.6 34.9   0      9      2\n21 21  9.1 28.0   9      1      3\n22 22 11.9 36.1   9      4      3\n23 23 10.5 34.2   9      3      2\n24 24 12.3 38.2   9      4      2\n25 25 11.0 35.7   9      3      2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#describing-or-summarizing-the-data",
    "href": "data-cleaning.html#describing-or-summarizing-the-data",
    "title": "6  Data Cleaning",
    "section": "6.4 Describing or summarizing the data",
    "text": "6.4 Describing or summarizing the data\nWe first use the glimpse() function to have a basic view of variable names and variable types\n\n\nCode\nblood3 %&gt;% glimpse()\n\n\nRows: 25\nColumns: 6\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 16, 17, 18, …\n$ hb     &lt;dbl&gt; 10.5, 11.9, 1.0, 8.9, 7.8, 10.0, 10.4, 11.3, 16.4, 14.4, 11.2, …\n$ hct    &lt;dbl&gt; 31.8, 37.2, 26.0, 26.8, 24.2, 30.9, 33.9, 35.0, NA, 43.6, 33.2,…\n$ sex    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, …\n$ bldgrp &lt;dbl&gt; 3, 4, 1, 1, 1, 2, 2, 3, 4, 4, 3, 3, 3, 4, 5, 4, 1, 3, 3, 9, 1, …\n$ pdonor &lt;dbl&gt; 3, 0, 1, 3, 2, 1, 0, 1, 1, 1, 99, 1, 2, 1, 2, 0, 4, 0, 3, 2, 3,…\n\n\nWe can also use the dfSummary() function from the summarytools package to give a more comprehensive output for each variable.\n\n\nCode\nblood3 %&gt;% summarytools::dfSummary()\n\n\nData Frame Summary  \nblood3  \nDimensions: 25 x 6  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values            Freqs (% of Valid)   Graph               Valid      Missing  \n---- ----------- ------------------------- -------------------- ------------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)      24 distinct values   : : : : :           25         0        \n     [numeric]   min &lt; med &lt; max:                               : : : : :           (100.0%)   (0.0%)   \n                 1 &lt; 13 &lt; 25                                    : : : : :                               \n                 IQR (CV) : 12 (0.6)                            : : : : :                               \n                                                                : : : : :                               \n\n2    hb          Mean (sd) : 11 (2.9)      21 distinct values             :         25         0        \n     [numeric]   min &lt; med &lt; max:                                         :         (100.0%)   (0.0%)   \n                 1 &lt; 11 &lt; 16.4                                            :                             \n                 IQR (CV) : 1.5 (0.3)                                   . :                             \n                                                                .     . : : : . :                       \n\n3    hct         Mean (sd) : 36.8 (14.3)   24 distinct values     :                 24         1        \n     [numeric]   min &lt; med &lt; max:                                 :                 (96.0%)    (4.0%)   \n                 24.2 &lt; 34.6 &lt; 99                                 :                                     \n                 IQR (CV) : 4.7 (0.4)                             :                                     \n                                                                : : .         .                         \n\n4    sex         Mean (sd) : 2.2 (3.5)     0 : 10 (40.0%)       IIIIIIII            25         0        \n     [numeric]   min &lt; med &lt; max:          1 : 10 (40.0%)       IIIIIIII            (100.0%)   (0.0%)   \n                 0 &lt; 1 &lt; 9                 9 :  5 (20.0%)       IIII                                    \n                 IQR (CV) : 1 (1.6)                                                                     \n\n5    bldgrp      Mean (sd) : 3.1 (1.7)     1 : 5 (20.0%)        IIII                25         0        \n     [numeric]   min &lt; med &lt; max:          2 : 2 ( 8.0%)        I                   (100.0%)   (0.0%)   \n                 1 &lt; 3 &lt; 9                 3 : 9 (36.0%)        IIIIIII                                 \n                 IQR (CV) : 2 (0.5)        4 : 7 (28.0%)        IIIII                                   \n                                           5 : 1 ( 4.0%)                                                \n                                           9 : 1 ( 4.0%)                                                \n\n6    pdonor      Mean (sd) : 5.6 (19.5)    0 : 4 (16.0%)        III                 25         0        \n     [numeric]   min &lt; med &lt; max:          1 : 7 (28.0%)        IIIII               (100.0%)   (0.0%)   \n                 0 &lt; 2 &lt; 99                2 : 7 (28.0%)        IIIII                                   \n                 IQR (CV) : 2 (3.5)        3 : 5 (20.0%)        IIII                                    \n                                           4 : 1 ( 4.0%)                                                \n                                           99 : 1 ( 4.0%)                                               \n--------------------------------------------------------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#cleaning-individual-variables",
    "href": "data-cleaning.html#cleaning-individual-variables",
    "title": "6  Data Cleaning",
    "section": "6.5 Cleaning individual variables",
    "text": "6.5 Cleaning individual variables\nWe note that all the variables are of type “double”. sex and bldgrp however, should be factors. This is done and subsequently summarized below.\n\n\nCode\nblood3 &lt;-  \n    blood3 %&gt;% \n    mutate(sex = factor(sex, \n                        levels = c(0,1,9),\n                        labels = c(\"Female\", \"Male\", \"Missing\")),\n           bldgrp= factor(bldgrp, \n                          levels = c(1, 2, 3, 4, 9),\n                          labels = c(\"A\", \"B\", \"O\", \"AB\", \"Missing\"))) \n\nblood3 %&gt;% summarytools::dfSummary()\n\n\nData Frame Summary  \nblood3  \nDimensions: 25 x 6  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values            Freqs (% of Valid)   Graph               Valid      Missing  \n---- ----------- ------------------------- -------------------- ------------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)      24 distinct values   : : : : :           25         0        \n     [numeric]   min &lt; med &lt; max:                               : : : : :           (100.0%)   (0.0%)   \n                 1 &lt; 13 &lt; 25                                    : : : : :                               \n                 IQR (CV) : 12 (0.6)                            : : : : :                               \n                                                                : : : : :                               \n\n2    hb          Mean (sd) : 11 (2.9)      21 distinct values             :         25         0        \n     [numeric]   min &lt; med &lt; max:                                         :         (100.0%)   (0.0%)   \n                 1 &lt; 11 &lt; 16.4                                            :                             \n                 IQR (CV) : 1.5 (0.3)                                   . :                             \n                                                                .     . : : : . :                       \n\n3    hct         Mean (sd) : 36.8 (14.3)   24 distinct values     :                 24         1        \n     [numeric]   min &lt; med &lt; max:                                 :                 (96.0%)    (4.0%)   \n                 24.2 &lt; 34.6 &lt; 99                                 :                                     \n                 IQR (CV) : 4.7 (0.4)                             :                                     \n                                                                : : .         .                         \n\n4    sex         1. Female                 10 (40.0%)           IIIIIIII            25         0        \n     [factor]    2. Male                   10 (40.0%)           IIIIIIII            (100.0%)   (0.0%)   \n                 3. Missing                 5 (20.0%)           IIII                                    \n\n5    bldgrp      1. A                      5 (20.8%)            IIII                24         1        \n     [factor]    2. B                      2 ( 8.3%)            I                   (96.0%)    (4.0%)   \n                 3. O                      9 (37.5%)            IIIIIII                                 \n                 4. AB                     7 (29.2%)            IIIII                                   \n                 5. Missing                1 ( 4.2%)                                                    \n\n6    pdonor      Mean (sd) : 5.6 (19.5)    0 : 4 (16.0%)        III                 25         0        \n     [numeric]   min &lt; med &lt; max:          1 : 7 (28.0%)        IIIII               (100.0%)   (0.0%)   \n                 0 &lt; 2 &lt; 99                2 : 7 (28.0%)        IIIII                                   \n                 IQR (CV) : 2 (3.5)        3 : 5 (20.0%)        IIII                                    \n                                           4 : 1 ( 4.0%)                                                \n                                           99 : 1 ( 4.0%)                                               \n--------------------------------------------------------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#checking-for-duplicated-records",
    "href": "data-cleaning.html#checking-for-duplicated-records",
    "title": "6  Data Cleaning",
    "section": "6.6 Checking for duplicated records",
    "text": "6.6 Checking for duplicated records\nWe begin official data cleaning by checking if we have duplicate records in our data\n\n\nCode\nblood3 %&gt;% janitor::get_dupes()\n\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: id, hb, hct, sex, bldgrp, pdonor\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: id &lt;dbl&gt;, hb &lt;dbl&gt;, hct &lt;dbl&gt;, sex &lt;fct&gt;, bldgrp &lt;fct&gt;,\n#   pdonor &lt;dbl&gt;, dupe_count &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#cleaning-individual-variables-1",
    "href": "data-cleaning.html#cleaning-individual-variables-1",
    "title": "6  Data Cleaning",
    "section": "6.7 Cleaning individual variables",
    "text": "6.7 Cleaning individual variables\nNext, we begin to sort the variables one by one. We begin with the study id variable. We begin by looking for duplicated study ids.\n\n\nCode\nblood3 %&gt;% janitor::get_dupes(id)\n\n\n# A tibble: 2 × 7\n     id dupe_count    hb   hct sex    bldgrp pdonor\n  &lt;dbl&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt;\n1    14          2  12.2  36.8 Female AB          1\n2    14          2  16.4  48.8 Female &lt;NA&gt;        2\n\n\nStudy id 14 is duplicated! Next, we visually inspect the study ids\n\n\nCode\nblood3$id\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 14 16 17 18 19 20 21 22 23 24 25\n\n\nIt looks like the study ids are in numeric order from 1 to 25 but 14 is duplicated while 15 is missing. We solve this by writing a new study id variable. Afterwards, we check to see if there are any more duplicates.\n\n\nCode\nblood3 &lt;- \n    blood3 %&gt;% \n    mutate(id = 1:25) \n\nblood3 %&gt;% janitor::get_dupes(id)\n\n\nNo duplicate combinations found of: id\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: id &lt;int&gt;, dupe_count &lt;int&gt;, hb &lt;dbl&gt;, hct &lt;dbl&gt;, sex &lt;fct&gt;,\n#   bldgrp &lt;fct&gt;, pdonor &lt;dbl&gt;\n\n\nNext, we inspect the hb variable with a summary and a boxplot. We observe from the summary none of the haemoglobin observations is missing. The boxplot of the hb is as shown in A below. We observe 4 are outliers and one looks very extreme.\n\n\nCode\nblood3 %$% summary(hb)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   10.40   11.00   10.99   11.90   16.40 \n\n\n\n\nCode\nA &lt;-\n    blood3 %&gt;% \n    ggplot(aes(y = hb)) +\n    geom_boxplot(fill = \"grey\") +\n    labs(y = \"Hemoglobin (mg/dl)\",\n         title = \"Boxplot of hemoglobin of participants \n         with outliers\") +\n    theme_bw()\n\n\nWe convert this observation to missing as below.\n\n\nCode\nblood3 &lt;- \n    blood3 %&gt;% \n    mutate(hb = ifelse(hb &lt; 4, NA, hb))\n\n\n\n\nCode\nB &lt;-\n    blood3 %&gt;% \n    ggplot(aes(y = hb)) +\n    geom_boxplot(fill = \"grey\") +\n    labs(y = \"Hemoglobin (mg/dl)\",\n         title = \"Boxplot of hemoglobin of participants \n         after outlier removed\") +\n    theme_bw()\n\nA + B + plot_annotation(tag_levels = 'A')\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nAnd redraw the boxplot without the outlier.\n\n\nCode\nblood3 %&gt;% \n    drop_na() %&gt;% \n    ggplot(aes(y = hb)) +\n    geom_boxplot(fill =\"grey\") +\n    labs(y = \"Hemoglobin (mg/dl)\",\n         title = \"Boxplot of hemoglobin of participants\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nNext, we focus on the hct variable. It is of note that it has 99 which represents ‘missing’. We therefore remove that as below\n\n\nCode\nblood3 &lt;- \n    blood3 %&gt;% \n    mutate(hct = ifelse(hct &gt;90, NA, hct))\n\n\nAnd draw the boxplot below\n\n\nCode\nblood3 %&gt;% \n    drop_na(hct) %&gt;% \n    ggplot(aes(y = hct)) + \n    geom_boxplot(fill = \"grey\")+ \n    labs(y = \"Hematocrit (%)\",\n         title = \"Boxplot of hematocrit of participants\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nBecause we know the hematocrit has a relationship with the haemoglobin, we use a scatter plot to visualise and possibly pick up suspicious data.\n\n\nCode\nblood3 %&gt;% \n    drop_na(hb, hct) %&gt;% \n    ggplot(aes(x = hct, y = hb)) + \n    geom_point(col = \"red\") + \n    labs(x = \"Hematocrit (%)\",\n         y = \"Hemoglobin (mg/dl)\",\n         title = \"Scatterplot showing the relationship \n         between the hematocrit and hemoglobin\")+\n    theme_bw()\n\n\n\n\n\n\n\n\n\nNext, we inspect the sex variable\n\n\nCode\nblood3 %&gt;% \n    count(sex)\n\n\n# A tibble: 3 × 2\n  sex         n\n  &lt;fct&gt;   &lt;int&gt;\n1 Female     10\n2 Male       10\n3 Missing     5\n\n\nWe then convert the “Missing” category to NA\n\n\nCode\nblood3 &lt;- \n    blood3 %&gt;% \n    mutate(sex = fct_recode(sex, NULL = \"Missing\"))\n\n\nAnd then check\n\n\nCode\nblood3 %&gt;% \n    count(sex)\n\n\n# A tibble: 3 × 2\n  sex        n\n  &lt;fct&gt;  &lt;int&gt;\n1 Female    10\n2 Male      10\n3 &lt;NA&gt;       5\n\n\nNext, we sort out the bldgrp variable\n\n\nCode\nblood3 %&gt;% \n    count(bldgrp)\n\n\n# A tibble: 6 × 2\n  bldgrp      n\n  &lt;fct&gt;   &lt;int&gt;\n1 A           5\n2 B           2\n3 O           9\n4 AB          7\n5 Missing     1\n6 &lt;NA&gt;        1\n\n\nWe convert Missing to NA and visualize the variable\n\n\nCode\nblood3 &lt;-\n    blood3 %&gt;% \n    mutate(bldgrp = fct_recode(bldgrp, NULL = \"Missing\"))\n\nblood3 %&gt;% count(bldgrp)\n\n\n# A tibble: 5 × 2\n  bldgrp     n\n  &lt;fct&gt;  &lt;int&gt;\n1 A          5\n2 B          2\n3 O          9\n4 AB         7\n5 &lt;NA&gt;       2\n\n\nNext, we sort out the pdonor\n\n\nCode\nblood3 &lt;- \n    blood3 %&gt;% \n    mutate(pdonor = ifelse(pdonor == 99, NA, pdonor))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#visualising-the-cleaned-data",
    "href": "data-cleaning.html#visualising-the-cleaned-data",
    "title": "6  Data Cleaning",
    "section": "6.8 Visualising the cleaned data",
    "text": "6.8 Visualising the cleaned data\nFinally, we summarize the data below\n\n\nCode\nblood3 %&gt;% \n    summarytools::dfSummary()\n\n\nData Frame Summary  \nblood3  \nDimensions: 25 x 6  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------\nNo   Variable    Stats / Values           Freqs (% of Valid)   Graph          Valid      Missing  \n---- ----------- ------------------------ -------------------- -------------- ---------- ---------\n1    id          Mean (sd) : 13 (7.4)     25 distinct values   : : : : :      25         0        \n     [integer]   min &lt; med &lt; max:         (Integer sequence)   : : : : :      (100.0%)   (0.0%)   \n                 1 &lt; 13 &lt; 25                                   : : : : :                          \n                 IQR (CV) : 12 (0.6)                           : : : : :                          \n                                                               : : : : :                          \n\n2    hb          Mean (sd) : 11.4 (2)     20 distinct values       :          24         1        \n     [numeric]   min &lt; med &lt; max:                                  :          (96.0%)    (4.0%)   \n                 7.8 &lt; 11.1 &lt; 16.4                                 :                              \n                 IQR (CV) : 1.5 (0.2)                            . :                              \n                                                               . : : : . :                        \n\n3    hct         Mean (sd) : 34.1 (5.4)   23 distinct values       :          23         2        \n     [numeric]   min &lt; med &lt; max:                                  : .        (92.0%)    (8.0%)   \n                 24.2 &lt; 34.2 &lt; 48.8                                : :                            \n                 IQR (CV) : 4.6 (0.2)                            . : :                            \n                                                               . : : : . .                        \n\n4    sex         1. Female                10 (50.0%)           IIIIIIIIII     20         5        \n     [factor]    2. Male                  10 (50.0%)           IIIIIIIIII     (80.0%)    (20.0%)  \n\n5    bldgrp      1. A                     5 (21.7%)            IIII           23         2        \n     [factor]    2. B                     2 ( 8.7%)            I              (92.0%)    (8.0%)   \n                 3. O                     9 (39.1%)            IIIIIII                            \n                 4. AB                    7 (30.4%)            IIIIII                             \n\n6    pdonor      Mean (sd) : 1.7 (1.1)    0 : 4 (16.7%)        III            24         1        \n     [numeric]   min &lt; med &lt; max:         1 : 7 (29.2%)        IIIII          (96.0%)    (4.0%)   \n                 0 &lt; 2 &lt; 4                2 : 7 (29.2%)        IIIII                              \n                 IQR (CV) : 1.2 (0.7)     3 : 5 (20.8%)        IIII                               \n                                          4 : 1 ( 4.2%)                                           \n--------------------------------------------------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data-cleaning.html#generating-new-variables",
    "href": "data-cleaning.html#generating-new-variables",
    "title": "6  Data Cleaning",
    "section": "6.9 Generating new variables",
    "text": "6.9 Generating new variables\nOften after cleaning individual variables a data analyst will be required to generate new variables from the old ones. We will put this into practice by generating the presence of Anemia if the hb is less than 11g/dl.\n```{r comment=NA} blood3 &lt;- blood3 %&gt;% mutate(anemia = case_when(hb &lt; 11 ~ “Yes”, hb &gt;= 11 ~ “No”) %&gt;% factor())\nsummarytools::dfSummary(blood3)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "7.1 Missing data\nNext, we derive explore the missing data. The plot outlines the percentage of missing data for each with the legend showing if the number of missing is good, ok or bad.\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_missing()\nCode\n# df_schisto %&gt;% dlookr::plot_na_hclust()\n# df_schisto %&gt;% dlookr::plot_na_pareto()\n# df_schisto %&gt;% dlookr::plot_na_intersect()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#categorical-variables",
    "href": "eda.html#categorical-variables",
    "title": "7  Exploratory Data Analysis",
    "section": "7.2 Categorical variables",
    "text": "7.2 Categorical variables\nWe begin by exploring the categorical variables\n\n\nCode\ndf_schisto %&gt;% \n    dlookr::diagnose_category() %&gt;% \n    flextable::flextable()\n\n\nvariableslevelsNfreqratiorankq4sexMale36719452.86103541q4sexFemale36717347.13896462q6religionChristianity36731084.46866491q6religionIslam3675414.71389652q6religionOther36730.81743873educationalstatusPrimary36723564.03269751educationalstatusJSS36713235.96730252q8haematuriaNo36718650.68119891q8haematuriaYes36716444.68664852q8haematuria367174.63215263q8dysuriaNo36720154.76839241q8dysuriaYes36714539.50953682q8dysuria367215.72207083q8abdominal_painNo36720756.40326981q8abdominal_painYes36713235.96730252q8abdominal_pain367287.62942783q8feverNo36726271.38964581q8feverYes3675715.53133512q8fever3674813.07901913q15pipe_borneYes36733591.28065401q15pipe_borneNo367328.71934602q15wellNo36723864.85013621q15wellYes36712935.14986382q15riverYes36725669.75476841q15riverNo36711130.24523162sh36717547.68392371sh++36710127.52043602sh+3676016.34877383sh+++367174.63215264sh_36792.45231615sh-36730.81743876sh++++36720.54495917colourstraw36728276.83923711colour3674111.17166212colourAmber367195.17711173colourL/amber367112.99727524colourB/Stained36761.63487745colourBloody36741.08991836colourRed36741.08991836appearanceClear36718851.22615801appearanceHazy3678122.07084472appearance3674411.98910083appearanceCloudy3674311.71662134appearanceBloody367102.72479565appearanceS/cloudy36710.27247966\n\n\nNext we visualise categorical variable with a barplot\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_bar()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#continuous-variables",
    "href": "eda.html#continuous-variables",
    "title": "7  Exploratory Data Analysis",
    "section": "7.3 Continuous variables",
    "text": "7.3 Continuous variables\nNow to continuous variables\n\n\nCode\ndf_schisto %&gt;% \n    dlookr::diagnose_numeric() %&gt;% \n    flextable::flextable()\n\n\nvariablesminQ1meanmedianQ3maxzerominusoutlierq3age7.0010.0012.53133513.014.0022.0001q35weight14.0026.0035.42568334.044.0073.0001q36height1.00132.10143.345479144.0155.00179.0002wbc3.005.506.9705886.47.7069.00011plt17.60165.50208.133127206.0252.00379.0003mch0.0025.3527.28823526.928.3091.8109mcv1.0061.0090.59752391.0125.00171.0000hct4.6431.4035.52489233.235.85354.70014hb0.0010.6012.05609911.312.30106.01011\n\n\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_histogram()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_boxplot(by = \"q4sex\")\n\n\nWarning: Removed 267 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_density()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_qq()\n\n\nWarning: Removed 267 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 267 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_qq( by = \"q4sex\", )\n\n\nWarning: Removed 267 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 267 rows containing non-finite outside the scale range\n(`stat_qq_line()`).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#outliers",
    "href": "eda.html#outliers",
    "title": "7  Exploratory Data Analysis",
    "section": "7.4 Outliers",
    "text": "7.4 Outliers\n\n\nCode\ndf_schisto %&gt;% \n    dlookr::diagnose_outlier() %&gt;% \n    flextable::flextable()\n\n\nvariablesoutliers_cntoutliers_ratiooutliers_meanwith_meanwithout_meanq3age10.272479622.0000012.53133512.505464q35weight10.272479673.0000035.42568335.322740q36height20.54495917.50000143.345479144.093939wbc112.997275220.627276.9705886.489103plt30.817438719.00000208.133127209.906250mch92.452316140.2777827.28823526.915924mcv00.000000090.59752390.597523hct143.814713981.6457135.52489233.435275hb112.997275229.5381812.05609911.439744\n\n\n\n\nCode\n# df_schisto %&gt;% dlookr::plot_outlier()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#correlation",
    "href": "eda.html#correlation",
    "title": "7  Exploratory Data Analysis",
    "section": "7.5 Correlation",
    "text": "7.5 Correlation\nNext, we look out for correlation in the continuous variables\n\n\nCode\ndf_schisto %&gt;% \n    drop_na() %&gt;% \n    DataExplorer::plot_correlation(type = \"continuous\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "eda.html#scatterplots",
    "href": "eda.html#scatterplots",
    "title": "7  Exploratory Data Analysis",
    "section": "7.6 Scatterplots",
    "text": "7.6 Scatterplots\nNext is scatterplots\n\n\nCode\ndf_schisto %&gt;% \n    DataExplorer::plot_scatterplot(by = \"q35weight\")\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "descriptive-stats-continuous.html",
    "href": "descriptive-stats-continuous.html",
    "title": "8  Descriptive Statistics: Continuous",
    "section": "",
    "text": "8.1 Single continuous variable",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics: Continuous</span>"
    ]
  },
  {
    "objectID": "descriptive-stats-continuous.html#single-continuous-variable",
    "href": "descriptive-stats-continuous.html#single-continuous-variable",
    "title": "8  Descriptive Statistics: Continuous",
    "section": "",
    "text": "8.1.1 Measures of Central Tendency & Dispersion\nBelow we determine the mean, median, standard deviation, range (minimum, maximum) and interquartile range of out initial blood pressure\n\n\nCode\nnewdrug %&gt;% \n    summarise(\n        Mean = mean(bp1), \n        Median = median(bp1), \n        Standard_Dev = sd(bp1), \n        Minimum = min(bp1), \n        Maximum = max(bp1),\n        IQR = IQR(bp1)) \n\n\n# A tibble: 1 × 6\n   Mean Median Standard_Dev Minimum Maximum   IQR\n  &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1  98.3   97.7         5.17    87.5    112.  3.78\n\n\nAlternatively, the psych package gives these measures in further details. The output includes a measure of the Kurtosis and Skewness, both describing the shape of the data.\n\n\nCode\nnewdrug %$% \n    psych::describe(bp1)\n\n\n   vars  n mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 50 98.3 5.17   97.7   97.89 2.97 87.5 111.7  24.2  0.7     0.62 0.73\n\n\nAnd to show the interquartile range we do the following.\n\n\nCode\nnewdrug %$% \n    psych::describe(bp1, IQR = TRUE,quant = c(.25, .75))\n\n\n   vars  n mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 50 98.3 5.17   97.7   97.89 2.97 87.5 111.7  24.2  0.7     0.62 0.73\n    IQR Q0.25 Q0.75\nX1 3.78 95.62  99.4\n\n\n\n\n8.1.2 Graphs - Histogram\n\n\nCode\nnewdrug %&gt;% \n    ggplot(aes(x = bp1)) + \n    geom_histogram(bins = 7, col=\"black\", alpha = .5, fill = \"red\") +\n    labs(\n        title = \"Histogram of Blood Pressure before  intervention\",\n        x= \"BP1\")+\n    theme_light()\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Graphs - Boxplot and violin plot\n\n\nCode\nnewdrug %&gt;% \n    ggplot(aes(y = bp1)) + \n    geom_boxplot(col=\"black\",  \n                 alpha = .2, \n                 fill = \"blue\", \n                 outlier.fill = \"black\",\n                 outlier.shape = 22) +\n    labs(\n        title = \"Boxplot of Blood Pressure before  intervention\",\n        y = \"BP1\")+\n    theme_light()\n\n\n\n\n\n\n\n\n\n\n8.1.3.1 Graphs - Density plot\n\n\nCode\nnewdrug %&gt;% \n    ggplot(aes(y = bp1)) + \n    geom_density(col=\"black\", fill = \"yellow\", alpha=.6) +\n    labs(\n        title = \"Density Plot of Blood Pressure before  intervention\",\n        y = \"Blood Pressure before  intervention\")+\n    coord_flip() +\n    theme_light()\n\n\n\n\n\n\n\n\n\n\n\n8.1.3.2 Graphs - Cumulative Frequency plot\n\n\nCode\nnewdrug %&gt;% \n    group_by(bp1) %&gt;% \n    summarize(n = n()) %&gt;% \n    ungroup() %&gt;% \n    mutate(cum = cumsum(n)/sum(n)*100) %&gt;% \n    ggplot(aes(y = cum, x = bp1)) +\n    geom_line(col=3, linewidth=1.2)+\n    labs(\n        title = \"Cumulative Frequency Plot of Blood Pressure before  intervention\",\n        x = \"BP1\",\n        y = \"Cumulative Frequency\")+\n    theme_light() \n\n\n\n\n\n\n\n\n\n\n\n\n8.1.4 Multiple Continuous variables\n\n8.1.4.1 Measures of Central tendency & Dispersion\n\n\nCode\nnewdrug %&gt;% \n    select(where(is.numeric)) %&gt;% \n    psych::describe()\n\n\n       vars  n  mean   sd median trimmed  mad  min   max range  skew kurtosis\nage       1 50 61.48 6.51  63.00   61.98 4.45 45.0  75.0  30.0 -0.60     0.16\nbp1       2 50 98.30 5.17  97.70   97.89 2.97 87.5 111.7  24.2  0.70     0.62\nbp2       3 50 88.60 4.56  88.15   88.46 4.52 78.0  99.7  21.7  0.25    -0.24\nbpdiff    4 50  9.70 6.20   8.25    8.95 5.49  0.5  26.3  25.8  0.93     0.24\n         se\nage    0.92\nbp1    0.73\nbp2    0.65\nbpdiff 0.88\n\n\nTo illustrate graphing multiple continuous variables we use the 2 bp variables\n\n\nCode\nbps &lt;- \n    newdrug %&gt;%\n    select(bp1, bp2) %&gt;% \n    pivot_longer(\n        cols = c(bp1, bp2),\n        names_to = \"measure\", \n        values_to = \"bp\") %&gt;% \n    mutate(\n        measure = fct_recode(\n            measure, \n            \"Pre-Treatment\" = \"bp1\", \n            \"Post-Treatment\" = \"bp2\"))\n\n\nNext, we create multiple density plots\n\n\nCode\nbps %&gt;% \n    ggplot(aes(y = measure, x = bp, fill = measure)) +\n    ggridges::geom_density_ridges2( col=\"black\", alpha = .5, scale=1, \n                                    show.legend = F) +\n    labs(\n        x = \"Blood pressure (mmHg)\", \n        y = \"Density\", \n        fill = \"Blood Pressure\") +\n    theme_bw()\n\n\nPicking joint bandwidth of 1.52\n\n\n\n\n\n\n\n\n\n\n\nCode\nbps %&gt;% \n    ggplot(aes(y = measure, x = bp, fill = measure))+\n    geom_boxplot(show.legend = FALSE) +\n    labs(y = NULL, \n         x = \"Blood Pressure\", \n         fill = \"Blood Pressure\") +\n    coord_flip()+\n    theme_light() \n\n\n\n\n\n\n\n\n\n\n\nCode\nbps %&gt;% \n    ggplot(aes(y = measure, x = bp, fill = measure))+\n    geom_violin(show.legend = FALSE) +\n    coord_flip()+\n    theme_light()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics: Continuous</span>"
    ]
  },
  {
    "objectID": "descriptive-stats-continuous.html#continuous-by-a-single-categorical-variable",
    "href": "descriptive-stats-continuous.html#continuous-by-a-single-categorical-variable",
    "title": "8  Descriptive Statistics: Continuous",
    "section": "8.2 Continuous by a single categorical variable",
    "text": "8.2 Continuous by a single categorical variable\n\n8.2.1 Summary\nWe do this with one variable.\n\n\nCode\nnewdrug %&gt;% \n    group_by(treat) %&gt;% \n    summarize(\n        mean.bp1 = mean(bp1),\n        sd.bp1 = sd(bp1),\n        var.bp1 = var(bp1),\n        se.mean.bp1 = sd(bp1)/sqrt(n()),\n        median.bp1 = median(bp1),\n        min.bp1 = min(bp1),\n        max.bp1 = max(bp1)) %&gt;% \n    ungroup()\n\n\n# A tibble: 2 × 8\n  treat   mean.bp1 sd.bp1 var.bp1 se.mean.bp1 median.bp1 min.bp1 max.bp1\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Control     97.1   3.56    12.7       0.760       97.4    89.8    103.\n2 Newdrug     99.2   6.05    36.6       1.14        98.2    87.5    112.\n\n\n\n\n8.2.2 Graph - Histogram, Boxplot, Density plot and cumulative frequency\nThe graphs are similar to the above so we skip them.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics: Continuous</span>"
    ]
  },
  {
    "objectID": "descriptive-stats-continuous.html#continuous-by-multiple-categorical-variables",
    "href": "descriptive-stats-continuous.html#continuous-by-multiple-categorical-variables",
    "title": "8  Descriptive Statistics: Continuous",
    "section": "8.3 Continuous by multiple categorical variables",
    "text": "8.3 Continuous by multiple categorical variables\n\n8.3.1 Summary\nThis can be done as below.\n\n\nCode\nnewdrug %&gt;% \n    group_by(treat, sex) %&gt;% \n    summarize(\n        mean.bp1 = mean(bp1),\n        sd.bp1 = sd(bp1),\n        var.bp1 = var(bp1),\n        se.mean.bp1 = sd(bp1)/sqrt(n()),\n        median.bp1 = median(bp1),\n        min.bp1 = min(bp1),\n        max.bp1 = max(bp1),\n        .groups = \"drop\") \n\n\n# A tibble: 4 × 9\n  treat   sex   mean.bp1 sd.bp1 var.bp1 se.mean.bp1 median.bp1 min.bp1 max.bp1\n  &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Control F         97.2   3.82    14.6        1.15       97.4    90.1    103.\n2 Control M         97.0   3.47    12.1        1.05       97.5    89.8    102.\n3 Newdrug F         98.6   6.01    36.1        1.55       98.4    87.5    112.\n4 Newdrug M        100.    6.25    39.1        1.73       98.1    91.7    112.\n\n\nAnd this can be presented in a boxplot below\n\n\nCode\nnewdrug %&gt;% \n    ggplot(aes(y = bp1, x = sex, fill = treat)) +\n    geom_boxplot()+\n    labs(\n        y = \"Blood Pressure (mmHg)\",\n        x =  \"Sex\",\n        fill = 'Treatment') +\n    theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics: Continuous</span>"
    ]
  },
  {
    "objectID": "descriptive-stats-categorical.html",
    "href": "descriptive-stats-categorical.html",
    "title": "9  Descriptive Statistics: Categorical",
    "section": "",
    "text": "9.1 Single Categorical Variable",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive Statistics: Categorical</span>"
    ]
  },
  {
    "objectID": "descriptive-stats-categorical.html#single-categorical-variable",
    "href": "descriptive-stats-categorical.html#single-categorical-variable",
    "title": "9  Descriptive Statistics: Categorical",
    "section": "",
    "text": "9.1.1 Frequencies & Proportions\nThe most common modality for presenting a single categorical variables is tabulating the observations, and subsequently expressing these frequencies as proportions or percentages. This is done below\n\n\nCode\ntitanic2 %&gt;% \n    gtsummary::tbl_summary(\n        include = class,\n        digits = class ~ c(0,1)\n    ) %&gt;% \n    gtsummary::bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 2,201\n1\n\n\n\n\nPassenger's cabin class\n\n\n\n\n    first\n325 (14.8%)\n\n\n    second\n285 (12.9%)\n\n\n    third\n706 (32.1%)\n\n\n    crew\n885 (40.2%)\n\n\n\n1\nn (%)\n\n\n\n\n\n\n\n\n\n\n9.1.2 Graph - Barchart\nWe first summarize the data\n\n\nCode\nbar_data &lt;-\n    titanic2 %&gt;% \n    drop_na(class) %&gt;% \n    count(class) %&gt;% \n    mutate(perc = `n` / sum(`n`)) %&gt;% \n    arrange(perc) %&gt;%\n    mutate(labels = paste(n, \" (\", scales::percent(perc), \")\", sep=\"\"))\n\nbar_data\n\n\n\n\nclassnperclabels\n\nsecond2850.129285 (12.9%)\n\nfirst3250.148325 (14.8%)\n\nthird7060.321706 (32.1%)\n\ncrew8850.402885 (40.2%)\n\n\n\n\nAnd the plot the barplot\n\n\nCode\nbar_data %&gt;% \n    ggplot() +\n    geom_bar(stat = \"identity\", \n             aes(y = n, x = class, fill = class), \n             col = \"black\", \n             show.legend = F) +\n    geom_label(aes(y = n, label = labels, x = class), \n               vjust = 1.2,\n               show.legend = FALSE, size=3.5) +\n    labs(x = NULL, \n         y = \"Count\", \n         title = \"Distribution of Class of passenger\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n9.1.2.1 Pie Chart\nTo do this we use the previously summarized data. Then we draw a customised Pie Chart\n\n\nCode\nbar_data %&gt;% \n    ggplot(aes(x = \"\", y = perc, fill = class)) +\n    geom_col() +\n    geom_label(aes(label = labels),\n               position = position_stack(vjust = 0.5),\n               show.legend = FALSE, size =3) +\n    coord_polar(theta = \"y\", start=0) +\n    labs(title = \"Distribution of Blood Groups of study participants\",\n         fill = \"Blood Group\") +\n    theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.3 Two categorical Variables\n\n9.1.3.1 Frequencies & Proportions\n\n\nCode\ntitanic2 %&gt;% \n    tbl_cross(row = sex, col = died) %&gt;% \n    bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDied at sea\n\nTotal\n\n\nNo\nYes\n\n\n\n\nSex of passenger\n\n\n\n\n\n\n\n\n    female\n344\n126\n470\n\n\n    male\n367\n1,364\n1,731\n\n\nTotal\n711\n1,490\n2,201\n\n\n\n\n\n\n\n\n\n9.1.3.2 Row percentages\n\n\nCode\ntitanic2 %&gt;% \n    tbl_cross(row = sex, col = died, percent = \"row\", digits = c(0,1)) %&gt;% \n    bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDied at sea\n\nTotal\n\n\nNo\nYes\n\n\n\n\nSex of passenger\n\n\n\n\n\n\n\n\n    female\n344 (73.2%)\n126 (26.8%)\n470 (100.0%)\n\n\n    male\n367 (21.2%)\n1,364 (78.8%)\n1,731 (100.0%)\n\n\nTotal\n711 (32.3%)\n1,490 (67.7%)\n2,201 (100.0%)\n\n\n\n\n\n\n\n\n\n9.1.3.3 Column percentages\n\n\nCode\ntitanic2 %&gt;% \n    tbl_cross(row = sex, col = died, percent = \"column\", digits = c(0,1)) %&gt;% \n    bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDied at sea\n\nTotal\n\n\nNo\nYes\n\n\n\n\nSex of passenger\n\n\n\n\n\n\n\n\n    female\n344 (48.4%)\n126 (8.5%)\n470 (21.4%)\n\n\n    male\n367 (51.6%)\n1,364 (91.5%)\n1,731 (78.6%)\n\n\nTotal\n711 (100.0%)\n1,490 (100.0%)\n2,201 (100.0%)\n\n\n\n\n\n\n\n\n\n9.1.3.4 Table Total Percentages\n\n\nCode\ntitanic2 %&gt;% \n    tbl_cross(\n        row = sex, \n        col = died, \n        percent = c(\"cell\"), \n        digits = c(0,1)) %&gt;% \n    bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDied at sea\n\nTotal\n\n\nNo\nYes\n\n\n\n\nSex of passenger\n\n\n\n\n\n\n\n\n    female\n344 (15.6%)\n126 (5.7%)\n470 (21.4%)\n\n\n    male\n367 (16.7%)\n1,364 (62.0%)\n1,731 (78.6%)\n\n\nTotal\n711 (32.3%)\n1,490 (67.7%)\n2,201 (100.0%)\n\n\n\n\n\n\n\n\n\n9.1.3.5 Bar Chart\n\n\nCode\ntitanic2 %&gt;% \n    ggplot(aes(x = class, fill = died)) +\n    geom_bar(position = position_dodge(), col = \"black\") +\n    labs(y = \"Count\", x = \"Class\", fill = \"Died\",\n          title = \"Bar plot of outcome of passengers for each class\") +\n    theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptive Statistics: Categorical</span>"
    ]
  },
  {
    "objectID": "analysis-of-continuous data.html",
    "href": "analysis-of-continuous data.html",
    "title": "10  Analysis of Numeric Data",
    "section": "",
    "text": "10.1 Confidence interval of a mean\nTo determine the confidence interval of the mean of a numeric variable in R, we use the One Sample Student’s T-test. The assumptions for the validity of this test are:\nWith our sample considered to be randomly selected and a sample size of 140, we apply the One-sample T-test as below.\nWe first import the data\nCode\ndf_data1 &lt;- \n    read_delim(\n        file = \"C:/Dataset/data1.txt\", \n        delim = \"\\t\",\n        col_types = c(\"c\", \"f\", \"i\",\"i\")\n    ) %&gt;% \n    mutate(sex = factor(sex))\nAnd then summarize it below\nCode\ndf_data1 %&gt;% \n    summarytools::dfSummary(graph.col = F)\n\n\nData Frame Summary  \ndf_data1  \nDimensions: 140 x 4  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------\nNo   Variable      Stats / Values            Freqs (% of Valid)   Valid      Missing  \n---- ------------- ------------------------- -------------------- ---------- ---------\n1    id            1. 1                        1 ( 0.7%)          140        0        \n     [character]   2. 10                       1 ( 0.7%)          (100.0%)   (0.0%)   \n                   3. 100                      1 ( 0.7%)                              \n                   4. 101                      1 ( 0.7%)                              \n                   5. 102                      1 ( 0.7%)                              \n                   6. 103                      1 ( 0.7%)                              \n                   7. 104                      1 ( 0.7%)                              \n                   8. 105                      1 ( 0.7%)                              \n                   9. 106                      1 ( 0.7%)                              \n                   10. 107                     1 ( 0.7%)                              \n                   [ 130 others ]            130 (92.9%)                              \n\n2    sex           1. Female                 64 (45.7%)           140        0        \n     [factor]      2. Male                   76 (54.3%)           (100.0%)   (0.0%)   \n\n3    weight        Mean (sd) : 12.2 (6.3)    28 distinct values   140        0        \n     [numeric]     min &lt; med &lt; max:                               (100.0%)   (0.0%)   \n                   2 &lt; 11 &lt; 33                                                        \n                   IQR (CV) : 7 (0.5)                                                 \n\n4    height        Mean (sd) : 90.9 (21.3)   70 distinct values   139        1        \n     [numeric]     min &lt; med &lt; max:                               (99.3%)    (0.7%)   \n                   49 &lt; 88 &lt; 137                                                      \n                   IQR (CV) : 33 (0.2)                                                \n--------------------------------------------------------------------------------------\nvariablenminmaxmedianiqrmeansdseci\n\nheight13949137883390.921.31.813.58\nCode\ndf_data1 %&gt;% \n    meantables::mean_table(height) \n\n\n\n\nresponse_varnmeansdsemlcluclminmax\n\nheight13990.921.31.8187.394.449137\nFor sex stratified confidence intervals we have\nCode\ndf_data1 %&gt;% \n    group_by(sex) %&gt;% \n    meantables::mean_table(height)\n\n\n\n\nresponse_vargroup_vargroup_catnmeansdsemlcluclminmax\n\nheightsexFemale6392.2232.8986.498  49137\n\nheightsexMale7689.8202.2985.294.355131",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of Numeric Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-continuous data.html#confidence-interval-of-a-mean",
    "href": "analysis-of-continuous data.html#confidence-interval-of-a-mean",
    "title": "10  Analysis of Numeric Data",
    "section": "",
    "text": "The sample should have been randomly chosen\nThe population distribution of the variable should be normal. This can be assumed to be present if\n\nThe distribution of the population is known to be normally distributed\nThe population distribution should have one mode, symmetric, without outliers and a sample size of 15 or less\nThe population distribution should be moderately skewed, without outliers, and have one mode and with a sample size between 16 and 40\nThe sample size is more than 40 and the data has no outliers.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of Numeric Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-continuous data.html#comparing-the-mean-to-a-hypothesised-value",
    "href": "analysis-of-continuous data.html#comparing-the-mean-to-a-hypothesised-value",
    "title": "10  Analysis of Numeric Data",
    "section": "10.2 Comparing the mean to a hypothesised value",
    "text": "10.2 Comparing the mean to a hypothesised value\nAssuming the objective of the data collected was to determine if the average weight of our population was similar to a population with a known mean weight of 14kgs.\nOur null hypothesis is:\n\nH0: There is no difference in mean weight between our population and a population with mean weight of 14kgs.\n\nTo test this hypothesis we use the One sample t-test after we have satisfied ourselves that the assumptions for its use have been met.\n\n\nCode\ndf_data1 %$% \n    t.test(weight, mu=14) %&gt;% \n    broom::tidy()\n\n\n\n\nestimatestatisticp.valueparameterconf.lowconf.highmethodalternative\n\n12.2-3.320.0011313911.213.3One Sample t-testtwo.sided\n\n\n\n\nThe p-value of 0.001 is the probability that our sample could have come from a population with a mean weight of 14kgs. Since this is very small we reject the null H0 at a 5% significance level and conclude that our population mean weight is significantly different from 14kgs. The confidence interval generated is that of our sample mean. With the hypothesized value of 14kgs outside the confidence interval of the mean we conclude that there is insuficient evidence to suggest that the mean weight of our population is 14kgs.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of Numeric Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-continuous data.html#comparing-mean-of-two-independent-groups",
    "href": "analysis-of-continuous data.html#comparing-mean-of-two-independent-groups",
    "title": "10  Analysis of Numeric Data",
    "section": "10.3 Comparing mean of two independent groups",
    "text": "10.3 Comparing mean of two independent groups\nThis is possibly the most common use for the t-test. To compare the mean weights of the males and female in our study we come up with\n\nH0: There is no difference in weight between the males and females in our population.\n\nTo test this assertion we first determine if our sample fits the assumption for the use of the Two sample t-test. These are:\n\nThe sample should have been randomly chosen\nThe two samples are completely independent\nEach population is at least 20 times larger than its respective sample.\nThe population distribution of the variable should be normal. This can be assumed to be present if\n\nThe distribution of the population is known to be normal\nThe population distribution should have one mode, symmetric, without outliers and a sample size of 15 or less\nThe population distribution should be moderately skewed, without outliers, have one mode and with a sample size between 16 and 40\nThe sample size is more than 40 and data has no outliers.\n\n\nOur data fulfils all the criteria above hence we apply the test\n\n\nCode\ndf_data1 %$% \n    t.test(formula = weight ~ sex) %&gt;% \n    broom::tidy()\n\n\n\n\nestimateestimate1estimate2statisticp.valueparameterconf.lowconf.highmethodalternative\n\n1.3112.911.61.20.234117-0.8563.47Welch Two Sample t-testtwo.sided\n\n\n\n\nWith a relatively high p-value we conclude that there is insufficient evidence to refute null hypothesis. In other words there is insufficient evidence to conclude the mean weights of males and females differ in the study population. Note that in our sample however females appear heavier than males as shown by the last two lines of the output above.\nThe confidence interval determined above (-0.86 to 3.47) is actually that for the mean sample difference between females and males. Since the confidence interval contains the null value from H0 above i.e. 0, we conclude that there isn’t enough evidence of a difference in mean weight between the two sexes. Hence both the confidence interval and p-value come to similar conclusions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of Numeric Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-continuous data.html#comparing-means-of-paired-observations",
    "href": "analysis-of-continuous data.html#comparing-means-of-paired-observations",
    "title": "10  Analysis of Numeric Data",
    "section": "10.4 Comparing means of paired observations",
    "text": "10.4 Comparing means of paired observations\nIn this section we use the bread.txt data which has the weight in grams before and after baking some loaves of bread. A description of the variables is contained in the data file. Paired observations occur in circumstances where repeated measurement are done on the same object or data collected has some characteristics in common. For the bread data the same bread is weighed before and after baking. Determining if there is a significant difference between the two measurements requires the use of the Paired t-test. As always be begin by importing the data\nNext we determine the structure of the data frame df2\n\n\nCode\ndf_bread &lt;- \n    read.table(\n        \"C:/Dataset/bread.txt\", \n        sep=\"\\t\", \n        header=T) %&gt;% \n    mutate(oven = factor(oven), type = factor(type))\n\n\nAnd then summarize it\n\n\nCode\ndf_bread %&gt;% summarytools::dfSummary(graph.col = F)\n\n\nData Frame Summary  \ndf_bread  \nDimensions: 399 x 5  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------\nNo   Variable    Stats / Values             Freqs (% of Valid)    Valid      Missing  \n---- ----------- -------------------------- --------------------- ---------- ---------\n1    sid         Mean (sd) : 200 (115.3)    399 distinct values   399        0        \n     [integer]   min &lt; med &lt; max:           (Integer sequence)    (100.0%)   (0.0%)   \n                 1 &lt; 200 &lt; 399                                                        \n                 IQR (CV) : 199 (0.6)                                                 \n\n2    type        1. maize                   212 (53.1%)           399        0        \n     [factor]    2. wheat                   187 (46.9%)           (100.0%)   (0.0%)   \n\n3    before      Mean (sd) : 379.5 (28.1)   120 distinct values   399        0        \n     [integer]   min &lt; med &lt; max:                                 (100.0%)   (0.0%)   \n                 304 &lt; 379 &lt; 479                                                      \n                 IQR (CV) : 40 (0.1)                                                  \n\n4    after       Mean (sd) : 217 (29.8)     121 distinct values   399        0        \n     [integer]   min &lt; med &lt; max:                                 (100.0%)   (0.0%)   \n                 140 &lt; 215 &lt; 295                                                      \n                 IQR (CV) : 48 (0.1)                                                  \n\n5    oven        1. Firewood                199 (49.9%)           399        0        \n     [factor]    2. Gas                     200 (50.1%)           (100.0%)   (0.0%)   \n--------------------------------------------------------------------------------------\n\n\nOur next task is to compare the weight of the loaves of bread before and after baking. We begin by looking at the mean and standard deviations of the two weights.\n\n\nCode\noptions(huxtable.knit_print_df = TRUE)\n\n\n\n\nCode\ndf_bread %&gt;% \n    select(before, after) %&gt;% \n    rstatix::get_summary_stats(type= \"mean_sd\")\n\n\n\n\nvariablenmeansd\n\nbefore39938028.1\n\nafter39921729.8\n\n\n\n\nIt is obvious that the mean weight of the bread before baking is much higher than after however the standard deviations appear similar. For a formal test to determine the difference in means we use the paired t-test. As before the we state the assumptions of the paired t-test\n\nThe sample should have been randomly chosen\nThe two samples are not independent (they are related)\nEach population is at least 20 times larger than its respective sample.\nThe population distribution of the difference between the two variables should be normal. This can be assumed to be present if\n\nThe distribution of the population is known to be normal\nThe population distribution should have one mode, symmetric, without outliers and a sample size of 15 or less\nThe population distribution should be moderately skewed, without outliers, have one mode and with a sample size between 16 and 40\nThe sample size is more than 40 and data has no outliers.\n\n\nThe only new assumption we need to evaluate here is the distribution of the difference between the weights before and after. We do so below\n\n\nCode\ndf_bread %&gt;% \n    mutate(diff_in_wgt = after - before) %&gt;% \n    ggplot(aes(x = diff_in_wgt)) +\n    geom_histogram(bins = 10, col = \"white\") + \n    labs(x = \"Difference in weight\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nAlternatively, we can perform a Shapiro-Wilk’s test for normality. This has H0 as not deviating from the normal distribution. This is done below\n\n\nCode\ndf_bread %&gt;% \n    mutate(diff_in_wgt = after - before) %&gt;%\n    rstatix::shapiro_test(vars = \"diff_in_wgt\")\n\n\n\n\nvariablestatisticp\n\ndiff_in_wgt0.9970.636\n\n\n\n\nThe output and graphical representation above shows our difference in weight is literally normally distributed. We therefore go ahead to determine the difference in mean weights. First we state our hypothesis\n\nH0: There is no change in weight of loaves of bread after baking\n\nAnd then perform the test after converting the data to the long format\n\n\nCode\ndf_bread %&gt;%\n    pivot_longer(\n        cols = c(before, after), names_to = \"time\",values_to = \"weight\"\n        ) %&gt;% \n    rstatix::t_test(formula = weight~time, paired = TRUE, detailed = TRUE)\n\n\n\n\nestimate.y.group1group2n1n2statisticpdfconf.lowconf.highmethodalternative\n\n-163weightafterbefore399399-94.15.48e-274398-166-159T-testtwo.sided\n\n\n\n\nThere was on average a 162.5g reduction in weight of the loaves of bread after baking. This reduction has a 95% confidence interval of 159.1g to 165.9g and is significantly different from 0 (p-value&lt;0.001).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of Numeric Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-continuous data.html#test-for-equality-of-variances",
    "href": "analysis-of-continuous data.html#test-for-equality-of-variances",
    "title": "10  Analysis of Numeric Data",
    "section": "10.5 Test for equality of variances",
    "text": "10.5 Test for equality of variances\nIn using the Student’s T-test to determine difference between the means of two independent groups we need to be mindful of the variances of each group. The computations done for the independent groups t-test are different when the variances between the groups are similar or different. Therefore to determine if the mean weight significantly differ between males and females we need to determine and compare their variances. The function var.test() in R compares the variances between two independent groups and can be used for this determination.Below we apply this F-test to compare the variances of the weight for the two sexes. First we determine the variances.\n\n\nCode\ndf_data1 %&gt;% \n    group_by(sex) %&gt;%\n    summarise(across(weight, list(var = var, meam = mean)))\n\n\n\n\nsexweight_varweight_meam\n\nFemale50.812.9\n\nMale30.211.6\n\n\n\n\nThere seem to be a big difference between the variance of the weights for the two sexes. That for the females is almost 1.7 times that of the males. To determine if this is not a chance finding we apply a formal statistical test. Here our\n\nH0: There is no difference in the variance of the weights for males and females in our population\n\nThe F-test actually tests the ratio of the variances not the difference. In that regard our null value would be 1.\n\n\nCode\ndf_data1 %&gt;% \n    var.test(formula = weight~sex, data = .) %&gt;% \n    broom::tidy() \n\n\nMultiple parameters; naming those columns num.df, den.df\n\n\n\n\nestimatenum.dfden.dfstatisticp.valueconf.lowconf.highmethodalternative\n\n1.6863751.680.03111.052.73F test to compare two variancestwo.sided\n\n\n\n\nThe significant p-value (at a significance level of 0.05) and a confidence interval not containing 1 (the null value) implies there is very little evidence that the variance between the two groups are the same (in other words they differ significantly). In that case the conclusion from the previous analysis is valid as R assumes the variances to be unequal by default if the t.test() function is used.\nNext we apply the same principle to the determine if the mean heights are similar for males and females in our population. We first determine if the variances are significantly different.\n\n\nCode\ndf_data1 %&gt;% \n    group_by(sex) %&gt;%\n    summarise(\n        across(height, list(var = ~var(., na.rm=T), meam = ~mean(., na.rm=T)))\n    )\n\n\n\n\nsexheight_varheight_meam\n\nFemale52892.2\n\nMale39989.8\n\n\n\n\nFrom the results above the variance for the females look much higher (1.3 times) than the males however we apply a test to formally determine this.\n\n\nCode\ndf_data1 %&gt;% \n    var.test(formula = height~sex, data = .) %&gt;% \n    broom::tidy() \n\n\nMultiple parameters; naming those columns num.df, den.df\n\n\n\n\nestimatenum.dfden.dfstatisticp.valueconf.lowconf.highmethodalternative\n\n1.3262751.320.2460.8232.15F test to compare two variancestwo.sided\n\n\n\n\nBoth p-value and confidence interval conclude there is insufficient evidence to say the two variances are different. To use the t.test() function to determine the possibility that mean height differ between males and females we specify that variance is equal as below.\n\n\nCode\ndf_data1 %&gt;%\n    rstatix::t_test(formula = height~sex, var.equal = TRUE, detailed = TRUE)#|\n\n\n\n\nestimateestimate1estimate2.y.group1group2n1n2statisticpdfconf.lowconf.highmethodalternative\n\n2.3892.289.8heightFemaleMale63760.6540.514137-4.829.59T-testtwo.sided",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of Numeric Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-categorical-data.html",
    "href": "analysis-of-categorical-data.html",
    "title": "11  Analysis of Categorical Data",
    "section": "",
    "text": "11.1 One-sample binomial test",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Categorical Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-categorical-data.html#one-sample-binomial-test",
    "href": "analysis-of-categorical-data.html#one-sample-binomial-test",
    "title": "11  Analysis of Categorical Data",
    "section": "",
    "text": "11.1.1 One sample proportion: Confidence interval\nIn a study to determine the prevalence of hypertension in a certain adult population, a random sample taken revealed 23 out of 67 had hypertension. The proportion of hypertensive patients in the sample is rather straightforward. Approximately 0.34 or (34.3%) of the sample are hypertensives. We however extrapolate to estimate this proportion in the population by estimating the confidence interval. To do this single proportion estimation we use the binom.test() function after we have fulfilled the conditions required for its use. These are:\n\nThe sample was obtained by simple random sampling\nThere are just two possible outcomes for the data, here hypertension (successes) or no hypertension (failures).\nThe sample includes at least 10 successes and 10 failures\nThe population is at least 20 times larger than the sample size.\n\nWith our sample not violating these conditions, we go ahead to use the one-sample binomial test as below:\n\n\nCode\nbinom.test(23, 67) %&gt;% \n    broom::tidy() %&gt;% \n    select(-method)\n\n\n\n\nestimatestatisticp.valueparameterconf.lowconf.highalternative\n\n0.343230.0139670.2320.469two.sided\n\n\n\n\nThe binomial exact confidence interval of the proportion of those with hypertension in our study population, therefore, is 23.2% to 46.9%.\n\n\n11.1.2 One sample proportion: Hypothesis testing\nBefore the data on hypertension above was collected, the investigator\nhypothesized that 50% of the population was hypertensive. Next, we test this hypothesis. As usual, we start by setting the null hypothesis.\n\nH0: The population proportion of hypertensive in the population is 50%\n\n\n\nCode\nbinom.test(x=23, n=67, p=.5) %&gt;% \n    broom::tidy()%&gt;% \n    select(-method)\n\n\n\n\nestimatestatisticp.valueparameterconf.lowconf.highalternative\n\n0.343230.0139670.2320.469two.sided\n\n\n\n\nIt would be realised that this p-value is identical to that in the last computation. This is because the binom.test() by default tests the proportion against a 50:50 proportion in the population. At a 5% significance level, we reject H0 and conclude the proportion of hypertensive in our population significantly differs from 50%.\nSo, what if the investigators had hypothesized that prevalence in the population is 40%? Our null hypothesis would then have been:\n\nH0: The population proportion of hypertensive is 40%\n\nNext, we test this hypothesis.\n\n\nCode\nbinom.test(x=23, n=67, p=.4) %&gt;% \n    broom::tidy()%&gt;% \n    select(-method)\n\n\n\n\nestimatestatisticp.valueparameterconf.lowconf.highalternative\n\n0.343230.384670.2320.469two.sided\n\n\n\n\nAt a significance level of 0.05, we fail to reject the null hypothesis, concluding that there is insufficient evidence to conclude our population prevalence is not 40%.\nFinally, what if the investigators had hypothesized that the population prevalence of hypertension is at least 47%? Remember this is a one-sided test and our null hypothesis would be\n\nH0: The population proportion of hypertension is greater than or equal to 47%\n\nAnd then we test the hypothesis\n\n\nCode\nbinom.test(23, 67, p=.47, alternative = \"less\") %&gt;% \n    broom::tidy()%&gt;% \n    select(-method)\n\n\n\n\nestimatestatisticp.valueparameterconf.lowconf.highalternative\n\n0.343230.02446700.45less\n\n\n\n\nThe small p-value leads us to reject H0 and conclude it is insufficient evidence to support the assertion that the prevalence of hypertension in the population is at least 47%.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Categorical Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-categorical-data.html#two-or-more-sample-binomial-test",
    "href": "analysis-of-categorical-data.html#two-or-more-sample-binomial-test",
    "title": "11  Analysis of Categorical Data",
    "section": "11.2 Two or more sample binomial test",
    "text": "11.2 Two or more sample binomial test\n\n11.2.1 Two sample proportion: Hypothesis testing\nTaking it further we decide to select from a different population and sample 100 persons to determine the proportion of those with hypertension. In this population, we came up with 52 hypertension patients out of a hundred. We aim to compare if there is a significant difference between the proportion of hypertension patients in the two populations. To do this we use the prop.test() function in R.\nFirst, we state the null hypothesis: &gt;H0: The population proportion of hypertension in both populations are the same.\nNext, we test the hypothesis. To do this we first create two vectors representing the number of persons with hypertension and the total samples chosen.\n\n\nCode\nhpt &lt;-c(52, 23) # Vector of numbers with hypertension\nn &lt;- c(100, 67) # Vector of numbers in the sample\n\n\nNext, we apply the test\n\n\nCode\nprop.test(x = hpt, n = n) %&gt;% \n    broom::tidy()%&gt;% \n    select(-method)\n\n\n\n\nestimate1estimate2statisticp.valueparameterconf.lowconf.highalternative\n\n0.520.3434.370.036510.01420.339two.sided\n\n\n\n\np-value of 0.0365 is less than our regular significance level of 0.05 so we reject the null hypothesis and say there is enough evidence to conclude that the prevalence of hypertension is not the same in the two populations.\nThe 95% confidence interval generated above is that of the difference between the two proportions, which is 0.177. The confidence interval of 0.014 to 0.339 does not include the null value 0 so we conclude that there is a difference in the prevalence of hypertension between the 2 populations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Categorical Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-categorical-data.html#chi-squared-test",
    "href": "analysis-of-categorical-data.html#chi-squared-test",
    "title": "11  Analysis of Categorical Data",
    "section": "11.3 Chi-squared test",
    "text": "11.3 Chi-squared test\nPearson’s chi-square test, also known as the chi-square goodness-of-fit test or chi-square test for independence is used to determine if observed frequencies in data are consistent with what is expected. For instance, in Asia, the percentages of ABO blood groups in the population are 38%, 10%, 3% and 49% for groups A, B, AB, and O respectively. In a random sample of 600 persons in Kumasi, Ghana 226, 82, 21 and 271 were blood groups A, B, AB, and O respectively. The chi-squared test can help us determine if the proportion of blood groups found in Kumasi is consistent with that seen in Asia.\n\n11.3.1 Chi-squared goodness of fit test\nIn the blood group example above the investigators may want to know if the proportions of the blood groups in Kumasi are consistent with that seen in Asia. To do this we use the Chi-squared goodness of fit test. As always, we begin by making sure the test can be appropriately used under this condition. The assumptions of the test are:\n\nThe data was obtained randomly from the population\nThe variable under study is categorical\nEach of the observed values in each category is at least 5\n\nWith none of the conditions violated we go ahead to state the null hypothesis as\n\nH0: The distribution of blood groups in Kumasi is not different from that in Asia.\n\nNext, we perform the test but to do that we first create a data frame of the expected and observed frequencies in Asia and Kumasi respectively.\n\n\nCode\nbld_grp &lt;- c(\"A\", \"B\", \"AB\", \"O\")\nAsia &lt;- c(38.0, 10.0, 3.0, 49.9)\nKumasi  &lt;- c(226, 82, 21, 271)\nKumasi &lt;- round(Kumasi/sum(Kumasi)*100, 1)\ndf_temp &lt;- data.frame(bld_grp, Asia, Kumasi)\ndf_temp\n\n\n\n\nbld_grpAsiaKumasi\n\nA38  37.7\n\nB10  13.7\n\nAB3  3.5\n\nO49.945.2\n\n\n\n\nNext, we illustrate these proportions using a barplot by plotting the same blood groups from different regions side by side.\n\n\nCode\ndf_temp %&gt;% \n    pivot_longer(\n        cols = c(Asia, Kumasi), \n        names_to = \"Place\", \n        values_to = \"Perc\") %&gt;%\n    mutate(\n        labels = paste(\n            format(round(Perc,1), nsmall=1), \"%\", sep=\"\")) %&gt;% \n    ggplot(aes(x = bld_grp, y = Perc, fill = Place)) +\n    geom_bar(stat=\"identity\", position= position_dodge()) +\n    geom_text(\n        aes(label=labels), vjust=1.5, color=\"black\", \n        size=3.5, position = position_dodge(0.9)) +\n    scale_fill_brewer(palette=\"Blues\") + \n    labs(\n        title=\"Comparative distribution of Blood Groups\", \n        x = \"Blood Group\", \n        y = \"Frequency\")+\n    theme_bw()\n\n\n\n\n\n\n\n\n\nWe observe the similarities between the proportions from Kumasi and Asia. For instance, the blood groups from both regions show decreasing frequency from O, A, B and AB. However, we also observe that adjacent bars are not exactly of the same height. Blood groups O and B for instance have approximately a 4%-point difference between the two populations. Next, we perform the actual test for the difference in proportions.\n\n\nCode\nchisq.test(x = Kumasi, p = Asia/sum(Asia)) %&gt;% \n    broom::tidy()\n\n\nWarning in chisq.test(x = Kumasi, p = Asia/sum(Asia)): Chi-squared\napproximation may be incorrect\n\n\n\n\nstatisticp.valueparametermethod\n\n1.910.5923Chi-squared test for given probabilities\n\n\n\n\nWith a p-value of 0.592, we fail to reject H0 and conclude there is no evidence that the proportions of blood groups in Kumasi are different from the observed proportions in Asia.\n\n\n11.3.2 Chi-squared test for independent data\nIn this subsection, we will use the ANCdata from the epicalc package. It can also\nbe obtained from the list of data that comes with this book. The ANCdata contains records of high-risk pregnant women in a trial to compare a new and an old method of antenatal care (anc) in two clinics (clinic). The outcome was perinatal mortality, the death of the baby within the first week of life (death).\nWe begin by loading the ANCdata\n\n\nCode\ndf_anc &lt;- \n    read.delim(\"C:/Dataset/ANCData.txt\") %&gt;% \n    mutate(\n        death = factor(\n            death, \n            levels = c(\"no\",\"yes\"), \n            labels = c(\"No\", \"Yes\")),\n        clinic = factor(clinic),\n        anc = factor(\n            anc, \n            levels = c(\"old\", \"new\"), \n            labels = c(\"Old\", \"New\")))\n\ndf_anc %&gt;% summarytools::dfSummary(graph.col = FALSE)\n\n\nData Frame Summary  \ndf_anc  \nDimensions: 755 x 3  \nDuplicates: 747  \n\n--------------------------------------------------------------------------\nNo   Variable   Stats / Values   Freqs (% of Valid)   Valid      Missing  \n---- ---------- ---------------- -------------------- ---------- ---------\n1    death      1. No            689 (91.3%)          755        0        \n     [factor]   2. Yes            66 ( 8.7%)          (100.0%)   (0.0%)   \n\n2    anc        1. Old           419 (55.5%)          755        0        \n     [factor]   2. New           336 (44.5%)          (100.0%)   (0.0%)   \n\n3    clinic     1. A             497 (65.8%)          755        0        \n     [factor]   2. B             258 (34.2%)          (100.0%)   (0.0%)   \n--------------------------------------------------------------------------\n\n\nOur objective in this section is to determine if there is a significant relationship between the anc type and perinatal death. In other words, does perinatal mortality differ in the population depending on the anc method used?\nTo begin we cross-tabulate the two variables.\n\n\nCode\ndf_anc %&gt;% \n    tbl_cross(\n        percent = \"column\",\n        row = death, \n        col = anc, \n        label = list(death ~ \"Death\", anc = \"ANC\")) %&gt;% \n    bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANC\n\nTotal\n\n\nOld\nNew\n\n\n\n\nDeath\n\n\n\n\n\n\n\n\n    No\n373 (89%)\n316 (94%)\n689 (91%)\n\n\n    Yes\n46 (11%)\n20 (6.0%)\n66 (8.7%)\n\n\nTotal\n419 (100%)\n336 (100%)\n755 (100%)\n\n\n\n\n\n\n\n\n\nCode\ndf_anc %&gt;% \n    group_by(death, anc) %&gt;% \n    count() %&gt;% \n    ggplot(aes(fill = death, y = n, x = anc)) + \n    geom_bar(position = \"fill\", stat = \"identity\", col = \"black\") +\n    scale_fill_discrete(name = \"Death\", type = c(\"white\",\"red\"))+\n    labs(\n        y = \"Proportion\", \n        x = \"ANC Type\",\n        title = \"Distribution of the ANC method used and perinatal deaths\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nThe cell proportions are not uniform. The proportion of deaths in those who used the old and anc methods is about 11.0% and 5.9% respectively. Is this enough evidence to conclude the new is better than the old in the population? This question we would have to answer using a formal statistical test.\nTest for independence for tabular data often entails the use of the Chi-squared test and/or Fisher’s exact test. Independence here simply means when one has one variable one cannot predict the other variable.\nNext, we apply the chi-squared test after we have verified that our data does not violate the assumption required for its use. These are.\n\nThe data was obtained randomly from the population\nThe variables under study are categorical\nEach observation fits into only one cell of the table\nThe expected values in each cell of the tabular data are at least 5\n\nOur data does not violate any of these so we go ahead to state the null hypothesis.\n\nH0: There is no difference in the proportion of perinatal deaths between mothers who used the new and old ANC methods.\n\nAnd perform the test\n\n\nCode\ndf_anc %$% \n    table(anc, death) %&gt;% \n    chisq.test() %&gt;% \n    broom::tidy()\n\n\n\n\nstatisticp.valueparametermethod\n\n5.290.02141Pearson's Chi-squared test with Yates' continuity correction\n\n\n\n\nThe test above yields a relatively small p-value compared to a significance level of 0.05, indicating the null hypothesis of independence of the cell proportions are unlikely. In other words, the cell proportions differ significantly, the old method can be said to result in significantly higher perinatal deaths compared to the new method.\n\n\n11.3.3 Chi-squared test for trend\nOften there arise situations in categorical data analysis where the objective is not to just see a difference in proportions as a Chi-squared test for independence or Fisher’s test does but to determine any trend in the proportions seen. Here the Chi-squared test for trend is often employed.\nFor example, in a study to determine the proportion of persons having eye changes the following data was obtained: 4 out of 42 persons aged less than 45yrs, 7 out of 43 aged between 46yrs and 55yrs, 12 out of 46 aged between 56yrs and 65yrs and 15 out of 44 aged more than 65yrs. First, we input these into R and calculate the percentages with the eye changes.\n\n\nCode\nNo.eye &lt;- c(4,7,12,15)\nNo.studied &lt;- c(42, 43, 46, 44)\n\n\nNext, we determine the percentage of those with eye changes for each age group\n\n\nCode\nPerc.eye&lt;-round(No.eye/No.studied * 100, 1)\n\n\nNext, we form a matrix showing the number of persons with eye changes, the number of persons studied and the percentage of persons with eye changes for each age group.\n\n\nCode\nnames(No.eye)&lt;-c(\"&lt;=45yrs\",\"46-55yrs\",\"56-65yrs\",\"&gt;65yrs\")\ncbind(No.eye, No.studied, Perc.eye)\n\n\n         No.eye No.studied Perc.eye\n&lt;=45yrs       4         42      9.5\n46-55yrs      7         43     16.3\n56-65yrs     12         46     26.1\n&gt;65yrs       15         44     34.1\n\n\nFrom the analysis above it can be said that the proportions of persons with eye changes increases with age. However, to determine if this apparent rise could be a chance finding we apply the Chi-Squared test after we confirm the data does not violate the assumptions for its use. The condition for its use is like the chi-squared test for independent data but with an addition that at least one of the variables must be ordered. Now satisfied that our data satisfies the conditions we state the null hypothesis.\n\nH0: There is no trend in the eye changes with increasing age\n\nNext, we put it to the formal test\n\n\nCode\nprop.trend.test(No.eye, No.studied) %&gt;% \n    broom::tidy()\n\n\n\n\nstatisticp.valueparametermethod\n\n8.860.002911Chi-squared Test for Trend in Proportions\n\n\n\n\nWith a p-value less than 0.05, we reject the H0 and conclude that there a significant trend (upward because we know it is) in developing eye changes with increasing age.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Categorical Data</span>"
    ]
  },
  {
    "objectID": "analysis-of-categorical-data.html#fishers-exact-test",
    "href": "analysis-of-categorical-data.html#fishers-exact-test",
    "title": "11  Analysis of Categorical Data",
    "section": "11.4 Fisher’s Exact test",
    "text": "11.4 Fisher’s Exact test\nA valid conclusion from the use of the chi-squared test can only be guaranteed if the counts in all the cells of the table in question are equal to or greater than 5. Whenever the count or value in any of the cells is below 5, Fisher’s exact test must be used instead. Its use and interpretation are similar to the chi-squared test and are demonstrated with the ANCdata as below.\n\n\nCode\ndf_anc %$% \n    table(anc, death) %&gt;% \n    fisher.test() %&gt;% \n    broom::tidy()\n\n\n\n\nestimatep.valueconf.lowconf.highmethodalternative\n\n0.5140.01910.2820.908Fisher's Exact Test for Count Datatwo.sided\n\n\n\n\nThe p-value here is quite like that obtained by the chi-squared test but not the same. However, the conclusion remains the same. The added advantage of using fisher.test() is the provision of the odds ratio and its 95% confidence interval. The odds ratio will be explained in subsequent sections.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysis of Categorical Data</span>"
    ]
  },
  {
    "objectID": "risk-and-odds.html",
    "href": "risk-and-odds.html",
    "title": "12  Risk and Odds",
    "section": "",
    "text": "12.1 Risk\nRisk is defined as the probability of having an outcome. Therefore, if in a the population of 100, 35 develop diabetes mellitus after a specified period of follow-up, the risk of developing diabetes in the population is\n\\[\\frac{35}{100} = 0.35\\] Tabulation of the ANC method and the occurrence of death below, we can conclude that the risk of perinatal mortality when one uses the old method is 0.11 (11.0%) and that for the new method is 0.06 (5.9%).\nCode\ndf_anc %&gt;% \n    tbl_cross(\n        percent = \"column\",\n        row = death, \n        col = anc, \n        label = list(death ~ \"Death\", anc = \"ANC\"),\n        digits = c(0,2)) %&gt;% \n    bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nANC\n\nTotal\n\n\nOld\nNew\n\n\n\n\nDeath\n\n\n\n\n\n\n\n\n    No\n373 (89.02%)\n316 (94.05%)\n689 (91.26%)\n\n\n    Yes\n46 (10.98%)\n20 (5.95%)\n66 (8.74%)\n\n\nTotal\n419 (100.00%)\n336 (100.00%)\n755 (100.00%)\nThis can be written as \\[Re = 0:06 \\text{ and } Rne = 0:11\\]\nWhere \\(Re\\) is the risk in the exposed group (new anc method) and \\(Rne\\) is the risk in the non-exposed (old anc method).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Risk and Odds</span>"
    ]
  },
  {
    "objectID": "risk-and-odds.html#risk-ratio",
    "href": "risk-and-odds.html#risk-ratio",
    "title": "12  Risk and Odds",
    "section": "12.2 Risk Ratio",
    "text": "12.2 Risk Ratio\nA comparative way of expressing the risks in the two groups is by the use of the Risk Ratio or Relative Risk (RR). Where \\[RR = \\frac{Re}{Rne}\\]\nNote that by inference if the \\(Re\\) is the same as \\(Rne\\) then \\(RR = 1\\). The \\(RR\\) of perinatal mortality of the new compared to the old method is\n\\[RR = \\frac{5.952381}{10.978520} = 0.5421843 \\approx 0.54\\]\nThe epiDisplay package has a function cs() which automatically calculates the RR and other relevant stats with their confidence intervals. This is applied to the ANCdata as below.\n\n\nCode\ndf_anc %$% epiDisplay::cs(outcome = death, exposure = anc, )\n\n\n\n          Exposure\nOutcome    Non-exposed Exposed Total\n  Negative 373         316     689  \n  Positive 46          20      66   \n  Total    419         336     755  \n                                    \n           Rne         Re      Rt   \n  Risk     0.11        0.06    0.09 \n                                         Estimate Lower95ci Upper95ci\n Risk difference (Re - Rne)              -0.05    -0.09     -0.01    \n Risk ratio                              0.54     0.32      0.91     \n Protective efficacy =(Rne-Re)/Rne*100   45.8     8.71      68.06    \n   or percent of risk reduced                                        \n Number needed to treat (NNT)            19.9     10.79     94.2     \n   or -1/(risk difference)                                           \n\n\nThe output above first tabulates the two variables producing a contingency table with the marginal totals. It then shows our previously calculated parameters, Re and Rne. Rt (Risk total) is the risk if both the exposed and unexposed are put together, here.\n\\[Rt =\\frac{20 + 46}{419 + 336} = \\frac{66}{755} \\approx 0.09\\]\nThe next section of the output shows the risk difference (difference between the risks of the two groups), the risk ratio, the protective efficacy and the number needed to treat (NNT) together with their confidence intervals.\nInterpreting the analysis so far, we conclude that the risk of perinatal death when using the new anc method is significantly less than using the old method. It significantly reduces the risk of death (Risk difference) by 0.05 (5%) and halves the chances of death (RR = 0.54, 95%CI: 0.32 to 0.91). About 20 (95%CI: 11 to 95) pregnant women need to be treated with the new anc method to prevent one perinatal death (NNT).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Risk and Odds</span>"
    ]
  },
  {
    "objectID": "risk-and-odds.html#odds",
    "href": "risk-and-odds.html#odds",
    "title": "12  Risk and Odds",
    "section": "12.3 Odds",
    "text": "12.3 Odds\nAnother way of expressing the risk of an outcome is using the Odds. Statistically the odds is defined as\n\\[Odds = \\frac{p}{1-p}\\]\nWhere p is the probability of the outcome occurring. Using the ANCdata the probability of death in the exposed is.\n\\[pe = \\frac{20}{336} = 0.05952381\\]\nThe odds of death in the exposed can then be determined as\n\\[Oddse = \\frac{0.05952381}{1-0.05952381} = 0.06329114\\]\nSimilarly, the probability of death in the non-exposed (old anc type) is\n\\[pne = \\frac{46}{419} = 0.1097852\\]\nAnd the odds of death in the non-exposed is\n\\[Oddsne = \\frac{0.1097852}{1-0.1097852} = 0.1233244\\]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Risk and Odds</span>"
    ]
  },
  {
    "objectID": "risk-and-odds.html#odds-ratio",
    "href": "risk-and-odds.html#odds-ratio",
    "title": "12  Risk and Odds",
    "section": "12.4 Odds ratio",
    "text": "12.4 Odds ratio\nThe comparative way of comparing the two odds is the Odds Ratio (OR). This is determined as\n\\[OR = \\frac{Oddse}{Oddsne} = 0.5132086 \\approx 0.51\\]\nOnce again fortunately we do not have to go through this tedious procedure each time we need to calculate the OR. The cc() function in the epiDisplay package does this very well. Below we apply it to the analysis just done.\n\n\nCode\ndf_anc %$% epiDisplay::cc(outcome=death, exposure=anc, graph = FALSE)\n\n\n\n       anc\ndeath   Old New Total\n  No    373 316   689\n  Yes    46  20    66\n  Total 419 336   755\n\nOR =  0.51 \n95% CI =  0.3, 0.89  \nChi-squared = 5.9, 1 d.f., P value = 0.015\nFisher's exact test (2-sided) P value = 0.019 \n\n\nThe output shows a table of the variables in question, the OR with its 95% confidence interval and both p-values determine by the chi-squared test and the Fisher’s test. With a confidence interval of the odds ratio not containing the null value 1, and small p-values from both methods it can be concluded that the odds of death in mothers who used the new ANC method is about half (0.5) of those who used the old method and the probability of obtaining an OR this values if the null was true, is low (p-value = 0.019). Therefore, the use of the new anc method is associated with significantly better perinatal outcomes compared to the old.\nOdds ratios are very important in regression analysis and will be dealt with in more detail in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Risk and Odds</span>"
    ]
  },
  {
    "objectID": "normality.html",
    "href": "normality.html",
    "title": "13  Normality",
    "section": "",
    "text": "14 Normality of data\nMany of the test statistical tests, specifically the parametric tests are done on the premise that numeric data is normally distributed. Unfortunately, this is not always so. In this chapter, we look at what is normally distributed data and how we can tell if our data is normally distributed. For this section, we will use the hb variable from the mps.dta data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normality</span>"
    ]
  },
  {
    "objectID": "normality.html#the-normal-distribution",
    "href": "normality.html#the-normal-distribution",
    "title": "13  Normality",
    "section": "14.1 The normal distribution",
    "text": "14.1 The normal distribution\nThe normal distribution, also called the Gaussian Distribution or a bell curve, is defined by two main statistics. These are the mean and standard deviation. The wider the standard deviation, the broader the curve. An example of the normal distribution is shown below:\n\n\nCode\ndf_temp &lt;- data.frame(x = rnorm(2000)) \n\ndf_temp %&gt;% \n    ggplot(aes(x = x))+\n    geom_histogram(\n        aes(y=after_stat(density)), \n        bins=10, fill = \"snow\", col = \"red\") +\n    stat_function(\n        fun = dnorm, \n        args = list(\n            mean = mean(df_temp$x, na.rm=T), \n            sd = sd(df_temp$x)), col = \"blue\",\n            linewidth = 1.5) +\n    labs(x = NULL, y = NULL) +\n    scale_x_continuous(labels = NULL)+\n    scale_y_continuous(labels = NULL)+\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nThese are some features of the normal distribution:\n\nIt is symmetrical.\nThe mean, median and mode are the same.\nApproximately 68% of the data falls within one standard deviation of the mean.\nApproximately 95% of the data falls within two standard deviations of the mean\nApproximately 99.7% of the data fall within three standard deviations of the mean.\n\nThe normal distribution with a mean of 1 and a standard deviation of 1 is called the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normality</span>"
    ]
  },
  {
    "objectID": "normality.html#evaluating-normality",
    "href": "normality.html#evaluating-normality",
    "title": "13  Normality",
    "section": "14.2 Evaluating normality",
    "text": "14.2 Evaluating normality\nThere are two main modalities for evaluating normality. These are graphical and formal hypothesis testing.\n\n14.2.1 Graphical evaluation\nHistogram: Probably the most well-known modality here is the histogram.\nBelow we first read the data and keep only the hb variable:\n\n\nCode\ndf_mps &lt;- haven::read_dta(\"C:/Dataset/mps.dta\")\n\n\nWe then draw the histogram of the hb.\n\n\nCode\ndf_mps %&gt;% \n    drop_na(hb) %&gt;% \n    ggplot(aes(x = hb)) +\n    geom_histogram(bins = 10, fill = 'white', col = \"black\") +\n    labs(title = \"Histogram of HB\", y = \"Frequency\", x = 'Hb (g/dL') +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nThere is near symmetry with a slightly heavier left tail.\nBoxplot: Our next graphical modality is the boxplot as drawn below.\n\n\nCode\ndf_mps %&gt;% \n    drop_na(hb) %&gt;% \n    ggplot(aes(x = hb)) +\n    geom_boxplot(fill = 'white', col = \"black\") +\n    labs(title = \"Boxplot of HB\", y = \"Frequency\", x = 'Hb (g/dL') +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nThe same conclusion of good symmetry and a slightly heavier lower tail is seen here.\nQ-Q plot: Finally, the Q-Q plot with a line. This graphical modality plots the actual values of the data against a theoretical normal distribution. Thus, if all the dots were to be in a straight line and along the line drawn that would be the ideal normal distribution. We therefore use this principle to determine if our data is for instance heavy at the tails, indicating skewness. The Q-Q plot of our data is as done below:\n\n\nCode\ndf_mps %&gt;%\n    drop_na(hb) %&gt;% \n    ggpubr::ggqqplot(x = \"hb\",title = \"Q-Q plot of the HB\", conf.int = FALSE)\n\n\n\n\n\n\n\n\n\nIt is seen that apart from a few points mainly on the right tail the rest pretty much follow the line.\n\n\n14.2.2 Statistical tests for normality\nFormal statistical tests are available for testing. H0: The data is sampled from a normally distributed population. Ha: The data is not sampled from a normally distributed population There are a few of these tests but we will be concentrating on the Shapiro-Wilk tests. It is never advisable to do different tests together as they use different algorithms and may produce different results and conclusions.\nShapiro-Wilk test: Below we perform the Shapiro-wilk test for normality.\n\n\nCode\ndf_mps %$% \n    shapiro.test(hb) %&gt;% \n    broom::tidy()\n\n\n\n\nstatisticp.valuemethod\n\n0.9940.108Shapiro-Wilk normality test\n\n\n\n\nA p-value greater than 0.05 indicates we reject the Null hypothesis and thus conclude our data comes from a normally distributed population.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normality</span>"
    ]
  },
  {
    "objectID": "normality.html#conclusion",
    "href": "normality.html#conclusion",
    "title": "13  Normality",
    "section": "14.3 Conclusion",
    "text": "14.3 Conclusion\nIn conclusion, it can be seen from the graphical presentations as well as the formal test that the data we have is coming from a normally distributed population.\nThe various tests can give contradictory results so I recommend evaluating the normality of a population, one should first plot the histogram, and Q-Q plot, and then perform one formal test, and combine these results before making the the judgement that numeric data is or is not normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Normality</span>"
    ]
  },
  {
    "objectID": "hypothesis-testing.html",
    "href": "hypothesis-testing.html",
    "title": "14  Hypothesis Testing",
    "section": "",
    "text": "14.1 Population and sample",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis-testing.html#population-and-sample",
    "href": "hypothesis-testing.html#population-and-sample",
    "title": "14  Hypothesis Testing",
    "section": "",
    "text": "14.1.1 Population\nA population in a statistical sense is different from the general sense. The population is a collection of items, people, places etc. that the investigator is generally interested in and wants to study. This population tends to be large necessitating the investigator to pick just a representative sample to study a particular property. For example:\n\nTo determine the proportion of Ghanaians who are males we may pick a representative sample to do that but the study population is all “Ghanaians”.\nTo determine the proportion of defective items produced by a factory we sample some of the items but the actual population involves all items produced and possibly those yet to be produced.\n\nTherefore, the statistical definition of a population could include items, places or persons not even born or non-existent.\n\n\n14.1.2 Sample\nThis is part of the population selected by whatever method, usually because the whole population is too large or unavailable to be studied. There are many ways to select the sample from the population. The sample is thus usually smaller than the population.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis-testing.html#descriptive-versus-inferential-statistics",
    "href": "hypothesis-testing.html#descriptive-versus-inferential-statistics",
    "title": "14  Hypothesis Testing",
    "section": "14.2 Descriptive versus Inferential Statistics",
    "text": "14.2 Descriptive versus Inferential Statistics\nIn almost every research the idea is to determine a specific parameter in the population. For instance, to determine the proportion of children under the age of 18 years on a specific flight (e.g. British Airways (BA) from London to Johannesburg) we start by randomly selecting a sample of BA flights between these two destinations. Next, we determine the ages of those on these selected flights and finally come up with the proportion of those with ages less than 18 years. Bear in mind the population includes future flights so the population parameter can rarely be obtained. However, the sample proportion (statistic) determined above could be a good estimate of the population parameter.\nDescriptive statistics involve statistical manipulations done on a specific sample whereas inferential statistics is the manipulation used to estimate the population parameter from the sample statistic. Using the example above, determining the proportion of under-18-year-old persons on our chosen flights falls under descriptive statistics whereas estimating the population proportion of under-18-year-old persons who fly BA along that route from our sample the statistic is by the use of inferential statistics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis-testing.html#sample-variation",
    "href": "hypothesis-testing.html#sample-variation",
    "title": "14  Hypothesis Testing",
    "section": "14.3 Sample variation",
    "text": "14.3 Sample variation\nThe Mid-upper arm circumference of children under five is a measure of how thin a child is and a quick way of determining his/her nutritional status. In a population of 2000 children, the mean and median mid-upper arm circumferences were to be determined independently by a group of 10 students. They decided to take a random sample of 10 children each to estimate these parameters. Each column of students shows the measurements made by each student in his/her sample chosen. We first read the data.\n\n\nCode\ndf_students &lt;- read.delim(\"C:/Dataset/students.txt\", sep = \" \")\n\n\nAnd visualise it\n\n\nCode\ndf_students\n\n\n   X1 X2 X3 X4 X5 X6 X7 X8 X9 X10\n1  14 14 17 16 10 17 15 17  7  14\n2  14 14 15 16 18 11 13  9 13   9\n3  16 15 17  9 13 13 15 15 14  17\n4  14  8 14 15 14  8 18 13 13  17\n5  15 12 17 16 22 17 16 17  8  13\n6  14 18  9 16 14 13 14 15 17  17\n7  17 17 17 20 19 18 18 15  9  15\n8  14 16 16 15 16 17 14 21 15  13\n9  12 17 14 14 10 13 14 11 13  10\n10 11 18 13 19  7 12 16 18 14  14\n\n\nNext, we determine the mean and median values obtained by each student\n\n\nCode\ndf_students %&gt;% \n    summarize(across(X1:X10, ~mean(.)))\n\n\n    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10\n1 14.1 14.9 14.9 15.6 14.3 13.9 15.3 15.1 12.3 13.9\n\n\nIt is obvious that despite all the data coming from the same population the means obtained were different each time. In effect despite using the same population, since choosing a sample is a random process the descriptive statistic(s) obtained each time a sample is chosen from a population will vary. The differences (variation) in the statistics obtained (mean and median MUAC) for instance, with every sample are described as sample variation. If these statistics vary then how can we determine the population parameter? This is what inferential statistics is all about, estimating the population parameter from the sample statistic.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis-testing.html#hypothesis-testing",
    "href": "hypothesis-testing.html#hypothesis-testing",
    "title": "14  Hypothesis Testing",
    "section": "14.4 Hypothesis testing",
    "text": "14.4 Hypothesis testing\nThe collection of scientific data is usually preceded by an idea one wants to prove or disprove. As humans, we often have preconceived ideas or opinions of the expected results of a study. This subconscious thinking is brought to light by formally setting a hypothesis and testing it. This section deals with formalizing the steps involved in this process and how it affects our study design, data collection, analysis, and presentation of results.\n\n14.4.1 Stating the hypothesis.\nStanding in the window one morning Mr Osei wondered (again) if the number of women using the services of the bank next door was the same as that of men. There is a vibrant market nearby which has mainly women trading their wares. The proximity of the market to the bank may have given him this impression. Now he has decided to investigate this. Subconsciously he is wondering if:\n\nThe proportion of men using the services of the bank is the same as that of women.\n\nConversely, he could also be wondering if :\n\nThe proportion of men using banking services is different from that of women.\n\nBear in mind this second line of thinking includes men using the services more compared to women or vice versa. These competing ideas give rise to the following hypothesis:\n\nNull hypothesis (H0): The proportion of men using the services of the bank next door is not different from that of women.\n\nOr alternatively\n\nAlternate Hypothesis (Ha): There is a difference in the proportion of men and women using banking services.\n\nThese two hypotheses describe Mr. Osei’s idea but in an opposing manner. The null and alternate hypotheses can then be tested through a well-designed study. For some statistical and technical reasons, the null hypothesis is often preferred in this regard.\n\n\n14.4.2 Testing the Hypothesis\nAfter the hypothesis has been stated the next objective is to collect evidence (data) to either prove or disprove it. It is obvious that the customers (study population) include future ones and so this assertion can never be completely determined if all customers are to be enumerated. Hence Mr Osei decides to pick a sample of the customers. Out of this sample, he needs to determine if he has enough evidence to disprove his null hypothesis. If he thinks he does he can conclude that\n\n“There is enough evidence to say the proportion of men and women using the bank’s services are not the same”.\n\nIn other words, they are different in proportions. If on the other hand, he does not come up with significant evidence against his null hypothesis he can conclude that\n\n“There is insufficient evidence to conclude the proportion of men using the bank’s services is the same as women”.\n\n\n\n14.4.3 Type I error\nWe recollect from earlier on in this chapter that since there are many ways of choosing a sample from a population the results tend to differ from sample to sample. This we called sample variability. Due to sample variability, sample statistics usually differ from the population parameter.\nIn the example involving Mr. Osei and the bank customers, he proceeded to collect the sexes of 250 systematically selected samples of customers and came up with the following results. There were 112(44.8%) males and 138(55.2%) females in his sample. We note here that this is a sample and by inference, another sample could give an entirely different result (sample variability).\nSo, the question is do we think we have enough evidence to reject H0? Do we think this difference in proportion is significant evidence against the notion that the proportions are the same? We can never be entirely sure if our rejection of H0 is right or wrong. However, we can conclude that the smaller the proportion of males in our obtained sample the higher the chance that the null hypothesis (that the proportions are the same) is false. In hypothesis testing a type I error is said to have been made if the null hypothesis is rejected when in fact it is true. We can therefore say that a type I error is made if H0 is wrongly rejected.\nIn our example above Mr Osei would be making a type I error if he concludes based on the data obtained that the proportions of men and women are not the same when in fact they are in the population.\n\n\n14.4.4 The Significance Level\nThe significance level, usually denoted by alpha (\\(\\alpha\\)) is the probability of making a type I error. It is usually set by the investigator and has a direct bearing on the sample size of a study.\n\n\n14.4.5 Type II error\nConversely, a type II error is said to have been made if the researcher fails to reject the null hypothesis when in fact it is false. Applied to our situation a type II error could be committed if Mr Osei based on his stated result above decides there is insufficient evidence to conclude the proportion of the two sexes differ when in fact they do differ in the population of bank users.\nThe two types of errors mentioned above always increase at the expense of the other. That is as type I error rises type II error falls and vice versa.\n\n\n14.4.6 Power\nThe probability of committing a type II error is called beta (\\(\\beta\\)). The probability of not committing a type II error is called the Power of the test. That is\n\np(Type II error) = \\(\\beta\\) and \\(Power = 1 - \\beta\\)\n\nThe power of a statistical test, therefore, measures its ability to reject the null hypothesis when it is false and hence make the right decision.\n\n\n14.4.7 Critical value\nAnother way of stating the null hypothesis put forward by Mr Osei is\n\nH0: The difference between the proportion of men and women using the services of the bank is zero.\n\nWhen both proportions are 50% the absolute difference (difference disregarding whether it is negative or positive) in proportions will be zero. It can thus be seen that the possible values the absolute difference in proportion can have is from 0% (when there are equal numbers of males and females in the sample) to 100% (when there are either no females or males). This is diagrammatically shown in Figure 21. In this diagram, we have the percentage of men and women on the x-axis with the absolute difference on the y-axis. The V-shaped graph is at its lowest (0%) when the percentages are equal for men and women. On the other hand when either the males are 100% (far right) or females are 100% (far left) the the absolute value rises to 100%. For the results obtained by Mr Osei, the absolute difference between the percentage of men and women is\n\\[55.2\\% - 44.8\\% = 10.4\\%\\] Also, intuitively if there is no difference between the proportion of sexes (i.e. 50% each) then the sample (if randomly selected) difference in proportion is more likely to be near 0 than 100. The nearer it is to 0 the less likely we are to reject the null hypothesis. The nearer it is to 100 the more likely we are to reject the null hypothesis.\nThe next issue arises! At what cut-off value would one decide there is enough evidence to reject the H0? This hypothetical value, often set by the investigator is called the critical value. If Mr. Osei had set a critical value of the difference in proportion of 10% (green horizontal line in the graph) he would have rejected the null hypothesis since 10.4% falls above this line. If on the other hand, he had set a critical value of 12.0% he would have failed to reject H0 as the value would be greater than his observed statistic of 10.4% hence falling below the green line.\n\n\n14.4.8 Critical region\nWhat the critical value does is to divide the range of the test statistic into two possible regions. The region where values obtained will not lead to rejection of H0 (below the green line) and the other where any value obtained would lead to rejection of H0 (above the green line). The critical region is the latter. The former is often called the acceptance region.\nFor Mr. Osei’s example, using a critical value of 10% we can divide the possible values of the test statistic into two regions. 0% to less than 10% and 10% to 100%. The region 10% to 100% is the critical region for his study. If he comes up with any value in this region he would automatically reject the null hypothesis.\n\n\n14.4.9 P-value\nP-values are very well-known in research. It is often misinterpreted and overemphasized. It however has a pivotal place in research and statistical inference. The probability value (p-value) is the probability of having a statistic as extreme as the one observed from the sample if the null hypothesis is true. It determines the strength of support for the null hypothesis. Thus the nearer the p-value is to 1 the better the data at hand or test statistics support the null value. On the other hand the nearer the p-value is to 0 the less the statistic or data supports the null hypothesis. In data analysis, the p-value is often compared to the significance level (\\(\\alpha\\)). If it is less than the significance level the result is said to be statistically significant. In the case of Mr Osei, he chose a significance level of 0.05 however the p-value (after analysis using software) was determined to be 0.1137. That is the probability of obtaining the proportions (112(44.8%) males and 138(55.2%) females) as observed if the null hypothesis (50% vs 50%) is true is 0.1137.\nWe therefore conclude based on a significance level of 0.05 that the results are not statistically significant.\n\n\n14.4.10 Steps in Hypothesis testing\nAfter going through the terms above we are now ready to outline how hypothesis testing is done in statistics.\nThere are 4 basic steps. These are:\n\nThe first step in all hypothesis testing is to state the null hypothesis (H0).\nNext we decide on the significance level (\\(\\alpha\\)). Typically we would use an \\(\\alpha\\) of 0.05 but 0.1 or 0.01 are also sometimes used.\nNext we compute the probability value (p-value). This has been explained above\nFinally, we compare the p-value and the significance level. If the p-value is lower than \\(\\alpha\\) we reject the null hypothesis if not we refuse to reject the null hypothesis.\n\nGenerally, the lower the p-value the more one is confident in rejecting H0. Note that failure to reject H0 does not mean H0 is true. It simply means we don’t have enough evidence to reject it.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis-testing.html#estimation",
    "href": "hypothesis-testing.html#estimation",
    "title": "14  Hypothesis Testing",
    "section": "14.5 Estimation",
    "text": "14.5 Estimation\n\n14.5.1 Point and interval estimates\nPreviously we came across statistics such as mean and proportion used as estimates of the population parameter. These are called point estimates as they usually give a single value as the population estimate. However, another way of determining the population parameter is to provide an interval rather than just a statistic. This is referred to as an interval estimate.\nAnother way Mr. Osei could express his result of the estimate for the proportion of men could be between 38.5% and 51.2%. Thus he is indicating that based on the data available to him he thinks the population estimate of the proportion of men is between 38.5% and 51.2%.\n\n\n14.5.2 Confidence Interval\nIf one has ever read a research journal article he may have come across the phrase confidence interval. This is used to express the uncertainty or precision of the results obtained from a sampling method.\nIn a study to determine the average age of approximately 200,000 factory workers in Ghana, an investigator decided to choose a sample of 200 and determine the mean age. Of course, the sample mean is likely to differ from the population mean because of sample variation. Based on the data the investigator can estimate the interval where the population mean is likely to fall. This estimate is the confidence interval. We can imagine that instead of just one study the same study is done by a hundred persons each choosing 200 workers randomly and then coming up with similar confidence intervals based on their samples. Because of sampling variation, each study is likely to come up with different values for the confidence intervals. The intervals generated by the hundred persons in all 100 studies is as plotted in Figure 22. The green horizontal line is the population mean age.\nThe 95% confidence interval is the confidence interval that is likely to contain the population mean 95% of the time. The estimate provided in Figure 22 is the 95% confidence interval of the individual studies. It can be seen that 5 out of the 100 randomly sampled workers yielded a confidence interval not including the population mean. Similarly, the 99% confidence interval for repeated studies is going to contain the population mean in 99 % of the cases. It is obvious from the above that interpreting the confidence interval in terms of one study can be problematic. However, an intuitive way of understanding the confidence interval is: There is a 95% chance that the 95% confidence interval calculated contains the true population mean. In other words, there is always a 5% chance that the 95% confidence interval generated does not contain the population mean.\nWhat determines the width of the confidence interval?\n\nIt is first determined by the size of the confidence interval. A 99% confidence interval is wider than a 95% confidence interval which in turn is wider than the 90% confidence interval.\nSecondly the smaller the sample size the wider the confidence interval. This is just are ection of the uncertainty in the estimate.\nFinally the variation in the data obtained reacts in the confidence intervals. Populations with elements that are similar will yield a smaller confidence interval compared to one in which the observations are so variable. For instance, the confidence interval of the mean weight of only babies will be much narrower than the confidence interval of the whole population. This is because the weights of the babies are likely to be near each other (less variability) than the weight of the whole population in which there are babies, children, adolescents and adults.\n\nA common mistake is when the population parameter is said to have for instance a 95% probability of lying within the 95% confidence interval. For instance interpreting the 95% confidence interval of 38.5% to 51.2% as the probability that the proportion of men using the bank lies within that range is wrong. This is because the population parameter is a fixed number and has no probability.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "confounding-and-interaction.html",
    "href": "confounding-and-interaction.html",
    "title": "15  Confounding & Interaction",
    "section": "",
    "text": "15.1 Confounding and Interaction",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Confounding & Interaction</span>"
    ]
  },
  {
    "objectID": "confounding-and-interaction.html#confounding-and-interaction",
    "href": "confounding-and-interaction.html#confounding-and-interaction",
    "title": "15  Confounding & Interaction",
    "section": "",
    "text": "15.1.1 Introduction\nA new waiter employed in a bar to serve ice in glasses in Accra made an observation which seem irrational. He realized that customers who he served ice often ended up drunk. He is the first to accept this does not make sense but strongly believe he is right. He approached a friend who is a researcher and they both decided to investigate this. The friend decided to take data on 510 randomly selected customers. This data is recorded in drinks.txt.\nThe variables collected include id the sequentially allocated study id, sex, sex of the customer, liquor whether the customer took an alcoholic liquor (spirits, brandy, etc), ice whether the customer was served ice in a glass, drunk whether the customer ended up being drunk, food whether the customer was served food and age, age in years of the customer. The task now is to determine from the data if ingesting ice is associated with being drunk.\n\n\n15.1.2 Effect and effect size\n\n15.1.2.1 Effect\nBefore we begin to discuss the above problem we first look at what in statistical terms is meant by effect. The effect is defined as a change that occurs as a consequence of an action. In statistical terms, the effect is usually the change in one variable by another. For instance, the effect could be the mean change in blood pressure (effect) after taking a drug (the action). Two of the most common ways of expressing effect in categorical data analysis are the odds ratio (OR) and Relative Risk (RR). These two usually determine the effect by comparing the odds or risk in two groups and expressing the effect as the ratios.\n\n\n15.1.2.2 Effect size\nIt does not only suffice to report the effect but also its size. People would want to know not only if an intervention makes a difference but also how much difference it makes. Effect size can be in the opposite direction. For instance, the difference between the blood pressure after taking a medication may be positive (when the pressure after taking the drug is more than before taking) or negative (when the pressure becomes less when the drug is taken). On the other hand, effects can be on the same side but with a difference in magnitude. An OR of 5.1 is much higher than 1.5 despite both indicating increased odds of having the outcome. We are going to apply this notion in determining confounders and effect modifiers.\nNow back to the waiter’s issue. First, we read the data.\n\n\nCode\ndf_drinks &lt;- \n    read.table(\"C:/Dataset/drinks.txt\", header=T, sep=\"\\t\") %&gt;% \n    mutate(\n        across(\n            c(liquor, ice, drunk, food), \n            ~factor(.x, levels = c(0,1), labels = c(\"No\",\"Yes\"))\n        ),\n        sex = factor(sex)\n    )\n\n\nAnd then summarize the data\n\n\nCode\noptions(huxtable.knit_print_df = FALSE)\ndf_drinks %&gt;% \n    summarytools::dfSummary(graph.col = F)\n\n\nData Frame Summary  \ndf_drinks  \nDimensions: 510 x 7  \nDuplicates: 0  \n\n---------------------------------------------------------------------------------------\nNo   Variable    Stats / Values              Freqs (% of Valid)    Valid      Missing  \n---- ----------- --------------------------- --------------------- ---------- ---------\n1    id          Mean (sd) : 255.5 (147.4)   510 distinct values   510        0        \n     [integer]   min &lt; med &lt; max:            (Integer sequence)    (100.0%)   (0.0%)   \n                 1 &lt; 255.5 &lt; 510                                                       \n                 IQR (CV) : 254.5 (0.6)                                                \n\n2    sex         1. Female                   256 (50.2%)           510        0        \n     [factor]    2. Male                     254 (49.8%)           (100.0%)   (0.0%)   \n\n3    liquor      1. No                       155 (30.4%)           510        0        \n     [factor]    2. Yes                      355 (69.6%)           (100.0%)   (0.0%)   \n\n4    ice         1. No                       191 (37.5%)           510        0        \n     [factor]    2. Yes                      319 (62.5%)           (100.0%)   (0.0%)   \n\n5    drunk       1. No                       186 (36.5%)           510        0        \n     [factor]    2. Yes                      324 (63.5%)           (100.0%)   (0.0%)   \n\n6    age         Mean (sd) : 35.8 (5.1)      31 distinct values    510        0        \n     [integer]   min &lt; med &lt; max:                                  (100.0%)   (0.0%)   \n                 19 &lt; 36 &lt; 51                                                          \n                 IQR (CV) : 7 (0.1)                                                    \n\n7    food        1. No                       288 (56.5%)           510        0        \n     [factor]    2. Yes                      222 (43.5%)           (100.0%)   (0.0%)   \n---------------------------------------------------------------------------------------\n\n\nCode\noptions(huxtable.knit_print_df = TRUE)\n\n\nIn this section, we make use of two main functions cc() and mhor() from the epicalc package. First, we determine the effect of having ice concerning being drunk by using the odds ratio.\n\n\nCode\ndf_drinks %$% epiDisplay::cc(drunk, ice)\n\n\n\n\n\n\n\n\n\n\n       ice\ndrunk    No Yes Total\n  No     85 101   186\n  Yes   106 218   324\n  Total 191 319   510\n\nOR =  1.73 \n95% CI =  1.2, 2.51  \nChi-squared = 8.5, 1 d.f., P value = 0.004\nFisher's exact test (2-sided) P value = 0.004 \n\n\nIt appears the waiter is right! Taking ice is associated with significantly higher odds of getting drunk (OR: 1.73, 95% CI: 1.17 to 2.55, p&lt;.001). This we all agree defies logic. The question then is: How did this arise?\n\n\n\n15.1.3 Confounding\nIn research there often arises the situation where the observed effect of a variable tends to depend on another. For instance, in our case, the effect of ice on making customers drunk could be because it is associated with another item. If customers who took ice also took in some hard liquor at the same time this may make it seem that taking ice is associated with being drunk. This would be a typical case of confounding with the liquor ingestion confounding the effect of ice.\nFor a variable to be a confounder it must meet these basic properties\n\nThe confounder must be related to the exposure (ice) variable\nThe confounder must be related to the outcome (drunk) variable\nThe confounder should not be on the causal pathway between the exposure(ice) and outcome (drunk).\n\nThe first two conditions are easily tested statistically but the last can only be tested with prior knowledge, often coming from epidemiological and scientific facts. The investigator of the bar study suspects liquor to be a confounder to the effect of ice so sets out to determine this. First, he looks out for the relationship between the suspected confounder and the outcome\n\n\nCode\ndf_drinks %$% epiDisplay::cc(drunk, liquor)\n\n\n\n\n\n\n\n\n\n\n       liquor\ndrunk    No Yes Total\n  No     79 107   186\n  Yes    76 248   324\n  Total 155 355   510\n\nOR =  2.41 \n95% CI =  1.63, 3.55  \nChi-squared = 20.2, 1 d.f., P value = 0\nFisher's exact test (2-sided) P value = 0 \n\n\nHaving liquor is strongly associated with being drunk with an OR = 2.41 (95%CI: 1.6, 3.62, p&lt;.001). Next, we test the association between the exposure and the possible confounder.\n\n\nCode\ndf_drinks %$% epiDisplay::cc(ice, liquor, graph = F)\n\n\n\n       liquor\nice      No Yes Total\n  No    120  71   191\n  Yes    35 284   319\n  Total 155 355   510\n\nOR =  13.71 \n95% CI =  8.68, 21.67  \nChi-squared = 151.85, 1 d.f., P value = 0\nFisher's exact test (2-sided) P value = 0 \n\n\nThere is a strong association between having ice and taking hard liquor (OR: 13.71, 95% CI: 8.48 to 22.33, p&lt;.001). With this relationship established the first two conditions stated above have been fulfilled.\nOur next task is to determine if the consumption of liquor is a confounder of the effect of ice. We do this by determining the adjusted OR and comparing it to the crude OR (unadjusted OR). We recall from above that the crude OR is 1.73 (95% CI = 1.17 to 2.55, p=0.004). The Mantel-Haenszel odd ratios determined by the mhor() function is an OR adjusted for the possible confounder. This is shown below\n\n\nCode\ndf_drinks %$% epiDisplay::mhor(drunk, ice, liquor)\n\n\n\nStratified analysis by  liquor \n               OR lower lim. upper lim. P value\nliquor No    1.13      0.497       2.58   0.848\nliquor Yes   1.14      0.621       2.05   0.666\nM-H combined 1.14      0.726       1.78   0.574\n\nM-H Chi2(1) = 0.32 , P value = 0.574 \nHomogeneity test, chi-squared 1 d.f. = 0 , P value = 0.986 \n\n\n\n\n\n\n\n\n\nThe function first stratifies the odds of getting drunk after taking ice into the individual groups of the possible confounding variable. It then determines the OR for each level of the confounder, 1.14(95%CI= 0.621 to 2.05, p=0.666) for those who took liquor and 1.13 (95%CI: 0.497 to 2.58, p=0.848) for those who did not. It then reports the MH combined (adjusted) OR of 1.14 (95%CI: 0.726 to 1.78, p=0.574). Thus stratification or adjustment has reduced the OR or effect of ice from a significant 1.7 to a non-significant relationship 1.1. The effect has all but disappeared as ice is no longer significantly associated with getting drunk after adjustment. This shows that ice on its own does not cause people to be drunk and the observed effect was just an issue of confounding by the consumption of liquor.\nAbove we have shown the relationship between the three variables involved in demonstrating the confounding effect of liquor on the effect of taking ice. We can then ask: Is ice a confounder to the relationship between being drunk and consumption of liquor? Remember the crude OR for this effect is 2.41 (95%CI: 1.6, 3.62, p&lt;.001). To answer the question we determine the adjusted OR below.\n\n\nCode\ndf_drinks %$% epiDisplay::mhor(drunk, liquor, ice)\n\n\n\nStratified analysis by  ice \n               OR lower lim. upper lim.  P value\nice No       2.22       1.16       4.33 0.010714\nice Yes      2.24       1.03       4.86 0.032828\nM-H combined 2.24       1.41       3.56 0.000589\n\nM-H Chi2(1) = 11.81 , P value = 0.001 \nHomogeneity test, chi-squared 1 d.f. = 0 , P value = 0.981 \n\n\n\n\n\n\n\n\n\nThe output shows that the adjusted OR (OR=2.24, 95%CI: 1.41 to 3.56, p&lt;.001) is barely different from the crude OR and also retains its significant effect. The consumption of ice is therefore not a confounder to the effect of hard liquor.\nIn summary to determine if a variable is a confounder we need to determine its association with the exposure and outcome variable. If there is an association we then determine the crude effect and adjusted effect. If these two are significantly different then we can conclude the presence of a confounder.\nHowever, the definition of “significant effect” is always difficult to determine. Some say a change in at least 10% of the crude effect can be considered enough. Whenever there is a possible confounder the adjusted effect should be reported. Also, it is worth noting that effect change could be making the effect higher or lower. A confounder that for instance results in a crude OR of 2.4 and an adjusted OR of 1.5 is a positive confounder while one that has a crude OR of 1.5 but an adjusted OR of 2.5 shows negative confounding.\n\n\n15.1.4 Interaction or Effect modification\nIt is common knowledge among those who drink alcoholic beverages that filling your tummy with food often delays getting drunk when one consumes alcoholic beverages. The investigator of the bar data now wants to determine if this is so. If this notion is right then getting drunk after taking hard liquor should be dependent on whether the customer also ordered and ate some food as well. This is a typical example of effect modification (a term often used by epidemiologists) or interaction (used by statisticians). The two essentially means the same. By definition, effect modification occurs when the effect size of an exposure (liquor) on an outcome (drunk) differs depending on the level of a third variable (eating food). When this occurs just computing and reporting the overall effect is misleading.\nWe begin our investigation for food causing an interaction in the effect of drinking hard liquor by determining the OR of getting drunk when one took some food and when one didn’t.\nWe begin with those who ate some food.\n\n\nCode\ndf_drinks %&gt;% \n    filter(food == \"Yes\") %$% \n    epiDisplay::cc(drunk, liquor)\n\n\n\n\n\n\n\n\n\n\n       liquor\ndrunk    No Yes Total\n  No     47  60   107\n  Yes    41  74   115\n  Total  88 134   222\n\nOR =  1.41 \n95% CI =  0.82, 2.43  \nChi-squared = 1.59, 1 d.f., P value = 0.208\nFisher's exact test (2-sided) P value = 0.219 \n\n\nThere is a statistically insignificant odds of getting drunk after taking hard liquor when one eats as well with an OR 1.41 (95% CI: 0.8 to 2.51, p=0.208). Next, we do the same for those who did not eat\n\n\nCode\ndf_drinks %&gt;% \n    filter(food == \"No\") %$% \n    epiDisplay::cc(drunk, liquor)\n\n\n\n\n\n\n\n\n\n\n       liquor\ndrunk    No Yes Total\n  No     32  47    79\n  Yes    35 174   209\n  Total  67 221   288\n\nOR =  3.38 \n95% CI =  1.9, 6.03  \nChi-squared = 18.13, 1 d.f., P value = 0\nFisher's exact test (2-sided) P value = 0 \n\n\nThe OR of getting drunk if one takes liquor and on an empty stomach is much higher and statistically significant (OR: 3.38, 95% CI: 1.82 to 6.26, p&lt;.001). We seem to have quite a substantial difference in the effect of drinking liquor depending on food intake or not. To be sure if this food intake is a significant effect modifier we subject this to a formal test using the mhor() function.\n\n\nCode\ndf_drinks %$% epiDisplay::mhor(drunk, liquor, food, graph = F)\n\n\n\nStratified analysis by  food \n               OR lower lim. upper lim.  P value\nfood No      3.37      1.817       6.26 4.16e-05\nfood Yes     1.41      0.795       2.51 2.19e-01\nM-H combined 2.08      1.406       3.09 1.78e-04\n\nM-H Chi2(1) = 14.05 , P value = 0 \nHomogeneity test, chi-squared 1 d.f. = 4.65 , P value = 0.031 \n\n\nThe stratified analysis output above shows our prior calculated stratified ORs as well as the combined Mantel-Haenszel OR. The Homogeneity test above tests the null hypothesis:\n\nH0: There is no difference between the two stratified Odds ratios.\n\nA p-value of 0.031, therefore, indicates a statistically significant difference in effects (OR) for those who ate and those who did not. We, therefore, conclude that there is significant interaction between food intake and getting drunk if one takes hard liquor.\nIn this section, we have dealt with detecting a confounder or interaction variable in categorical data analysis. This is just scratching the surface. In subsequent chapters, we would be dealing a lot more with these but using regression analysis.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Confounding & Interaction</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html",
    "href": "diagnostic-tests.html",
    "title": "16  Diagnostics Tests",
    "section": "",
    "text": "16.1 True prevalence of the disease\nThe true prevalence of the disease is the proportion of the diseased individuals observed in the study population as determined by the gold standard. This is mathematically given by \\[True~prevalence = \\frac{tp + fn}{tp + tn + fp + fn}\\]\nAnd determined with our data as\nCode\ntrue.prevalence &lt;- (tp+fn)/(tp+tn+fp+fn)\ntrue.prevalence\n\n\n[1] 0.54",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html#apparent-prevalence-of-the-disease",
    "href": "diagnostic-tests.html#apparent-prevalence-of-the-disease",
    "title": "16  Diagnostics Tests",
    "section": "16.2 Apparent prevalence of the disease",
    "text": "16.2 Apparent prevalence of the disease\nThe apparent prevalence of the disease is the proportion of the diseased individuals observed in the study population as determined by the RDT.1 test. This is mathematically given by \\[Apparent~prevalence = \\frac{tp + fp}{tp + tn + fp + fn}\\] And determined with our data by\n\n\nCode\napparent.prevalence&lt;-(tp+fp)/(tp+tn+fp+fn)\napparent.prevalence\n\n\n[1] 0.52",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html#sensitivity-of-a-test",
    "href": "diagnostic-tests.html#sensitivity-of-a-test",
    "title": "16  Diagnostics Tests",
    "section": "16.3 Sensitivity of a test",
    "text": "16.3 Sensitivity of a test\nThe sensitivity of a test defines as the proportion of individuals with the disease who are correctly identified by the test applied. It ranges from 0, a completely useless test to 1, a perfect test. Mathematically this is defined as\n\\[Sensitivity = \\frac{tp}{tp + fn}\\] And is determined below\n\n\nCode\nsensitivity &lt;- tp/(tp+fn)\nsensitivity\n\n\n[1] 0.9259259",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html#specificity-of-a-test",
    "href": "diagnostic-tests.html#specificity-of-a-test",
    "title": "16  Diagnostics Tests",
    "section": "16.4 Specificity of a test",
    "text": "16.4 Specificity of a test\nThe specificity of a test is defined as the proportion of individuals without the disease who are correctly identified by the test used. It ranges from 0, a completely useless test to 1, a perfect test. Mathematically this is defined as \\[Specificity = \\frac{tn}{tn + fp}\\]\nAnd determine as below\n\n\nCode\nspecificity&lt;-tn/(tn+fp)\nspecificity\n\n\n[1] 0.9565217",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html#predictive-value-of-a-test",
    "href": "diagnostic-tests.html#predictive-value-of-a-test",
    "title": "16  Diagnostics Tests",
    "section": "16.5 Predictive value of a test",
    "text": "16.5 Predictive value of a test\n\n16.5.1 Positive predictive value of a test\nThe positive predictive value (PPV) of a test is defined as the proportion of individuals with a positive test result who have the disease. This is a more useful measure compared to the sensitivity and specificity because it indicates how much weight one has to put on a positive test result when confronted with one. Mathematically it is defined as:\n\\[PPV = \\frac{tp}{tp + fp}\\]\n\n\nCode\nppv &lt;- tp/(tp+fp)\nppv\n\n\n[1] 0.9615385\n\n\n\n\n16.5.2 Negative predictive value of a test\nThe negative predictive value (npv) of a test is defined as the proportion of individuals with a negative test result who do not have the disease. As with the ppv this is a more useful measure compared to the sensitivity and specificity as it indicates how much weight one has to put on a negative test result when confronted with one. Mathematically it is defined as: \\[NPV = \\frac{tn}{tn + fn}\\] And determined below\n\n\nCode\nnpv &lt;- tn/(tn+fn)\nnpv\n\n\n[1] 0.9166667",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html#likelihood-ratio-of-a-test",
    "href": "diagnostic-tests.html#likelihood-ratio-of-a-test",
    "title": "16  Diagnostics Tests",
    "section": "16.6 Likelihood ratio of a test",
    "text": "16.6 Likelihood ratio of a test\nThe likelihood ratio of a test is another way of expressing its usefulness. Unlike the previous statistics about tests, the likelihood ratios stretch beyond 0 to 1. A likelihood ratio of 1 indicates a useless (non-discriminatory) test.\n\n16.6.1 The Positive likelihood ratio (LR+)\nThis is the ratio of the chance of a positive result if the patient has the disease to the chance of a positive result if he does not have the disease. The higher the positive likelihood the better the test.\nThis is mathematically equivalent to\n\\[LR+ = \\frac{Sensitivity}{1-Specificity}\\]\nApplying this to our data so far we have\n\n\nCode\npLR &lt;- sensitivity/(1-specificity)\npLR\n\n\n[1] 21.2963\n\n\n\n\n16.6.2 Negative liklihood ratio (LR-)\nThe negative likelihood ratio (LR-) on the other hand is the ratio of the chance of a person having a negative result having the disease to the chance of a negative result in a person not having the disease. The lower the negative likelihood the better the test.\nComputationally this is equivalent to \\[LR+ = \\frac{1-Sensitivity}{Specificity}\\] Applying this to our data so far we have\n\n\nCode\nnLR&lt;-(1-sensitivity)/specificity\nnLR\n\n\n[1] 0.07744108",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "diagnostic-tests.html#summary",
    "href": "diagnostic-tests.html#summary",
    "title": "16  Diagnostics Tests",
    "section": "16.7 Summary",
    "text": "16.7 Summary\nFortunately, all these can be obtained in one go using the epi.tests() function from the epiRpackage. The function however requires a table formatted in a specific way. Below we create the table\n\n\nCode\ntable.test &lt;- \n    df_malaria %$%\n    table(rdt.1, gold)\n\ntable.test\n\n\n          gold\nrdt.1      Positive Negative\n  Positive       50        2\n  Negative        4       44\n\n\nAnd then we evaluate the test\n\n\nCode\ntable.test %&gt;% epiR::epi.tests()\n\n\n          Outcome +    Outcome -      Total\nTest +           50            2         52\nTest -            4           44         48\nTotal            54           46        100\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.52 (0.42, 0.62)\nTrue prevalence *                      0.54 (0.44, 0.64)\nSensitivity *                          0.93 (0.82, 0.98)\nSpecificity *                          0.96 (0.85, 0.99)\nPositive predictive value *            0.96 (0.87, 1.00)\nNegative predictive value *            0.92 (0.80, 0.98)\nPositive likelihood ratio              21.30 (5.48, 82.77)\nNegative likelihood ratio              0.08 (0.03, 0.20)\nFalse T+ proportion for true D- *      0.04 (0.01, 0.15)\nFalse T- proportion for true D+ *      0.07 (0.02, 0.18)\nFalse T+ proportion for T+ *           0.04 (0.00, 0.13)\nFalse T- proportion for T- *           0.08 (0.02, 0.20)\nCorrectly classified proportion *      0.94 (0.87, 0.98)\n--------------------------------------------------------------\n* Exact CIs\n\n\nConclusion: With the high (all above 0.9) Sensitivity, Specificity, PPV and NPV, the test appears to be a very good one. This is confirmed by the relatively high LR+ and low LR-.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Diagnostics Tests</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "17  Correlation",
    "section": "",
    "text": "We begin by reading in the data and selecting our desired variables\n\n\nCode\nmatDF &lt;- \n    readstata13::read.dta13(\"C:/Dataset/olivia_data_wide.dta\") %&gt;% \n    select(hct1, hct2, hct3, hct4, hct5)\n\n\nNext we summarize the data\n\n\nCode\nsummarytools::dfSummary(matDF, graph.col = F)\n\n\nData Frame Summary  \nmatDF  \nDimensions: 350 x 5  \nDuplicates: 3  \n\n------------------------------------------------------------------------------------\nNo   Variable    Stats / Values           Freqs (% of Valid)    Valid      Missing  \n---- ----------- ------------------------ --------------------- ---------- ---------\n1    hct1        Mean (sd) : 34.2 (5.6)   133 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                               (100.0%)   (0.0%)   \n                 10.5 &lt; 33.7 &lt; 49.9                                                 \n                 IQR (CV) : 7.6 (0.2)                                               \n\n2    hct2        Mean (sd) : 34.2 (5.6)   119 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                               (100.0%)   (0.0%)   \n                 10.2 &lt; 33.2 &lt; 54.9                                                 \n                 IQR (CV) : 7.9 (0.2)                                               \n\n3    hct3        Mean (sd) : 34.8 (6.3)   128 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                               (100.0%)   (0.0%)   \n                 4.3 &lt; 33.2 &lt; 75.5                                                  \n                 IQR (CV) : 8.6 (0.2)                                               \n\n4    hct4        Mean (sd) : 36.8 (7.6)   164 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                               (100.0%)   (0.0%)   \n                 10.4 &lt; 36 &lt; 76.5                                                   \n                 IQR (CV) : 6.4 (0.2)                                               \n\n5    hct5        Mean (sd) : 45.9 (9.5)   186 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                               (100.0%)   (0.0%)   \n                 6.2 &lt; 45.7 &lt; 92.3                                                  \n                 IQR (CV) : 8.6 (0.2)                                               \n------------------------------------------------------------------------------------\n\n\nWe begin by running a correlation coefficient matrix with lower segment shown\n\n\nCode\npsych::lowerCor(matDF, method = \"pearson\")\n\n\n     hct1  hct2  hct3  hct4  hct5 \nhct1  1.00                        \nhct2  0.36  1.00                  \nhct3  0.37  0.30  1.00            \nhct4  0.01 -0.14 -0.04  1.00      \nhct5  0.05  0.06 -0.01 -0.01  1.00\n\n\nAnd then a lot more detail with p-values and confidence interval (Normal)\n\n\nCode\npsych::corr.test(matDF, method = \"pearson\") %&gt;% \n    print(short=F)\n\n\nCall:psych::corr.test(x = matDF, method = \"pearson\")\nCorrelation matrix \n     hct1  hct2  hct3  hct4  hct5\nhct1 1.00  0.36  0.37  0.01  0.05\nhct2 0.36  1.00  0.30 -0.14  0.06\nhct3 0.37  0.30  1.00 -0.04 -0.01\nhct4 0.01 -0.14 -0.04  1.00 -0.01\nhct5 0.05  0.06 -0.01 -0.01  1.00\nSample Size \n[1] 350\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n     hct1 hct2 hct3 hct4 hct5\nhct1 0.00 0.00 0.00 1.00    1\nhct2 0.00 0.00 0.00 0.07    1\nhct3 0.00 0.00 0.00 1.00    1\nhct4 0.86 0.01 0.51 0.00    1\nhct5 0.35 0.25 0.82 0.84    0\n\n Confidence intervals based upon normal theory.  To get bootstrapped values, try cor.ci\n          raw.lower raw.r raw.upper raw.p lower.adj upper.adj\nhct1-hct2      0.26  0.36      0.45  0.00      0.22      0.48\nhct1-hct3      0.28  0.37      0.46  0.00      0.24      0.50\nhct1-hct4     -0.10  0.01      0.11  0.86     -0.10      0.11\nhct1-hct5     -0.05  0.05      0.15  0.35     -0.09      0.19\nhct2-hct3      0.20  0.30      0.39  0.00      0.16      0.43\nhct2-hct4     -0.24 -0.14     -0.03  0.01     -0.28      0.00\nhct2-hct5     -0.04  0.06      0.17  0.25     -0.08      0.20\nhct3-hct4     -0.14 -0.04      0.07  0.51     -0.17      0.10\nhct3-hct5     -0.12 -0.01      0.09  0.82     -0.14      0.12\nhct4-hct5     -0.12 -0.01      0.09  0.84     -0.13      0.11\n\n\nBootstrapped coefficients and confidence interval can be obtained as below\n\n\nCode\npsych::cor.ci(matDF, cex.axis = 2, cex.lab = 3)\n\n\n\n\n\n\n\n\n\nCall:corCi(x = x, keys = keys, n.iter = n.iter, p = p, overlap = overlap, \n    poly = poly, method = method, plot = plot, minlength = minlength, \n    n = n, cex.axis = 2, cex.lab = 3)\n\n Coefficients and bootstrapped confidence intervals \n     hct1  hct2  hct3  hct4  hct5 \nhct1  1.00                        \nhct2  0.36  1.00                  \nhct3  0.37  0.30  1.00            \nhct4  0.01 -0.14 -0.04  1.00      \nhct5  0.05  0.06 -0.01 -0.01  1.00\n\n scale correlations and bootstrapped confidence intervals \n          lower.emp lower.norm estimate upper.norm upper.emp    p\nhct1-hct2      0.25       0.25     0.36       0.47      0.47 0.00\nhct1-hct3      0.29       0.27     0.37       0.48      0.48 0.00\nhct1-hct4     -0.07      -0.08     0.01       0.10      0.08 0.86\nhct1-hct5     -0.09      -0.07     0.05       0.18      0.17 0.41\nhct2-hct3      0.16       0.15     0.30       0.44      0.43 0.00\nhct2-hct4     -0.21      -0.23    -0.14      -0.05     -0.04 0.00\nhct2-hct5     -0.02      -0.03     0.06       0.16      0.15 0.17\nhct3-hct4     -0.18      -0.15    -0.04       0.08      0.07 0.55\nhct3-hct5     -0.09      -0.10    -0.01       0.08      0.08 0.86\nhct4-hct5     -0.12      -0.13    -0.01       0.11      0.09 0.88\n\n\n\n\nCode\nmatDF %&gt;% \n    cor() %&gt;% \n    ggcorrplot::ggcorrplot(hc.order = FALSE, \n           type = \"lower\", \n           lab = TRUE, \n           lab_size = 3, \n           method=\"square\", \n           colors = c(\"tomato2\", \"white\", \"springgreen3\"), \n           title=\"Correlogram of blood indices\", \n           ggtheme=theme_bw)\n\n\n\n\n\n\n\n\n\nGraphically we can use this\n\n\nCode\nmatDF %&gt;% \n    cor() %&gt;% \n    corrplot::corrplot(type = \"lower\", tl.pos = \"ld\",\n                       title = \"Out correlation matrix\", addCoef.col = \"black\",\n                       outline = \"black\", number.cex = .8)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#|mmessage: false\n#|warning: false\nGGally::ggpairs(data = matDF, ggplot2::aes(color = hct1&gt;30))\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Correlation</span>",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html",
    "href": "lin-reg-single-continuous.html",
    "title": "19  Single Continuous Variable",
    "section": "",
    "text": "19.1 Plotting\nWe begin by plotting the distribution of the variables involved\nCode\ndf_blood %&gt;% \n    pivot_longer(cols = c(hb, hct)) %&gt;% \n    ggplot(aes( x = value)) +\n    geom_histogram(bins = 8, fill = \"gold\", color = \"black\")+\n    facet_wrap(facets = \"name\", scales = \"free\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nFigure 19.1: Relationship between hemoglobin and hematocrit\nNext, we plot the relationship between the hctand hb variables and note the linear relationship.\nCode\ndf_blood %&gt;% \n    ggplot(aes(x = hct, y = hb)) +\n    geom_point()+\n    geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)+\n    theme_bw()\n\n\n\n\n\n\n\n\nFigure 19.2: Relationship between hemoglobin and hematocrit",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html#assumptions",
    "href": "lin-reg-single-continuous.html#assumptions",
    "title": "19  Single Continuous Variable",
    "section": "19.2 Assumptions",
    "text": "19.2 Assumptions\n\nLinearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nIndependence: The observations are independent of each other.\nHomoscedasticity: The residuals (errors) have constant variance at every level of X.\nNormality: The residuals of the model are normally distributed.\nNo multicollinearity: This is usually more relevant for multiple regression, but it means the independent variables aren’t too highly correlated with each other.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html#model-fitting",
    "href": "lin-reg-single-continuous.html#model-fitting",
    "title": "19  Single Continuous Variable",
    "section": "19.3 Model fitting",
    "text": "19.3 Model fitting\n\n\nCode\nmodel &lt;- \n    df_blood %&gt;% \n    lm(hb ~ hct, data = .)",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html#visualising-model",
    "href": "lin-reg-single-continuous.html#visualising-model",
    "title": "19  Single Continuous Variable",
    "section": "19.4 Visualising model",
    "text": "19.4 Visualising model\n\n19.4.1 R base summary\n\n\nCode\nmodel %&gt;% summary()\n\n\n\nCall:\nlm(formula = hb ~ hct, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77666 -0.17021  0.02036  0.16771  0.63128 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.314403   0.222547  -1.413    0.164    \nhct          0.347682   0.008925  38.957   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3184 on 48 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.9687 \nF-statistic:  1518 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n19.4.2 tab_model\n\n\nCode\nmodel %&gt;% sjPlot::tab_model() \n\n\n\n\n\n \nhb\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.31\n-0.76 – 0.13\n0.164\n\n\nhct\n0.35\n0.33 – 0.37\n&lt;0.001\n\n\nObservations\n50\n\n\nR2 / R2 adjusted\n0.969 / 0.969\n\n\n\n\n\n\n\n\n\n19.4.3 tidy\n\n\nCode\nmodel %&gt;% broom::tidy() %&gt;% kableExtra::kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.3144029\n0.2225472\n-1.412747\n0.1641817\n\n\nhct\n0.3476823\n0.0089248\n38.956816\n0.0000000\n\n\n\n\n\n\n\n19.4.4 tbl_uvregression\n\n\nCode\ndf_blood %&gt;% \n    gtsummary::tbl_uvregression(\n        y = hb,\n        method = \"lm\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nBeta\n95% CI\n1\np-value\n\n\n\n\nhct\n50\n0.35\n0.33, 0.37\n&lt;0.001\n\n\n\n1\nCI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html#checking-assumptions",
    "href": "lin-reg-single-continuous.html#checking-assumptions",
    "title": "19  Single Continuous Variable",
    "section": "19.5 Checking Assumptions",
    "text": "19.5 Checking Assumptions\nWe see no significant violation of the model assumptions\n\n\nCode\nperformance::check_model(model)\n\n\n\n\n\n\n\n\nFigure 19.3: Model Assumptions of simple linear regression",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html#prediction-interval",
    "href": "lin-reg-single-continuous.html#prediction-interval",
    "title": "19  Single Continuous Variable",
    "section": "19.6 Prediction interval",
    "text": "19.6 Prediction interval\n\n\nCode\nmodel %&gt;% \n    predict(interval = \"predict\") %&gt;% \n    as_tibble() %&gt;% \n    bind_cols(df_blood) %&gt;% \n    ggplot(aes(x = hct, y = hb)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y~x, se=T)+\n    geom_line(aes(y = lwr), col = \"coral2\", linetype = \"dashed\") +\n    geom_line(aes(y = upr), col = \"coral2\", linetype = \"dashed\") +\n    labs(\n        x = \"HCT (%)\", \n        y = \"HB (mg/dl)\", \n        caption = \"Nurse Data 2015\")+\n    theme_bw()\n\n\n\n\n\n\n\n\nFigure 19.4: Relationship between HB4 and HCT4 with fillted line, prediction and se intervals”\n\n\n\n\n\n\n\nCode\nmodel %&gt;% \n    emmeans::emmeans(~hct, at = list(hct = c(20, 25, 30, 35)))\n\n\n hct emmean     SE df lower.CL upper.CL\n  20   6.64 0.0599 48     6.52     6.76\n  25   8.38 0.0453 48     8.29     8.47\n  30  10.12 0.0671 48     9.98    10.25\n  35  11.85 0.1046 48    11.64    12.06\n\nConfidence level used: 0.95",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "lin-reg-single-continuous.html#report",
    "href": "lin-reg-single-continuous.html#report",
    "title": "19  Single Continuous Variable",
    "section": "19.7 Report",
    "text": "19.7 Report\n\n\nCode\nreport::report(model)\n\n\nWe fitted a linear model (estimated using OLS) to predict hb with hct (formula:\nhb ~ hct). The model explains a statistically significant and substantial\nproportion of variance (R2 = 0.97, F(1, 48) = 1517.63, p &lt; .001, adj. R2 =\n0.97). The model's intercept, corresponding to hct = 0, is at -0.31 (95% CI\n[-0.76, 0.13], t(48) = -1.41, p = 0.164). Within this model:\n\n  - The effect of hct is statistically significant and positive (beta = 0.35, 95%\nCI [0.33, 0.37], t(48) = 38.96, p &lt; .001; Std. beta = 0.98, 95% CI [0.93,\n1.04])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Single Continuous Variable</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "23  Linear Regression",
    "section": "",
    "text": "23.1 Acquiring the data\nWe begin by reading the data. Here we use the data from the Carotid Intima dataset\nCode\ndat &lt;- \n  dget(\"C:/Dataset/cint_data_clean\")%&gt;%\n  select(cca_0, sex, ageyrs, resid, hba1c, tobacco, alcohol, bmi, \n         whratio, totchol, ldl, hdl, trig, sbp, dbp)  %&gt;%\n  filter(!is.na(totchol), !is.na(ldl)) %&gt;% \n  mutate(trig = if_else(trig &gt; 40, trig/10, trig, missing = NULL))\nNext, we summarize the data\nCode\ndat %&gt;% \n    summarytools::dfSummary(graph.col = F)\n\n\nData Frame Summary  \ndat  \nDimensions: 702 x 15  \nDuplicates: 4  \n\n-------------------------------------------------------------------------------------\nNo   Variable    Stats / Values            Freqs (% of Valid)    Valid      Missing  \n---- ----------- ------------------------- --------------------- ---------- ---------\n1    cca_0       Mean (sd) : 0.9 (0.2)     40 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 0.5 &lt; 0.8 &lt; 1.6                                                     \n                 IQR (CV) : 0.2 (0.2)                                                \n\n2    sex         1. Female                 545 (77.6%)           702        0        \n     [factor]    2. Male                   157 (22.4%)           (100.0%)   (0.0%)   \n\n3    ageyrs      Mean (sd) : 44.3 (9)      46 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 26 &lt; 43 &lt; 75                                                        \n                 IQR (CV) : 12 (0.2)                                                 \n\n4    resid       1. Rural                   24 ( 3.4%)           702        0        \n     [factor]    2. Periurban              174 (24.8%)           (100.0%)   (0.0%)   \n                 3. Urban                  504 (71.8%)                               \n\n5    hba1c       Mean (sd) : 5.4 (1)       56 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 3.1 &lt; 5.3 &lt; 15.5                                                    \n                 IQR (CV) : 0.8 (0.2)                                                \n\n6    tobacco     1. No                     657 (93.6%)           702        0        \n     [factor]    2. Yes                     45 ( 6.4%)           (100.0%)   (0.0%)   \n\n7    alcohol     1. No                     458 (65.2%)           702        0        \n     [factor]    2. Yes                    244 (34.8%)           (100.0%)   (0.0%)   \n\n8    bmi         Mean (sd) : 26.6 (5.6)    582 distinct values   702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 13.3 &lt; 26.1 &lt; 45.3                                                  \n                 IQR (CV) : 7.7 (0.2)                                                \n\n9    whratio     Mean (sd) : 0.9 (0.1)     456 distinct values   702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 0.6 &lt; 0.9 &lt; 1.2                                                     \n                 IQR (CV) : 0.1 (0.1)                                                \n\n10   totchol     Mean (sd) : 5.1 (1.3)     73 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 1.1 &lt; 5 &lt; 11.1                                                      \n                 IQR (CV) : 1.7 (0.3)                                                \n\n11   ldl         Mean (sd) : 3.2 (1.1)     59 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 0 &lt; 3.1 &lt; 8.1                                                       \n                 IQR (CV) : 1.4 (0.3)                                                \n\n12   hdl         Mean (sd) : 1.4 (0.5)     31 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 0.3 &lt; 1.3 &lt; 4.4                                                     \n                 IQR (CV) : 0.6 (0.4)                                                \n\n13   trig        Mean (sd) : 1.4 (0.9)     227 distinct values   702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 0.3 &lt; 1.2 &lt; 10.7                                                    \n                 IQR (CV) : 0.8 (0.7)                                                \n\n14   sbp         Mean (sd) : 125 (23.9)    118 distinct values   702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 66 &lt; 121 &lt; 231                                                      \n                 IQR (CV) : 28 (0.2)                                                 \n\n15   dbp         Mean (sd) : 79.5 (14.1)   71 distinct values    702        0        \n     [numeric]   min &lt; med &lt; max:                                (100.0%)   (0.0%)   \n                 43 &lt; 79 &lt; 135                                                       \n                 IQR (CV) : 19 (0.2)                                                 \n-------------------------------------------------------------------------------------",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#linear-regression-with-a-single-continuous-variable",
    "href": "linear-regression.html#linear-regression-with-a-single-continuous-variable",
    "title": "23  Linear Regression",
    "section": "23.2 Linear regression with a single continuous variable",
    "text": "23.2 Linear regression with a single continuous variable\nWe begin by looking at the relationship between the dependent and independent variables\n\n\nCode\ndat %&gt;% \n  ggplot(aes(x = ageyrs, y = cca_0)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y~x) +\n  labs(\n      x = \"Age in years\", \n      y = \"Common Carotid Intima thickness\",\n      title = \"Relationship between CCA and Age of patient\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nSince the relationship between the two looks linear we will go on to fit the model\n\n\nCode\nlm.1 &lt;- \n    lm(cca_0 ~ ageyrs, data = dat)\n\n\nNext w,e summarise the model, extract the coefficients and confidence intervals with the help of the flextable package.\n\n\nCode\nlm.1 %&gt;% flextable::as_flextable()\n\n\nEstimateStandard Errort valuePr(&gt;|t|)(Intercept)0.6280.02921.9880.0000***ageyrs0.0050.0018.3810.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05Residual standard error: 0.1511 on 700 degrees of freedomMultiple R-squared: 0.0912, Adjusted R-squared: 0.0899F-statistic: 70.25 on 700 and 1 DF, p-value: 0.0000\n\n\nNext, we extract some regression analysis stats required for the regression diagnostics\n\n\nCode\ntibble(\n    resid = residuals(lm.1), \n    fits = fitted(lm.1), \n    st.resid = rstandard(lm.1),\n    cookd = cooks.distance(lm.1),\n    covr = covratio(lm.1),\n    hatv = hatvalues(lm.1),\n    dfit = dffits(lm.1),\n    dfbeta1 = dfbeta(lm.1)[,2]) %&gt;% \n  arrange(desc(cookd)) %&gt;% \n  round(5) %&gt;% \n  slice(1:10) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresid\nfits\nst.resid\ncookd\ncovr\nhatv\ndfit\ndfbeta1\n\n\n\n\n0.40242\n0.97258\n2.67540\n0.03210\n0.99127\n0.00889\n0.25449\n0.00015\n\n\n0.52507\n0.92493\n3.48188\n0.02314\n0.97212\n0.00380\n0.21685\n0.00011\n\n\n-0.25435\n1.00435\n-1.69521\n0.02018\n1.00862\n0.01385\n-0.20119\n-0.00012\n\n\n0.71213\n0.88787\n4.71760\n0.02012\n0.94181\n0.00180\n0.20372\n0.00006\n\n\n0.53625\n0.81375\n3.55451\n0.01868\n0.96985\n0.00295\n0.19491\n-0.00009\n\n\n0.34330\n0.95670\n2.28006\n0.01800\n0.99487\n0.00688\n0.19033\n0.00011\n\n\n0.19948\n1.02552\n1.33222\n0.01614\n1.01593\n0.01786\n0.17975\n0.00011\n\n\n-0.24376\n0.99376\n-1.62316\n0.01608\n1.00748\n0.01206\n-0.17953\n-0.00011\n\n\n0.35389\n0.94611\n2.34901\n0.01585\n0.99279\n0.00571\n0.17864\n0.00010\n\n\n-0.25287\n0.97787\n-1.68180\n0.01375\n1.00445\n0.00963\n-0.16604\n-0.00010\n\n\n\n\n\nAnd then plot the regression diagnostic graphs\n\n\nCode\nopar &lt;- par(mfrow = c(2,2))\nplot(lm.1, pch = 18, cex=.5)\n\n\n\n\n\n\n\n\n\nCode\npar(opar)\n\n\nBelow we formally check the model assumption with the performance package. First the normality of the residuals\n\n\nCode\nperformance::check_predictions(lm.1)\n\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\n\n\nCode\nperformance::check_normality(lm.1)\n\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nCode\nperformance::check_normality(lm.1) %&gt;% plot()\n\n\n\n\n\n\n\n\n\nNext heteroscedasticity\n\n\nCode\nperformance::check_heteroscedasticity(lm.1)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.018).\n\n\nCode\nperformance::check_heteroscedasticity(lm.1) %&gt;% \n    plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\nperformance::check_distribution(lm.1)\n\n\n# Distribution of Model Family\n\nPredicted Distribution of Residuals\n\n Distribution Probability\n       normal         34%\n         beta         19%\n       cauchy         16%\n\nPredicted Distribution of Response\n\n  Distribution Probability\n inverse-gamma         28%\n         gamma         22%\n          beta         16%\n\n\nCode\nperformance::check_distribution(lm.1) %&gt;% \n    plot()\n\n\n\n\n\n\n\n\n\n\n23.2.1 Linear regression with one continuous and one categorical predictor\n\n\nCode\nlm.2 &lt;- \n    lm(cca_0 ~ ageyrs + sex, data = dat)\n\nlm.2 %&gt;% \n    summary() \n\n\n\nCall:\nlm(formula = cca_0 ~ ageyrs + sex, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35690 -0.11033 -0.01512  0.08711  0.71810 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.6301958  0.0285622  22.064  &lt; 2e-16 ***\nageyrs      0.0051368  0.0006377   8.056 3.42e-15 ***\nsexMale     0.0234025  0.0138148   1.694   0.0907 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1509 on 699 degrees of freedom\nMultiple R-squared:  0.09491,   Adjusted R-squared:  0.09233 \nF-statistic: 36.65 on 2 and 699 DF,  p-value: 7.294e-16\n\n\nAnd then plot the diagnostics\n\n\nCode\nopar &lt;- par(mfrow = c(2,2))\nlm.2 %&gt;% \n    plot(pch = 18, cex=.5)\n\n\n\n\n\n\n\n\n\nCode\npar(opar)\n\n\nFurther, we plot the marginal plot for the various sexes\n\n\nCode\ndat %&gt;% \n  ggplot(aes(x = ageyrs, y = cca_0, col = sex)) +\n  geom_smooth(formula = y~x, method = \"lm\") +\n  geom_point() +\n  labs(\n      x = \"Age in years\", \n      y = \"CCA\", \n      title = \"CCA vrs Age for each sex\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLinear regression with one continuous and one categorical predictor with interaction\n\n\nCode\nlm.3 &lt;- \n    lm(cca_0 ~ ageyrs*sex, data = dat)\n\nlm.3 %&gt;% \n    summary()\n\n\n\nCall:\nlm(formula = cca_0 ~ ageyrs * sex, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35711 -0.10997 -0.01482  0.08664  0.71789 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.6284770  0.0330494  19.016  &lt; 2e-16 ***\nageyrs          0.0051762  0.0007429   6.968 7.45e-12 ***\nsexMale         0.0303114  0.0681101   0.445    0.656    \nageyrs:sexMale -0.0001503  0.0014510  -0.104    0.918    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.151 on 698 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.09104 \nF-statistic:  24.4 on 3 and 698 DF,  p-value: 5.025e-15\n\n\nAnd then plot the diagnostics\n\n\nCode\nopar &lt;- par(mfrow = c(2,2))\nlm.3 %&gt;% \n    plot(pch = 18, cex=.5)\n\n\n\n\n\n\n\n\n\nCode\npar(opar)\n\n\nNext, we make a plot of the marginal effects after\n\n\nCode\npr.3 &lt;- \n    ggeffects::ggpredict(lm.3, terms = c(\"ageyrs\", \"sex\"))\n\npr.3 %&gt;% \n  plot(ci = TRUE, add.data = TRUE) +\n  labs(\n      x = \"Age in years\", \n      y = \"CCA thickness\", \n      title = \"Marginal relationship\")\n\n\nWarning: Argument `add.data` is deprecated and will be removed in the future.\n  Please use `show_data` instead.\n\n\nWarning: Argument `ci` is deprecated and will be removed in the future. Please\n  use `show_ci` instead.\n\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n\n\n\n\n\nLinear regression with one continuous and two categorical predictors with interaction\n\n\nCode\nlm.4 &lt;- \n    lm(cca_0 ~ ageyrs*sex*resid, data = dat)\n\nlm.4 %&gt;% \n    summary()\n\n\n\nCall:\nlm(formula = cca_0 ~ ageyrs * sex * resid, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.36621 -0.10171 -0.01644  0.08202  0.70879 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                    0.434224   0.155773   2.788  0.00546 **\nageyrs                         0.010017   0.003277   3.057  0.00232 **\nsexMale                       -1.476681   1.361101  -1.085  0.27834   \nresidPeriurban                 0.171485   0.167778   1.022  0.30710   \nresidUrban                     0.213439   0.160840   1.327  0.18494   \nageyrs:sexMale                 0.034164   0.031289   1.092  0.27527   \nageyrs:residPeriurban         -0.005066   0.003557  -1.424  0.15476   \nageyrs:residUrban             -0.005046   0.003400  -1.484  0.13821   \nsexMale:residPeriurban         1.661961   1.367568   1.215  0.22468   \nsexMale:residUrban             1.452145   1.363439   1.065  0.28722   \nageyrs:sexMale:residPeriurban -0.035862   0.031413  -1.142  0.25401   \nageyrs:sexMale:residUrban     -0.033653   0.031336  -1.074  0.28322   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1499 on 690 degrees of freedom\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1044 \nF-statistic: 8.427 on 11 and 690 DF,  p-value: 5.047e-14\n\n\nAnd then plot the diagnostics\n\n\nCode\nopar &lt;- par(mfrow = c(2,2))\nlm.4 %&gt;% \n    plot(pch = 18, cex=.5)\n\n\n\n\n\n\n\n\n\nCode\npar(opar)\n\n\nNext, we make a plot of the marginal effects after\n\n\nCode\npr.4 &lt;- ggeffects::ggpredict(lm.4, terms = c(\"ageyrs\", \"sex\", \"resid\"))\npr.4 %&gt;% \n  plot(ci = TRUE, add.data = TRUE, dot.size = .5, dot.alpha = .5) +\n  labs(x = \"Age in years\", y = \"CCA thickness\", title = \"Marginal relationship\")\n\n\nWarning: Argument `add.data` is deprecated and will be removed in the future.\n  Please use `show_data` instead.\n\n\nWarning: Argument `ci` is deprecated and will be removed in the future. Please\n  use `show_ci` instead.\n\n\nWarning: Argument `dot.alpha` is deprecated and will be removed in the future.\n  Please use `dot_alpha` instead.\n\n\nWarning: Argument `dot.size` is deprecated and will be removed in the future.\n  Please use `dot_size` instead.\n\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n\n\n\n\n\nThere is a set of criteria for evaluating linear regression equations. These are:\n\nThere must be a linear relationship between the predictor and dependent variables\nThe residuals must be independent\nHomoscedaticity: The variance of the residuals must be uniform at every level of x\nThe residual must be normally distributed\n\n\n\nCode\ncorr.mat &lt;- \n  dat %&gt;% \n  select(cca_0, ageyrs, whratio, totchol, ldl, sbp, dbp) %&gt;% \n  cor() %&gt;% \n  round(4)\n\ncorr.mat %&gt;% \n  kableExtra::kbl(align = c(rep(\"l\", 7)), caption = \"Correlation matrix\") %&gt;% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"bordered\", \"condensed\", \"hover\", \"responsive\"), \n                            full_width= FALSE, html_font = \"san_serif\", font_size = 16)\n\n\n\n\nCorrelation matrix\n\n\n\ncca_0\nageyrs\nwhratio\ntotchol\nldl\nsbp\ndbp\n\n\n\n\ncca_0\n1.0000\n0.3020\n0.1398\n0.1387\n0.1131\n0.1963\n0.1538\n\n\nageyrs\n0.3020\n1.0000\n0.2962\n0.2196\n0.1563\n0.2755\n0.1782\n\n\nwhratio\n0.1398\n0.2962\n1.0000\n0.1521\n0.1247\n0.1681\n0.1509\n\n\ntotchol\n0.1387\n0.2196\n0.1521\n1.0000\n0.9086\n0.3198\n0.2647\n\n\nldl\n0.1131\n0.1563\n0.1247\n0.9086\n1.0000\n0.2553\n0.2069\n\n\nsbp\n0.1963\n0.2755\n0.1681\n0.3198\n0.2553\n1.0000\n0.7936\n\n\ndbp\n0.1538\n0.1782\n0.1509\n0.2647\n0.2069\n0.7936\n1.0000\n\n\n\n\n\n\n\n\nAnd then graphically present the correlation matrix\n\n\nCode\ndat %&gt;% \n  select(cca_0, ageyrs, whratio, totchol, ldl, sbp, dbp) %&gt;% \n  pairs(pch=\".\")\n\n\n\n\n\n\n\n\n\nCode\ncorrplot::corrplot(corr.mat)\n\n\n\n\n\n\n\n\n\nBoth the correlation matrix and the diagram indicate a high correlation between\n\nsbp and dbp\ntotchol, ldl and hdl\n\nThese could potentially be indicative of multicollinearity",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#fitting-the-model",
    "href": "linear-regression.html#fitting-the-model",
    "title": "23  Linear Regression",
    "section": "23.3 Fitting the model",
    "text": "23.3 Fitting the model\nWe then fit the linear model and summarize it\n\n\nCode\nlm1 &lt;- lm(cca_0 ~ sbp + dbp + ldl + ageyrs + totchol + whratio, data = dat)\nsummary(lm1)\n\n\n\nCall:\nlm(formula = cca_0 ~ sbp + dbp + ldl + ageyrs + totchol + whratio, \n    data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.33971 -0.10555 -0.01751  0.08800  0.68657 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.4697581  0.0744135   6.313 4.88e-10 ***\nsbp         0.0006056  0.0004044   1.498    0.135    \ndbp         0.0002161  0.0006659   0.325    0.746    \nldl         0.0021276  0.0129757   0.164    0.870    \nageyrs      0.0044292  0.0006872   6.446 2.15e-10 ***\ntotchol     0.0035070  0.0104519   0.336    0.737    \nwhratio     0.0895525  0.0839122   1.067    0.286    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1502 on 695 degrees of freedom\nMultiple R-squared:  0.1086,    Adjusted R-squared:  0.1009 \nF-statistic: 14.11 on 6 and 695 DF,  p-value: 3.37e-15\n\n\nThe broom package gives us the opportunity of extracting the coefficient, residual, etc into a dataframe. We next do this below\n\n\nCode\nlm1 %&gt;% broom::tidy()\n\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.470     0.0744       6.31  4.88e-10\n2 sbp         0.000606  0.000404     1.50  1.35e- 1\n3 dbp         0.000216  0.000666     0.325 7.46e- 1\n4 ldl         0.00213   0.0130       0.164 8.70e- 1\n5 ageyrs      0.00443   0.000687     6.45  2.15e-10\n6 totchol     0.00351   0.0105       0.336 7.37e- 1\n7 whratio     0.0896    0.0839       1.07  2.86e- 1\n\n\nCode\nlm1 %&gt;% broom::augment()\n\n\n# A tibble: 702 × 13\n   cca_0   sbp   dbp   ldl ageyrs totchol whratio .fitted  .resid    .hat .sigma\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.925   130    80   4.5     58     6.7   0.9     0.936 -0.0113 0.00677  0.150\n 2 0.975   120    80   2.3     48     4.4   0.924   0.875  0.0996 0.00359  0.150\n 3 0.85    110    80   1.3     40     2.7   0.877   0.822  0.0284 0.00730  0.150\n 4 0.875   120    80   3.5     38     5.1   0.869   0.831  0.0438 0.00301  0.150\n 5 1.6     150   100   3.1     49     4.9   1.01    0.913  0.687  0.00863  0.148\n 6 1.02    130    70   4.2     60     6.3   0.970   0.947  0.0777 0.00986  0.150\n 7 1       106    71   2.1     42     3.8   0.949   0.838  0.162  0.00503  0.150\n 8 1.32    130   100   3.2     55     4.9   0.843   0.913  0.412  0.0130   0.149\n 9 1.1     110    80   3.3     36     5.1   0.814   0.811  0.289  0.00503  0.150\n10 1       110    70   0.7     55     3.3   0.912   0.890  0.110  0.0172   0.150\n# ℹ 692 more rows\n# ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\nCode\nlm1 %&gt;% broom::glance()\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.109         0.101 0.150      14.1 3.37e-15     6   338. -661. -624.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nCode\nlm1 %&gt;% \n  broom::augment() %&gt;% \n  ggplot(aes(x=.resid)) + \n  geom_histogram(fill = \"blue\", col = \"black\", alpha = .4) +\n  theme_classic()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n  broom::augment() %&gt;%\n  ggpubr::ggqqplot(x = \".resid\", col = \"red\", \n                   title = \"QQ plot of the residuals of  the model\") \n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n  plot(1, pch = 16, col = \"skyblue\")\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n  plot(2, pch = 16, col = \"skyblue\")\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n  plot(3, pch = 16, col = \"skyblue\")\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n  plot(4, pch = 16, col = \"skyblue\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#checking-multi-colinearity",
    "href": "linear-regression.html#checking-multi-colinearity",
    "title": "23  Linear Regression",
    "section": "23.4 Checking multi-colinearity",
    "text": "23.4 Checking multi-colinearity\n\n\nCode\ncar::vif(lm1) %&gt;% round(3)\n\n\n    sbp     dbp     ldl  ageyrs totchol whratio \n  2.914   2.725   5.829   1.198   6.155   1.114",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#variable-selection-in-multiple-linear-regression",
    "href": "linear-regression.html#variable-selection-in-multiple-linear-regression",
    "title": "23  Linear Regression",
    "section": "23.5 Variable selection in multiple linear regression",
    "text": "23.5 Variable selection in multiple linear regression\nHere we go through the drill of selecting the best model from a set of predictor variables. We do this with the help of the olsrr package.\n\n23.5.1 Forward regression using the p-values for the selection of the best model\nWe begin by running the model as before\n\n\nCode\nlm1 &lt;- lm(cca_0 ~ sbp + dbp + ldl + ageyrs + totchol + whratio, data = dat)\nsummary(lm1)\n\n\n\nCall:\nlm(formula = cca_0 ~ sbp + dbp + ldl + ageyrs + totchol + whratio, \n    data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.33971 -0.10555 -0.01751  0.08800  0.68657 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.4697581  0.0744135   6.313 4.88e-10 ***\nsbp         0.0006056  0.0004044   1.498    0.135    \ndbp         0.0002161  0.0006659   0.325    0.746    \nldl         0.0021276  0.0129757   0.164    0.870    \nageyrs      0.0044292  0.0006872   6.446 2.15e-10 ***\ntotchol     0.0035070  0.0104519   0.336    0.737    \nwhratio     0.0895525  0.0839122   1.067    0.286    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1502 on 695 degrees of freedom\nMultiple R-squared:  0.1086,    Adjusted R-squared:  0.1009 \nF-statistic: 14.11 on 6 and 695 DF,  p-value: 3.37e-15\n\n\nNext, we perform the forward stepwise selection using a cut-off p.value of 0.1. The table below gives a summary of the selection criteria\n\n\nCode\nfwd.fit.pval &lt;- olsrr::ols_step_forward_p(lm1, penter=0.1)\nfwd.fit.pval\n\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Base Model    -592.088    -582.980    -2584.497    0.00000    0.00000 \n 1      ageyrs        -657.220    -643.558    -2649.447    0.09120    0.08990 \n 2      sbp           -665.999    -647.783    -2658.153    0.10505    0.10249 \n 3      totchol       -665.472    -642.703    -2657.597    0.10692    0.10308 \n 4      whratio       -664.658    -637.334    -2656.749    0.10843    0.10331 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.329       RMSE                  0.149 \nR-Squared               0.108       MSE                   0.022 \nAdj. R-Squared          0.103       Coef. Var            17.374 \nPred R-Squared          0.095       AIC                -664.658 \nMAE                     0.117       SBC                -637.334 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression      1.907          4          0.477    21.192    0.0000 \nResidual       15.676        697          0.022                     \nTotal          17.583        701                                    \n--------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t       Sig      lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.473         0.073                 6.461    0.000     0.330    0.617 \n     ageyrs    0.004         0.001        0.251    6.461    0.000     0.003    0.006 \n        sbp    0.001         0.000        0.106    2.742    0.006     0.000    0.001 \n    totchol    0.005         0.004        0.043    1.132    0.258    -0.004    0.014 \n    whratio    0.091         0.084        0.041    1.085    0.278    -0.073    0.255 \n-------------------------------------------------------------------------------------\n\n\nThe final best-fitting selected model is as below. However, I will run it on the standardized version of our variables as the coefficients are too small\n\n\nCode\ndat2 &lt;-  \n  dat %&gt;% \n  mutate_at(c(\"ageyrs\", \"sbp\"), ~(scale(.) %&gt;% as.vector))\n\nlm.pval &lt;- lm(cca_0 ~ sbp + ageyrs, data = dat2)\ncoefficients(lm.pval)\n\n\n(Intercept)         sbp      ageyrs \n 0.86317664  0.01938752  0.04248649 \n\n\nCode\nlm.pval %&gt;% flextable::as_flextable()\n\n\nEstimateStandard Errort valuePr(&gt;|t|)(Intercept)0.8630.006152.4270.0000***sbp0.0190.0063.2890.0011 **ageyrs0.0420.0067.2070.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05Residual standard error: 0.15 on 699 degrees of freedomMultiple R-squared: 0.105, Adjusted R-squared: 0.1025F-statistic: 41.02 on 699 and 2 DF, p-value: 0.0000\n\n\nCode\nlm.pval %&gt;%\n    broom::tidy(conf.int=T) %&gt;% \n    mutate(across(estimate:conf.high,~round(.x, 3))) %&gt;% \n    flextable::flextable() %&gt;% \n    flextable::font(i=1:3, fontname = \"Times New Roman\") %&gt;% \n    flextable::bold(i=1, j=1:7,part = \"header\") %&gt;% \n    flextable::theme_zebra()\n\n\ntermestimatestd.errorstatisticp.valueconf.lowconf.high(Intercept)0.8630.006152.4270.0000.8520.874sbp0.0190.0063.2890.0010.0080.031ageyrs0.0420.0067.2070.0000.0310.054\n\n\nCode\nlm.pval %&gt;% performance::check_collinearity()\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n   Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n    sbp 1.08 [1.03, 1.23]         1.04      0.92     [0.81, 0.97]\n ageyrs 1.08 [1.03, 1.23]         1.04      0.92     [0.81, 0.97]\n\n\nCode\nlm.pval %&gt;% performance::check_collinearity() %&gt;% plot()\n\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\n\nCode\nlm.pval %&gt;% performance::check_heteroscedasticity()\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.011).\n\n\nCode\nlm.pval %&gt;% performance::check_heteroscedasticity() %&gt;% plot()\n\n\n\n\n\n\n\n\n\nCode\nlm.pval %&gt;% performance::check_normality()\n\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nCode\nlm.pval %&gt;% performance::check_normality() %&gt;% plot() \n\n\n\n\n\n\n\n\n\nCode\nlm.pval %&gt;% performance::check_predictions()\n\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\n\n\n\n\n\n\n\n\nCode\nlm.pval %&gt;% performance::check_predictions() %&gt;% plot()\n\n\n\n\n\n\n\n\n\nCode\nlm.pval %&gt;% performance::check_outliers()\n\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.8).\n- For variable: (Whole model)\n\n\nCode\nlm.pval %&gt;% performance::check_outliers() %&gt;% plot()\n\n\n\n\n\n\n\n\n\nCode\nplot(lm.pval)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23.5.2 Forward regression using the AIC for the selection of the best model\nWe change next to perform the forward stepwise selection using the AIC. The table below gives a summary of the selection criteria\n\n\nCode\nfwd.fit.aic &lt;- \n    olsrr::ols_step_forward_aic(lm1)\nfwd.fit.aic\n\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Base Model    -592.088    -582.980    -2584.497    0.00000    0.00000 \n 1      ageyrs        -657.220    -643.558    -2649.447    0.09120    0.08990 \n 2      sbp           -665.999    -647.783    -2658.153    0.10505    0.10249 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.324       RMSE                  0.150 \nR-Squared               0.105       MSE                   0.023 \nAdj. R-Squared          0.102       Coef. Var            17.382 \nPred R-Squared          0.097       AIC                -665.999 \nMAE                     0.117       SBC                -647.783 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression      1.847          2          0.924    41.023    0.0000 \nResidual       15.736        699          0.023                     \nTotal          17.583        701                                    \n--------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t        Sig     lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.553         0.036                 15.199    0.000    0.482    0.625 \n     ageyrs    0.005         0.001        0.268     7.207    0.000    0.003    0.006 \n        sbp    0.001         0.000        0.122     3.289    0.001    0.000    0.001 \n-------------------------------------------------------------------------------------\n\n\nCode\nplot(fwd.fit.aic)\n\n\n\n\n\n\n\n\n\n\n\n23.5.3 Backward regression using the p.values for the selection of the best model\nWe change next to perform the backward stepwise removal using the p-value. The table below gives a summary of the selection criteria\n\n\nCode\nbwd.fit.pval &lt;- \n    olsrr::ols_step_backward_p(lm1, prem=0.1)\nbwd.fit.pval\n\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Full Model    -660.788    -624.357    -2652.837    0.10860    0.10090 \n 1      ldl           -662.761    -630.883    -2654.831    0.10856    0.10216 \n 2      dbp           -664.658    -637.334    -2656.749    0.10843    0.10331 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.329       RMSE                  0.149 \nR-Squared               0.108       MSE                   0.022 \nAdj. R-Squared          0.103       Coef. Var            17.374 \nPred R-Squared          0.095       AIC                -664.658 \nMAE                     0.117       SBC                -637.334 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression      1.907          4          0.477    21.192    0.0000 \nResidual       15.676        697          0.022                     \nTotal          17.583        701                                    \n--------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t       Sig      lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.473         0.073                 6.461    0.000     0.330    0.617 \n        sbp    0.001         0.000        0.106    2.742    0.006     0.000    0.001 \n     ageyrs    0.004         0.001        0.251    6.461    0.000     0.003    0.006 \n    totchol    0.005         0.004        0.043    1.132    0.258    -0.004    0.014 \n    whratio    0.091         0.084        0.041    1.085    0.278    -0.073    0.255 \n-------------------------------------------------------------------------------------\n\n\n\n\n23.5.4 Backward regression using the aic for the selection of the best model\n\n\nCode\nbwd.fit.aic &lt;- \n    olsrr::ols_step_backward_aic(lm1)\nbwd.fit.aic\n\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Full Model    -660.788    -624.357    -2652.837    0.10860    0.10090 \n 1      ldl           -662.761    -630.883    -2654.847    0.10856    0.10216 \n 2      dbp           -664.658    -637.334    -2656.776    0.10843    0.10331 \n 3      whratio       -665.472    -642.703    -2657.616    0.10692    0.10308 \n 4      totchol       -665.999    -647.783    -2658.163    0.10505    0.10249 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.324       RMSE                  0.150 \nR-Squared               0.105       MSE                   0.023 \nAdj. R-Squared          0.102       Coef. Var            17.382 \nPred R-Squared          0.097       AIC                -665.999 \nMAE                     0.117       SBC                -647.783 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression      1.847          2          0.924    41.023    0.0000 \nResidual       15.736        699          0.023                     \nTotal          17.583        701                                    \n--------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t        Sig     lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.553         0.036                 15.199    0.000    0.482    0.625 \n        sbp    0.001         0.000        0.122     3.289    0.001    0.000    0.001 \n     ageyrs    0.005         0.001        0.268     7.207    0.000    0.003    0.006 \n-------------------------------------------------------------------------------------\n\n\nCode\nplot(bwd.fit.aic)\n\n\n\n\n\n\n\n\n\n\n\n23.5.5 Both direction regression using the p.values for the selection of the best model\nWe change next to perform the backward stepwise removal using the p-value. The table below gives a summary of the selection criteria\n\n\nCode\nboth.fit.pval &lt;- \n    olsrr::ols_step_both_p(\n        lm1, \n        prem = 0.1, \n        penter = 0.1,  \n        progress = TRUE)\n\n\nStepwise Selection Method \n-------------------------\n\nCandidate Terms: \n\n1. sbp \n2. dbp \n3. ldl \n4. ageyrs \n5. totchol \n6. whratio \n\n\nVariables Added/Removed: \n\n=&gt; ageyrs added \n=&gt; sbp added \n\nNo more variables to be added or removed.\n\n\nCode\nboth.fit.pval\n\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Base Model    -592.088    -582.980    -2584.497    0.00000    0.00000 \n 1      ageyrs (+)    -657.220    -643.558    -2649.447    0.09120    0.08990 \n 2      sbp (+)       -665.999    -647.783    -2658.153    0.10505    0.10249 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.324       RMSE                  0.150 \nR-Squared               0.105       MSE                   0.023 \nAdj. R-Squared          0.102       Coef. Var            17.382 \nPred R-Squared          0.097       AIC                -665.999 \nMAE                     0.117       SBC                -647.783 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression      1.847          2          0.924    41.023    0.0000 \nResidual       15.736        699          0.023                     \nTotal          17.583        701                                    \n--------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t        Sig     lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.553         0.036                 15.199    0.000    0.482    0.625 \n     ageyrs    0.005         0.001        0.268     7.207    0.000    0.003    0.006 \n        sbp    0.001         0.000        0.122     3.289    0.001    0.000    0.001 \n-------------------------------------------------------------------------------------\n\n\n\n\n23.5.6 Both direction regression using the aic for the selection of the best model\n\n\nCode\nboth.fit.aic &lt;- \n    olsrr::ols_step_both_aic(lm1, progress = TRUE)\n\n\nStepwise Selection Method \n-------------------------\n\nCandidate Terms: \n\n1. sbp \n2. dbp \n3. ldl \n4. ageyrs \n5. totchol \n6. whratio \n\n\nVariables Added/Removed: \n\n=&gt; ageyrs added \n=&gt; sbp added \n\nNo more variables to be added or removed.\n\n\nCode\nboth.fit.aic\n\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC         R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Base Model    -592.088    -582.980    -2584.497    0.00000    0.00000 \n 1      ageyrs (+)    -657.220    -643.558    -2649.447    0.09120    0.08990 \n 2      sbp (+)       -665.999    -647.783    -2658.153    0.10505    0.10249 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.324       RMSE                  0.150 \nR-Squared               0.105       MSE                   0.023 \nAdj. R-Squared          0.102       Coef. Var            17.382 \nPred R-Squared          0.097       AIC                -665.999 \nMAE                     0.117       SBC                -647.783 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression      1.847          2          0.924    41.023    0.0000 \nResidual       15.736        699          0.023                     \nTotal          17.583        701                                    \n--------------------------------------------------------------------\n\n                                 Parameter Estimates                                  \n-------------------------------------------------------------------------------------\n      model     Beta    Std. Error    Std. Beta      t        Sig     lower    upper \n-------------------------------------------------------------------------------------\n(Intercept)    0.553         0.036                 15.199    0.000    0.482    0.625 \n     ageyrs    0.005         0.001        0.268     7.207    0.000    0.003    0.006 \n        sbp    0.001         0.000        0.122     3.289    0.001    0.000    0.001 \n-------------------------------------------------------------------------------------\n\n\nCode\nplot(both.fit.aic)\n\n\n\n\n\n\n\n\n\n\n\n23.5.7 Pick the best predicting model.\nThe output below is divided into two tables. The first picks out the best 1, 2, 3, etc predictir model. Hence the best 3 predictor model for our linear regression is sbp ageyrs totchol. However, to pick the best model out of the lot, in case 6, we look at the second table. The best will have a high R-Square Adj.R-Square and Pred R-Square. Also, the best model will give the lower C(p), AIC, SBIC, SBC, MSEP, FPE, HSP, and APC. Looking at our output it appears the best will be the 2 predictor mode of lm(cca_0 ~ ageyrs + sbp).\n\n\nCode\nmodcompare &lt;- \n    olsrr::ols_step_best_subset(lm1)\n\nmodcompare\n\n\n             Best Subsets Regression             \n-------------------------------------------------\nModel Index    Predictors\n-------------------------------------------------\n     1         ageyrs                             \n     2         sbp ageyrs                         \n     3         sbp ageyrs totchol                 \n     4         sbp ageyrs totchol whratio         \n     5         sbp dbp ageyrs totchol whratio     \n     6         sbp dbp ldl ageyrs totchol whratio \n-------------------------------------------------\n\n                                                     Subsets Regression Summary                                                      \n-------------------------------------------------------------------------------------------------------------------------------------\n                       Adj.        Pred                                                                                               \nModel    R-Square    R-Square    R-Square     C(p)         AIC          SBIC          SBC        MSEP       FPE       HSP       APC  \n-------------------------------------------------------------------------------------------------------------------------------------\n  1        0.0912      0.0899      0.0858    10.5636    -657.2199    -2649.4469    -643.5581    16.0250    0.0229    0.0000    0.9140 \n  2        0.1050      0.1025      0.0968     1.7667    -665.9991    -2658.1525    -647.7834    15.8035    0.0226    0.0000    0.9026 \n  3        0.1069      0.1031      0.0961     2.3041    -665.4722    -2657.5966    -642.7025    15.7930    0.0226    0.0000    0.9033 \n  4        0.1084      0.1033      0.0952     3.1292    -664.6577    -2656.7487    -637.3341    15.7890    0.0227    0.0000    0.9044 \n  5        0.1086      0.1022      0.0929     5.0269    -662.7610    -2654.8305    -630.8835    15.8093    0.0227    0.0000    0.9068 \n  6        0.1086      0.1009      0.0912     7.0000    -660.7882    -2652.8371    -624.3567    15.8315    0.0228    0.0000    0.9094 \n-------------------------------------------------------------------------------------------------------------------------------------\nAIC: Akaike Information Criteria \n SBIC: Sawa's Bayesian Information Criteria \n SBC: Schwarz Bayesian Criteria \n MSEP: Estimated error of prediction, assuming multivariate normality \n FPE: Final Prediction Error \n HSP: Hocking's Sp \n APC: Amemiya Prediction Criteria",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Linear Regression</span>",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html",
    "href": "anova-oneway.html",
    "title": "24  Oneway ANOVA",
    "section": "",
    "text": "24.1 Background\nOneway analysis of variance is used when there are more than two levels of a predictor variable and one dependent variable.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html#hypothesis",
    "href": "anova-oneway.html#hypothesis",
    "title": "24  Oneway ANOVA",
    "section": "24.2 Hypothesis",
    "text": "24.2 Hypothesis\nH0 - There is no difference in the means for each group\nHa - At least one of the means is significantly different from the others",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html#assumptions",
    "href": "anova-oneway.html#assumptions",
    "title": "24  Oneway ANOVA",
    "section": "24.3 Assumptions",
    "text": "24.3 Assumptions\n\nNormality – Each sample should be drawn from a normally distributed population. It is not required for large sample sizes. If normality is violated use Kruskal-Wallis test\nEqual Variance - The variance of all the groups must be similar. If variances are equal, use ANOVA. If variances are not equal, use the Welch ANOVA\nIndependence - The observation in each group must be independent from each other. They must also come from a random process.\nOutlier - Data should be devoid of significant outliers",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html#data",
    "href": "anova-oneway.html#data",
    "title": "24  Oneway ANOVA",
    "section": "24.4 Data",
    "text": "24.4 Data\nWe look at data from a nursery of newborns, comparing their weights after grouping their 5-minute APGAR scores into Low (0-3), Medium(4-7), and High (8-10).\n\n\nCode\ndf_babies &lt;- \n    readxl::read_xlsx(\"C:/Dataset/mbu2.xlsx\") %&gt;% \n    filter(age == 0 & apgar2 &lt;= 10 & sex != \"Missing\" & wt &lt; 7.5) %&gt;% \n    select(apgar5 = apgar2, bwt = wt, sex) %&gt;% \n    drop_na() %&gt;% \n    mutate(\n        sex = factor(sex),\n        apgar5 = case_when(\n            apgar5 &lt; 4 ~ \"0-3\",\n            apgar5 &lt; 8 ~ \"4-7\",\n            apgar5 &lt;= 10 ~ \"8-10\") %&gt;% \n            factor(levels = c(\"0-3\", \"4-7\", \"8-10\"), ordered = TRUE))\n\ndf_babies %&gt;% \n    summarytools::dfSummary(graph.col = F)\n\n\nData Frame Summary  \ndf_babies  \nDimensions: 3356 x 3  \nDuplicates: 3113  \n\n-----------------------------------------------------------------------------------------\nNo   Variable            Stats / Values         Freqs (% of Valid)   Valid      Missing  \n---- ------------------- ---------------------- -------------------- ---------- ---------\n1    apgar5              1. 0-3                  157 ( 4.7%)         3356       0        \n     [ordered, factor]   2. 4-7                 1438 (42.8%)         (100.0%)   (0.0%)   \n                         3. 8-10                1761 (52.5%)                             \n\n2    bwt                 Mean (sd) : 2.6 (1)    50 distinct values   3356       0        \n     [numeric]           min &lt; med &lt; max:                            (100.0%)   (0.0%)   \n                         0.4 &lt; 2.7 &lt; 5.5                                                 \n                         IQR (CV) : 1.5 (0.4)                                            \n\n3    sex                 1. Female              1510 (45.0%)         3356       0        \n     [factor]            2. Male                1846 (55.0%)         (100.0%)   (0.0%)   \n-----------------------------------------------------------------------------------------\n\n\nData summary and visualization\n\n\nCode\ndf_babies %&gt;% \n    group_by(apgar5) %&gt;% \n    summarise(across(\n        .cols = c(bwt), \n        .fns = list(\n            \"Mean\" = ~mean(.x), \n            \"SD\" = ~sd(.x),\n            \"Variance\" = ~var(.x),\n            \"N\" = ~n()))) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\napgar5\nbwt_Mean\nbwt_SD\nbwt_Variance\nbwt_N\n\n\n\n\n0-3\n2.095541\n1.1222313\n1.2594031\n157\n\n\n4-7\n2.435814\n0.9477166\n0.8981667\n1438\n\n\n8-10\n2.742476\n0.9409445\n0.8853766\n1761\n\n\n\n\n\n\n\nCode\ndf_babies %&gt;% \n    ggplot(aes(y = bwt, x = apgar5)) + \n    geom_boxplot(alpha = .3) +\n    labs(\n        y = \"Birth Weight (kgs)\", \n        x = \"APGAR\",\n        fill = \"APGAR Category\") +\n    theme_light()\n\n\n\n\n\n\n\n\nFigure 24.1: Distriubution of birth weight for various APGAR categories\n\n\n\n\n\n\n\nCode\ndf_babies %&gt;% \n    ggplot(aes(y = bwt, x = apgar5)) + \n    geom_boxplot(alpha = .3) +\n    labs(\n        y = \"Birth Weight (kgs)\", \n        x = \"APGAR\") +\n    theme_light()+\n    theme(\n        strip.text = element_text(face = \"bold\", size = 12),\n        strip.background = element_rect(fill = \"black\"))+\n    facet_wrap(. ~ sex)\n\n\n\n\n\n\n\n\nFigure 24.2: Distriubution of birth weight for various APGAR categories",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html#fit-model",
    "href": "anova-oneway.html#fit-model",
    "title": "24  Oneway ANOVA",
    "section": "24.5 Fit model",
    "text": "24.5 Fit model\nAll data together\n\n\nCode\ndf_babies %&gt;% \n    rstatix::anova_test(bwt ~ apgar5) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\napgar5\n2\n3353\n62.333\n0\n*\n0.036\n\n\n\n\n\nData grouped by sex\n\n\nCode\ndf_babies %&gt;% \n    group_by(sex) %&gt;% \n    rstatix::anova_test(bwt ~ apgar5) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\nsex\nEffect\nDFn\nDFd\nF\np\np&lt;.05\nges\n\n\n\n\nFemale\napgar5\n2\n1507\n35.698\n0\n*\n0.045\n\n\nMale\napgar5\n2\n1843\n32.912\n0\n*\n0.034",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html#mutltiple-comparison",
    "href": "anova-oneway.html#mutltiple-comparison",
    "title": "24  Oneway ANOVA",
    "section": "24.6 Mutltiple comparison",
    "text": "24.6 Mutltiple comparison\nAll data together\n\n\nCode\ndf_babies %&gt;% \n    rstatix::tukey_hsd(bwt ~ apgar5) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ngroup1\ngroup2\nnull.value\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\n\napgar5\n0-3\n4-7\n0\n0.3402722\n0.1524480\n0.5280964\n6.58e-05\n****\n\n\napgar5\n0-3\n8-10\n0\n0.6469345\n0.4608135\n0.8330554\n0.00e+00\n****\n\n\napgar5\n4-7\n8-10\n0\n0.3066622\n0.2272388\n0.3860856\n0.00e+00\n****\n\n\n\n\n\nData grouped by sex\n\n\nCode\ndf_babies %&gt;% \n    group_by(sex) %&gt;% \n    rstatix::tukey_hsd(bwt ~ apgar5) %&gt;%  \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsex\nterm\ngroup1\ngroup2\nnull.value\nestimate\nconf.low\nconf.high\np.adj\np.adj.signif\n\n\n\n\nFemale\napgar5\n0-3\n4-7\n0\n0.1355326\n-0.1319827\n0.4030478\n4.60e-01\nns\n\n\nFemale\napgar5\n0-3\n8-10\n0\n0.5217204\n0.2573289\n0.7861119\n1.19e-05\n****\n\n\nFemale\napgar5\n4-7\n8-10\n0\n0.3861878\n0.2704095\n0.5019661\n0.00e+00\n****\n\n\nMale\napgar5\n0-3\n4-7\n0\n0.5004289\n0.2419560\n0.7589019\n1.77e-05\n****\n\n\nMale\napgar5\n0-3\n8-10\n0\n0.7532432\n0.4966045\n1.0098818\n0.00e+00\n****\n\n\nMale\napgar5\n4-7\n8-10\n0\n0.2528142\n0.1456318\n0.3599967\n1.00e-07\n****",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "anova-oneway.html#checking-assumptions",
    "href": "anova-oneway.html#checking-assumptions",
    "title": "24  Oneway ANOVA",
    "section": "24.7 Checking assumptions",
    "text": "24.7 Checking assumptions\n\n24.7.1 Plotting\nThis is a large sample size but it will be performed for practice\n\n\nCode\ndf_babies %&gt;% \n    aov(bwt ~ apgar5, data = .) %&gt;% \n    performance::check_model()\n\n\n\n\n\n\n\n\n\nThere appear to be significant violations of the assumptions of the model\n\n\n24.7.2 Testing\n\n24.7.2.1 Normality\n\n\nCode\ndf_babies %&gt;% \n    aov(bwt ~ apgar5, data = .) %&gt;% \n    broom::augment() %&gt;% \n    rstatix::shapiro_test(.resid) %&gt;%\n    mystyle()\n\n\nWarning: The `augment()` method for objects of class `aov` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\n\n\n\n\n\n\nvariable\nstatistic\np\n\n\n\n\n.resid\n0.9784008\n3.43482e-22\n\n\n\n\n\n\n\n\n\n24.7.2.2 Equality of variance\n\n\nCode\ndf_babies %&gt;% \n    rstatix::levene_test(bwt ~ apgar5)%&gt;%\n    mystyle()\n\n\n\n\n\n\n\n\ndf1\ndf2\nstatistic\np\n\n\n\n\n2\n3353\n11.77934\n7.983371e-06",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Analysis of Variance</span>",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Oneway ANOVA</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html",
    "href": "log-reg-single-categorical.html",
    "title": "27  Single Categorical Predictor",
    "section": "",
    "text": "27.1 Research question",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#research-question",
    "href": "log-reg-single-categorical.html#research-question",
    "title": "27  Single Categorical Predictor",
    "section": "",
    "text": "What is the association between death and the fifth-minute APGAR Score categories?\nWhat are the predicted probabilities of death for the various fifth minute APGAR score categories?\nDo these probabilities differ significantly from each other?",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#graphing-variables",
    "href": "log-reg-single-categorical.html#graphing-variables",
    "title": "27  Single Categorical Predictor",
    "section": "27.2 Graphing variables",
    "text": "27.2 Graphing variables\n\n\nCode\nbabies %&gt;% \n    group_by(apgar5cat, died) %&gt;% \n    summarize(count = n(), .groups = \"drop\") %&gt;% \n    group_by(apgar5cat) %&gt;% \n    mutate(perc = count/sum(count)) %&gt;% \n    ggplot(\n        aes(\n            x = apgar5cat, \n            y = count, \n            fill = died, \n            label = paste0(count, \"\\n (\", scales::percent(perc), \")\"))) +\n    geom_bar(stat = \"identity\", position = position_dodge())+\n    labs(x = \"Five minute APGAR\", y = \"Frequency\", fill = \"Mortality\")+\n    geom_text(\n        vjust = -.25, \n        color= \"black\", \n        size = 3, \n        fontface=\"italic\", \n        position = position_dodge(width = 1))+\n    ylim(c(0, 6000)) +\n    scale_fill_manual(values = c(\"#70161E\",\"#A4B6AC\"))+\n    theme_bw()\n\n\n\n\n\n\n\n\nFigure 27.1: Barplot of Five minute APGAR categories and Mortality\n\n\n\n\n\nAlternatively, we can use the fifth-minute\n\n\nCode\nbabies %$% \n    sjPlot::plot_xtab(\n        grp = died, x = apgar5cat, margin = \"row\", \n        bar.pos = \"stack\", show.summary = TRUE, \n        coord.flip = TRUE)+\n    theme_bw()\n\n\n\n\n\n\n\n\nFigure 27.2: Distribution of Five minute APGAR categories and Mortality",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#regression",
    "href": "log-reg-single-categorical.html#regression",
    "title": "27  Single Categorical Predictor",
    "section": "27.3 Regression",
    "text": "27.3 Regression\n\n\nCode\nmodel &lt;- \n    babies %&gt;% \n    glm(died ~ apgar5cat, family = binomial, data = .)\n\nsummary(model)\n\n\n\nCall:\nglm(formula = died ~ apgar5cat, family = binomial, data = .)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.6543     0.1135   5.765 8.17e-09 ***\napgar5catMedium  -1.4886     0.1236 -12.041  &lt; 2e-16 ***\napgar5catHigh    -2.9693     0.1222 -24.295  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7382.2  on 8257  degrees of freedom\nResidual deviance: 6453.1  on 8255  degrees of freedom\nAIC: 6459.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCode\nbabies %&gt;% \n    select(died, apgar5cat) %&gt;% \n    gtsummary::tbl_uvregression(\n        method = glm,\n        y = died,\n        method.args = family(binomial),\n        exponentiate = TRUE,\n        pvalue_fun = function(x) gtsummary::style_pvalue(x, digits = 3)) %&gt;% \n    gtsummary::modify_header(\n            estimate ~ \"**OR**\",\n            label ~ \"**Variable**\") %&gt;%\n    gtsummary::bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nFifth minute APGAR Category\n8,258\n\n\n\n\n\n\n\n\n    Low\n\n\n—\n—\n\n\n\n\n    Medium\n\n\n0.23\n0.18, 0.29\n&lt;0.001\n\n\n    High\n\n\n0.05\n0.04, 0.07\n&lt;0.001\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#model-evaluation",
    "href": "log-reg-single-categorical.html#model-evaluation",
    "title": "27  Single Categorical Predictor",
    "section": "27.4 Model evaluation",
    "text": "27.4 Model evaluation\n\n\nCode\nmodel %&gt;% performance::check_model(residual_type = \"normal\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#predicted-probabilities",
    "href": "log-reg-single-categorical.html#predicted-probabilities",
    "title": "27  Single Categorical Predictor",
    "section": "27.5 Predicted probabilities",
    "text": "27.5 Predicted probabilities\n\n\nCode\nmodel %&gt;% \n    ggeffects::ggeffect() %&gt;% \n    as_tibble() %&gt;% \n    unnest(cols = c(apgar5cat)) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\nx\npredicted\nstd.error\nconf.low\nconf.high\ngroup\n\n\n\n\nLow\n0.6579710\n0.1134895\n0.6063106\n0.7061381\napgar5cat\n\n\nMedium\n0.3027383\n0.0490134\n0.2828524\n0.3233919\napgar5cat\n\n\nHigh\n0.0898839\n0.0453608\n0.0828713\n0.0974268\napgar5cat",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#plotting-effects-predicted-probabilities",
    "href": "log-reg-single-categorical.html#plotting-effects-predicted-probabilities",
    "title": "27  Single Categorical Predictor",
    "section": "27.6 Plotting effects & predicted probabilities",
    "text": "27.6 Plotting effects & predicted probabilities\n\n\nCode\nmodel %&gt;% \n    sjPlot::plot_model(type = \"pred\", terms = \"apgar5cat\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel %&gt;% \n    sjPlot::plot_model(\n        type = \"est\", \n        show.p = TRUE, \n        show.values = T, \n        show.intercept = T) +\n    theme_bw()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "log-reg-single-categorical.html#pairwise-comparison",
    "href": "log-reg-single-categorical.html#pairwise-comparison",
    "title": "27  Single Categorical Predictor",
    "section": "27.7 Pairwise comparison",
    "text": "27.7 Pairwise comparison\n\n\nCode\nbabies %&gt;% \n    select(died, apgar5cat) %&gt;% \n    gtsummary::tbl_uvregression(\n        method = glm,\n        y = died,\n        method.args = family(binomial),\n        exponentiate = TRUE,\n        pvalue_fun = function(x) gtsummary::style_pvalue(x, digits = 3),\n        add_pairwise_contrast = TRUE,\n        contrast_adjust = \"holm\") %&gt;% \n    gtsummary::modify_header(\n            estimate ~ \"**OR**\",\n            label ~ \"**Variable**\") %&gt;%\n    gtsummary::bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nFifth minute APGAR Category\n8,258\n\n\n\n\n\n\n\n\n    Medium / Low\n\n\n0.23\n0.17, 0.30\n&lt;0.001\n\n\n    High / Low\n\n\n0.05\n0.04, 0.07\n&lt;0.001\n\n\n    High / Medium\n\n\n0.23\n0.19, 0.27\n&lt;0.001\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Single Categorical Predictor</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html",
    "href": "logistic-regression.html",
    "title": "32  Logistic Regression",
    "section": "",
    "text": "32.1 Reading and visualizing data\nFirst, we read the data and visualize it\nCode\nlibrary(tidyverse)\nadm &lt;- \n    read.table(\"C:/Dataset/Admissions.txt\", header = TRUE) %&gt;% \n    select(-x) %&gt;% \n    tibble()\n\nadm %&gt;% \n    summarytools::dfSummary(graph.col = F)\n\n\nData Frame Summary  \nadm  \nDimensions: 400 x 4  \nDuplicates: 5  \n\n---------------------------------------------------------------------------------------\nNo   Variable    Stats / Values              Freqs (% of Valid)    Valid      Missing  \n---- ----------- --------------------------- --------------------- ---------- ---------\n1    admit       Min  : 0                    0 : 273 (68.2%)       400        0        \n     [integer]   Mean : 0.3                  1 : 127 (31.8%)       (100.0%)   (0.0%)   \n                 Max  : 1                                                              \n\n2    gmat        Mean (sd) : 587.7 (115.5)   26 distinct values    400        0        \n     [integer]   min &lt; med &lt; max:                                  (100.0%)   (0.0%)   \n                 220 &lt; 580 &lt; 800                                                       \n                 IQR (CV) : 140 (0.2)                                                  \n\n3    gpa         Mean (sd) : 3.4 (0.4)       132 distinct values   400        0        \n     [numeric]   min &lt; med &lt; max:                                  (100.0%)   (0.0%)   \n                 2.3 &lt; 3.4 &lt; 4                                                         \n                 IQR (CV) : 0.5 (0.1)                                                  \n\n4    rank        Mean (sd) : 2.5 (0.9)       1 :  61 (15.2%)       400        0        \n     [integer]   min &lt; med &lt; max:            2 : 151 (37.8%)       (100.0%)   (0.0%)   \n                 1 &lt; 2 &lt; 4                   3 : 121 (30.2%)                           \n                 IQR (CV) : 1 (0.4)          4 :  67 (16.8%)                           \n---------------------------------------------------------------------------------------",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#assumptions-for-a-logistic-regression",
    "href": "logistic-regression.html#assumptions-for-a-logistic-regression",
    "title": "32  Logistic Regression",
    "section": "32.2 Assumptions for a logistic regression",
    "text": "32.2 Assumptions for a logistic regression\n\nCases are randomly sampled\nData free of bivariate or multivariate outliers\nThe outcome variable is dichotomous\nThe association between the continuous predictor and logit transformation is linear\nModel-free of collinearity",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#building-the-model",
    "href": "logistic-regression.html#building-the-model",
    "title": "32  Logistic Regression",
    "section": "32.3 Building the model",
    "text": "32.3 Building the model\nThen we build a logistic regression model using the rank variable as a numeric variable\n\n\nCode\nmod.1 &lt;- \n    glm(admit ~ .,  data = adm, family = \"binomial\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#visualizing-the-model-and-its-properties",
    "href": "logistic-regression.html#visualizing-the-model-and-its-properties",
    "title": "32  Logistic Regression",
    "section": "32.4 Visualizing the model and its properties",
    "text": "32.4 Visualizing the model and its properties\nA summary of the model can be visualized from the summary() function. Next, we use the broom package to display various properties of the model in a tabular form. These can then be used for further analysis.\n\n\nCode\nmod.1 %&gt;% summary()\n\n\n\nCall:\nglm(formula = admit ~ ., family = \"binomial\", data = adm)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.449548   1.132846  -3.045  0.00233 ** \ngmat         0.002294   0.001092   2.101  0.03564 *  \ngpa          0.777014   0.327484   2.373  0.01766 *  \nrank        -0.560031   0.127137  -4.405 1.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 459.44  on 396  degrees of freedom\nAIC: 467.44\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\nmod.1 %&gt;% broom::glance()\n\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          500.     399  -230.  467.  483.     459.         396   400\n\n\nCode\nmod.1 %&gt;% \n    broom::augment() %&gt;% \n    arrange(desc(.cooksd)) %&gt;% \n    head(10) %&gt;% \n    gt::gt()\n\n\n\n\n\n\n\n\nadmit\ngmat\ngpa\nrank\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n300\n2.84\n2\n-1.6747048\n1.921687\n0.015824364\n1.074078\n0.02179898\n1.937074\n\n\n1\n400\n3.23\n4\n-2.2623363\n2.173188\n0.008445841\n1.072887\n0.02062862\n2.182424\n\n\n1\n580\n2.86\n4\n-2.1369186\n2.120602\n0.009038879\n1.073152\n0.01949815\n2.130251\n\n\n1\n680\n2.42\n1\n-0.5693145\n1.426733\n0.039553560\n1.076001\n0.01894216\n1.455815\n\n\n1\n680\n3.00\n4\n-1.7987408\n1.975802\n0.012174862\n1.073843\n0.01884634\n1.987941\n\n\n1\n480\n3.71\n4\n-1.7058530\n1.935323\n0.011479446\n1.074035\n0.01617082\n1.946528\n\n\n1\n560\n2.65\n3\n-1.7859393\n1.970240\n0.010212491\n1.073878\n0.01554574\n1.980379\n\n\n1\n340\n3.00\n2\n-1.4586242\n1.826316\n0.013837152\n1.074514\n0.01529544\n1.839084\n\n\n1\n520\n2.68\n3\n-1.8543872\n1.999914\n0.008953886\n1.073744\n0.01455841\n2.008928\n\n\n1\n520\n3.74\n4\n-1.5907842\n1.884802\n0.011106681\n1.074267\n0.01393459\n1.895357\n\n\n\n\n\n\n\nThe maximum Cook’s Distance of 0.02 (&lt;0.04) is not very different from the rest and can therefore be said not to have outliers.\nNext, we check for linear association by applying the Box Tidwell test\n\n\nCode\nglm(admit ~ gmat + gmat:log(gmat) + gpa  + gpa:log(gpa) + rank + \n        rank:log(rank), data = adm, family = \"binomial\") %&gt;% \n    broom::tidy()\n\n\n# A tibble: 7 × 5\n  term           estimate std.error statistic p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     0.913    16.0        0.0572   0.954\n2 gmat            0.0265    0.0626     0.423    0.672\n3 gpa            -2.71     10.3       -0.264    0.792\n4 rank           -1.39      1.04      -1.34     0.180\n5 gmat:log(gmat) -0.00328   0.00848   -0.387    0.699\n6 gpa:log(gpa)    1.57      4.64       0.339    0.734\n7 rank:log(rank)  0.456     0.562      0.811    0.417\n\n\nNone of the variables has a significant interaction and hence the linearity between the variable and the logit of the outcome can be assumed.\nResiduals are referred to as deviant residuals. Also, we have out beta estimates and significance (p-values). Null deviance is a measure of error if you estimate only the model with the intercept term and not the x variable at all at the right. So we compare the value of the Residual deviance to the Null deviance. Also, we can use the AIC, smaller is better here.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#plotting-coefficients",
    "href": "logistic-regression.html#plotting-coefficients",
    "title": "32  Logistic Regression",
    "section": "32.5 Plotting coefficients",
    "text": "32.5 Plotting coefficients\nThe coefficient of the regression can be plotted using the coefplotpackage. This is illustrated below\n\n\nCode\ncoefplot::coefplot(\n    mod.1, \n    predictors=c(\"gpa\", \"rank\", \"gmat\"), \n    guide = \"none\", innerCI = 2, \n    outerCI=0, \n    title = \"Coefficient Plot of Model 1\", \n    ylab = \"Predictors\",\n    decreasing = FALSE,  \n    newNames = c(\n        gpa = \"Grade Point Avg.\", \n        rank = \"Rank of School\",\n        gmat = \"GMAT Score\")) + \n  theme_light()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#predictions-for-the-model---extracting-and-plotting",
    "href": "logistic-regression.html#predictions-for-the-model---extracting-and-plotting",
    "title": "32  Logistic Regression",
    "section": "32.6 Predictions for the model - Extracting and plotting",
    "text": "32.6 Predictions for the model - Extracting and plotting\nPredictions from the model can be obtained with the effects package. This is illustrated below. First, we predict the model using the gpa, convert it to a tibble, plot the predicted probabilities and finally compare the plotted probabilities for the persons ranked as 1 to 5.\n\n\nCode\nggeffects::ggpredict(model = mod.1, terms = c(\"gpa[all]\"))\n\n\n# Predicted probabilities of admit\n\n gpa | Predicted |     95% CI\n-----------------------------\n2.26 |      0.18 | 0.09, 0.33\n2.78 |      0.25 | 0.18, 0.35\n2.97 |      0.28 | 0.21, 0.36\n3.14 |      0.31 | 0.25, 0.38\n3.31 |      0.34 | 0.29, 0.40\n3.48 |      0.37 | 0.31, 0.43\n3.64 |      0.40 | 0.33, 0.47\n4.00 |      0.47 | 0.36, 0.58\n\nAdjusted for:\n* gmat = 580.00\n* rank =   2.00\n\n\n\nNot all rows are shown in the output. Use `print(..., n = Inf)` to show\n  all rows.\n\n\nCode\nggeffects::ggpredict(model = mod.1, terms = c(\"gpa[all]\")) %&gt;% \n    tibble()\n\n\n# A tibble: 132 × 6\n       x predicted std.error conf.low conf.high group\n   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;\n 1  2.26     0.185     0.395   0.0947     0.330 1    \n 2  2.42     0.204     0.346   0.115      0.336 1    \n 3  2.48     0.212     0.328   0.124      0.338 1    \n 4  2.52     0.217     0.315   0.130      0.340 1    \n 5  2.55     0.221     0.306   0.135      0.341 1    \n 6  2.56     0.223     0.303   0.136      0.342 1    \n 7  2.62     0.231     0.286   0.146      0.344 1    \n 8  2.63     0.232     0.283   0.148      0.345 1    \n 9  2.65     0.235     0.277   0.152      0.346 1    \n10  2.67     0.238     0.271   0.155      0.347 1    \n# ℹ 122 more rows\n\n\nCode\nggeffects::ggpredict(model = mod.1, terms = c(\"gpa[all]\")) %&gt;% \n    plot()\n\n\n\n\n\n\n\n\n\nCode\nglm(admit ~ gpa*rank + gmat,  data = adm, family = \"binomial\") %&gt;% \n    ggeffects::ggpredict(terms = c(\"gpa[all]\", \"rank[2,4]\")) %&gt;% \n    plot()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#comparing-two-nested-models",
    "href": "logistic-regression.html#comparing-two-nested-models",
    "title": "32  Logistic Regression",
    "section": "32.7 Comparing two nested models",
    "text": "32.7 Comparing two nested models\nWe can compare the two models using the anova function in R. The first one is the Null Mode and the other includes the gpa variable. This we do with the analysis of the deviance table as below. Remember these must be nested in each other.\n\n\nCode\nfirst_model &lt;- \n    glm(admit ~ 1,  data = adm, family = \"binomial\")\nsecond_model &lt;- \n    glm(admit ~ gpa,  data = adm, family = \"binomial\")\nanova(first_model, second_model, test = \"Chisq\") %&gt;% \n    broom::tidy() %&gt;% \n    gt::gt() %&gt;% \n    gt::opt_stylize(style = 6, color = \"gray\")\n\n\n\n\n\n\n\n\nterm\ndf.residual\nresidual.deviance\ndf\ndeviance\np.value\n\n\n\n\nadmit ~ 1\n399\n499.9765\nNA\nNA\nNA\n\n\nadmit ~ gpa\n398\n486.9676\n1\n13.0089\n0.0003100148\n\n\n\n\n\n\n\nCode\nrm(first_model, second_model)\n\n\nResults indicate a significant improvement between the Null model and the one with the gpa as a predictor\n\n32.7.1 ROC curves for model\nNext, we compute the predicted probabilities of being admitted for each individual. And then generate ROC curves after we re-categorize standard error of the Rank variable into High and Low.\n\n\nCode\ndata.frame(\n    adm = adm$admit, \n    pred = mod.1$fitted.values, \n    rank2 = ifelse(adm$rank &gt; 2, \"High\", \"Low\") %&gt;% \n        as.factor()) %&gt;% \n    arrange(adm) %&gt;% \n    ggplot(aes(d=adm, m=pred, col=rank2))+ \n    plotROC::geom_roc(n.cuts = 5) + \n    geom_segment(\n        x=0, y=0, xend=1, yend=1, col=\"black\", lty = \"solid\", \n        linewidth = 0.7) +\n    labs(\n        title = \"ROC for the various groups of Ranks\",\n        x = \"1 - Sepcificity\",\n        y = \"Sensitivity\",\n        col = \"Rank\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nRank is considered as a numeric variable but it is a categorical one so we convert it to one below\n\n\nCode\nadm &lt;- \n    adm %&gt;% \n    mutate(catrank = factor(rank))\nadm\n\n\n# A tibble: 400 × 5\n   admit  gmat   gpa  rank catrank\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;  \n 1     0   380  3.61     3 3      \n 2     1   660  3.67     3 3      \n 3     1   800  4        1 1      \n 4     1   640  3.19     4 4      \n 5     0   520  2.93     4 4      \n 6     1   760  3        2 2      \n 7     1   560  2.98     1 1      \n 8     0   400  3.08     2 2      \n 9     1   540  3.39     3 3      \n10     0   700  3.92     2 2      \n# ℹ 390 more rows\n\n\nAnd then create a second model\n\n\nCode\nmod.2 &lt;- \n    glm(admit ~ gmat + gpa + catrank,  data = adm, family = \"binomial\")\n\nmod.2 %&gt;% \n    summary()\n\n\n\nCall:\nglm(formula = admit ~ gmat + gpa + catrank, family = \"binomial\", \n    data = adm)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.989979   1.139951  -3.500 0.000465 ***\ngmat         0.002264   0.001094   2.070 0.038465 *  \ngpa          0.804038   0.331819   2.423 0.015388 *  \ncatrank2    -0.675443   0.316490  -2.134 0.032829 *  \ncatrank3    -1.340204   0.345306  -3.881 0.000104 ***\ncatrank4    -1.551464   0.417832  -3.713 0.000205 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 499.98  on 399  degrees of freedom\nResidual deviance: 458.52  on 394  degrees of freedom\nAIC: 470.52\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe note a significant change in our Null Deviance and the residual deviance as well as a small AIC.\nNext, we build some confidence intervals using the profile log-likelihood.\n\n\nCode\nmod.2 %&gt;% confint()\n\n\nWaiting for profiling to be done...\n\n\n                    2.5 %       97.5 %\n(Intercept) -6.2716202334 -1.792547080\ngmat         0.0001375921  0.004435874\ngpa          0.1602959439  1.464142727\ncatrank2    -1.3008888002 -0.056745722\ncatrank3    -2.0276713127 -0.670372346\ncatrank4    -2.4000265384 -0.753542605\n\n\nAnd then using the standard errors\n\n\nCode\nmod.2 %&gt;% \n    confint.default()\n\n\n                    2.5 %       97.5 %\n(Intercept) -6.2242418514 -1.755716295\ngmat         0.0001202298  0.004408622\ngpa          0.1536836760  1.454391423\ncatrank2    -1.2957512650 -0.055134591\ncatrank3    -2.0169920597 -0.663415773\ncatrank4    -2.3703986294 -0.732528724\n\n\nNext, we can create a coefficients confidence interval table as below\n\n\nCode\nconf_table &lt;- \n    cbind(mod.2$coefficient, confint(mod.2))\n\n\nWaiting for profiling to be done...\n\n\nCode\nconf_table\n\n\n                                 2.5 %       97.5 %\n(Intercept) -3.989979073 -6.2716202334 -1.792547080\ngmat         0.002264426  0.0001375921  0.004435874\ngpa          0.804037549  0.1602959439  1.464142727\ncatrank2    -0.675442928 -1.3008888002 -0.056745722\ncatrank3    -1.340203916 -2.0276713127 -0.670372346\ncatrank4    -1.551463677 -2.4000265384 -0.753542605\n\n\nWe convert to odd ratios with\n\n\nCode\nexp(conf_table)\n\n\n                            2.5 %    97.5 %\n(Intercept) 0.0185001 0.001889165 0.1665354\ngmat        1.0022670 1.000137602 1.0044457\ngpa         2.2345448 1.173858216 4.3238349\ncatrank2    0.5089310 0.272289674 0.9448343\ncatrank3    0.2617923 0.131641717 0.5115181\ncatrank4    0.2119375 0.090715546 0.4706961\n\n\nWe can also use the epiDisplay package to display this\n\n\nCode\noptions(width = 100)\nepiDisplay::logistic.display(mod.2)\n\n\n\nLogistic regression predicting admit \n \n                  crude OR(95%CI)         adj. OR(95%CI)          P(Wald's test) P(LR-test)\ngmat (cont. var.) 1.0036 (1.0017,1.0055)  1.0023 (1.0001,1.0044)  0.038          0.037     \n                                                                                           \ngpa (cont. var.)  2.86 (1.59,5.14)        2.23 (1.17,4.28)        0.015          0.014     \n                                                                                           \ncatrank: ref.=1                                                                  &lt; 0.001   \n   2              0.47 (0.26,0.86)        0.51 (0.27,0.95)        0.033                    \n   3              0.26 (0.13,0.49)        0.26 (0.13,0.52)        &lt; 0.001                  \n   4              0.19 (0.08,0.41)        0.21 (0.09,0.48)        &lt; 0.001                  \n                                                                                           \nLog-likelihood = -229.2587\nNo. of observations = 400\nAIC value = 470.5175",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#the-lessr-logit-function",
    "href": "logistic-regression.html#the-lessr-logit-function",
    "title": "32  Logistic Regression",
    "section": "32.8 The lessR Logit function",
    "text": "32.8 The lessR Logit function\nThe lessR package gives a detailed output for logistic regression. This is shown below.\n\n\nCode\nlessR::Logit(admit ~ gpa + rank + gmat,  data = adm, brief = F)\n\n\n\nResponse Variable:   admit\nPredictor Variable 1:  gpa\nPredictor Variable 2:  rank\nPredictor Variable 3:  gmat\n\nNumber of cases (rows) of data:  400 \nNumber of cases retained for analysis:  400 \n\n\n   BASIC ANALYSIS \n\n-- Estimated Model of admit for the Logit of Reference Group Membership\n\n             Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%\n(Intercept)   -3.4495     1.1328   -3.045    0.002     -5.6699     -1.2292 \n        gpa    0.7770     0.3275    2.373    0.018      0.1352      1.4189 \n       rank   -0.5600     0.1271   -4.405    0.000     -0.8092     -0.3108 \n       gmat    0.0023     0.0011    2.101    0.036      0.0002      0.0044 \n\n\n-- Odds Ratios and Confidence Intervals\n\n             Odds Ratio   Lower 95%   Upper 95%\n(Intercept)      0.0318      0.0034      0.2925 \n        gpa      2.1750      1.1447      4.1324 \n       rank      0.5712      0.4452      0.7328 \n       gmat      1.0023      1.0002      1.0044 \n\n\n-- Model Fit\n\n    Null deviance: 499.977 on 399 degrees of freedom\nResidual deviance: 459.442 on 396 degrees of freedom\n\nAIC: 467.4418 \n\nNumber of iterations to convergence: 4 \n\n\nCollinearity\n\n     Tolerance       VIF\ngpa      0.852     1.173\nrank     0.985     1.016\ngmat     0.842     1.188\n\n   ANALYSIS OF RESIDUALS AND INFLUENCE \nData, Fitted, Residual, Studentized Residual, Dffits, Cook's Distance\n   [sorted by Cook's Distance]\n   [res_rows = 20 out of 400 cases (rows) of data]\n--------------------------------------------------------------------\n     gpa rank gmat admit  P(Y=1) residual rstudent  dffits   cooks\n316 2.84    2  300     1 0.15780   0.8422    1.944  0.2287 0.02180\n198 3.23    4  400     1 0.09429   0.9057    2.192  0.1877 0.02063\n156 2.86    4  580     1 0.10556   0.8944    2.139  0.1896 0.01950\n373 2.42    1  680     1 0.36140   0.6386    1.452  0.2746 0.01894\n279 3.00    4  680     1 0.14200   0.8580    1.995  0.2055 0.01885\n319 3.71    4  480     1 0.15370   0.8463    1.952  0.1953 0.01617\n342 2.65    3  560     1 0.14357   0.8564    1.986  0.1873 0.01555\n317 3.00    2  340     1 0.18868   0.8113    1.843  0.2027 0.01530\n40  2.68    3  520     1 0.13536   0.8646    2.014  0.1778 0.01456\n28  3.74    4  520     1 0.16927   0.8307    1.899  0.1870 0.01393\n318 3.63    4  780     1 0.25354   0.7465    1.673  0.2103 0.01372\n4   3.19    4  640     1 0.14895   0.8511    1.965  0.1754 0.01331\n385 2.62    2  480     1 0.19267   0.8073    1.829  0.1902 0.01328\n314 3.65    4  520     1 0.15967   0.8403    1.929  0.1780 0.01311\n395 3.99    3  460     1 0.27406   0.7259    1.625  0.2084 0.01285\n255 3.52    4  740     1 0.22148   0.7785    1.751  0.1950 0.01280\n122 2.67    2  480     1 0.19879   0.8012    1.811  0.1819 0.01192\n294 3.97    1  800     0 0.71307  -0.7131   -1.595 -0.2027 0.01183\n254 3.55    4  540     1 0.15544   0.8446    1.941  0.1667 0.01170\n142 3.52    4  700     1 0.20606   0.7939    1.790  0.1801 0.01142\n\n\n   PREDICTION \n\nProbability threshold for classification : 0.5\n\n\nData, Fitted Values, Standard Errors\n   [sorted by fitted value]\n   [pred_all=TRUE to see all intervals displayed]\n--------------------------------------------------------------------\n     gpa rank gmat admit label  fitted std.err\n290 2.26    4  420     0     0 0.04879 0.02069\n49  2.48    4  440     0     0 0.05990 0.02207\n72  2.92    4  300     0     0 0.06108 0.02257\n84  2.91    4  380     0     0 0.07197 0.02290\n\n... for the rows of data where fitted is close to 0.5 ...\n\n     gpa rank gmat admit label fitted std.err\n271 3.95    2  640     1     0 0.4919 0.04976\n68  3.30    1  620     0     0 0.4942 0.05149\n281 3.94    2  660     0     1 0.5015 0.04891\n91  3.83    2  700     0     1 0.5030 0.04501\n105 3.95    2  660     1     1 0.5034 0.04952\n\n... for the last 4 rows of sorted data ...\n\n     gpa rank gmat admit label fitted std.err\n151 3.74    1  800     1     1 0.6752 0.06168\n13  4.00    1  760     1     1 0.6989 0.06003\n294 3.97    1  800     0     1 0.7131 0.06126\n3   4.00    1  800     1     1 0.7178 0.06139\n--------------------------------------------------------------------\n\n\n----------------------------\nSpecified confusion matrices\n----------------------------\n\nProbability threshold for predicting : 0.5\n\n              Baseline         Predicted \n---------------------------------------------------\n             Total  %Tot        0      1  %Correct \n---------------------------------------------------\n        1      127  31.8       98     29     22.8 \nadmit   0      273  68.2      253     20     92.7 \n---------------------------------------------------\n      Total    400                           70.5 \n\nAccuracy: 70.50 \nSensitivity: 22.83 \nPrecision: 59.18",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#selecting-the-best-model",
    "href": "logistic-regression.html#selecting-the-best-model",
    "title": "32  Logistic Regression",
    "section": "32.9 Selecting the best model",
    "text": "32.9 Selecting the best model\nTo select the best model we first run a stepwise backward regression\n\n\nCode\nmod.3 &lt;- \n    MASS::stepAIC(mod.2, direction = \"backward\", trace = FALSE)\n\nmod.3\n\n\n\nCall:  glm(formula = admit ~ gmat + gpa + catrank, family = \"binomial\", \n    data = adm)\n\nCoefficients:\n(Intercept)         gmat          gpa     catrank2     catrank3     catrank4  \n  -3.989979     0.002264     0.804038    -0.675443    -1.340204    -1.551464  \n\nDegrees of Freedom: 399 Total (i.e. Null);  394 Residual\nNull Deviance:      500 \nResidual Deviance: 458.5    AIC: 470.5\n\n\nAll factors are considered significant and retained in the model\nNext, we run a bootstrap diagnostic retention of variables to determine the best predictive variables\n\n\nCode\nbootStepAIC::boot.stepAIC(mod.2, data = adm, B = 100)\n\n\n\nSummary of Bootstrapping the 'stepAIC()' procedure for\n\nCall:\nglm(formula = admit ~ gmat + gpa + catrank, family = \"binomial\", \n    data = adm)\n\nBootstrap samples: 100 \nDirection: backward \nPenalty: 2 * df\n\nCovariates selected\n        (%)\ncatrank  96\ngmat     82\ngpa      82\n\nCoefficients Sign\n          + (%)  - (%)\ngmat     100.00   0.00\ngpa      100.00   0.00\ncatrank2   1.04  98.96\ncatrank3   0.00 100.00\ncatrank4   0.00 100.00\n\nStat Significance\n           (%)\ncatrank3 97.92\ncatrank4 96.88\ngpa      75.61\ngmat     69.51\ncatrank2 58.33\n\n\nThe stepAIC() for the original data-set gave\n\nCall:  glm(formula = admit ~ gmat + gpa + catrank, family = \"binomial\", \n    data = adm)\n\nCoefficients:\n(Intercept)         gmat          gpa     catrank2     catrank3     catrank4  \n  -3.989979     0.002264     0.804038    -0.675443    -1.340204    -1.551464  \n\nDegrees of Freedom: 399 Total (i.e. Null);  394 Residual\nNull Deviance:      500 \nResidual Deviance: 458.5    AIC: 470.5\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nadmit ~ gmat + gpa + catrank\n\nFinal Model:\nadmit ~ gmat + gpa + catrank\n\n\n  Step Df Deviance Resid. Df Resid. Dev      AIC\n1                        394   458.5175 470.5175\n\n\nThe suggested final predictive model therefore is\n\n\nCode\nmod.4 &lt;- \n    glm(admit ~ gmat + gpa + catrank,  data = adm, family = \"binomial\")\n\nepiDisplay::logistic.display(mod.4)\n\n\n\nLogistic regression predicting admit \n \n                  crude OR(95%CI)         adj. OR(95%CI)          P(Wald's test) P(LR-test)\ngmat (cont. var.) 1.0036 (1.0017,1.0055)  1.0023 (1.0001,1.0044)  0.038          0.037     \n                                                                                           \ngpa (cont. var.)  2.86 (1.59,5.14)        2.23 (1.17,4.28)        0.015          0.014     \n                                                                                           \ncatrank: ref.=1                                                                  &lt; 0.001   \n   2              0.47 (0.26,0.86)        0.51 (0.27,0.95)        0.033                    \n   3              0.26 (0.13,0.49)        0.26 (0.13,0.52)        &lt; 0.001                  \n   4              0.19 (0.08,0.41)        0.21 (0.09,0.48)        &lt; 0.001                  \n                                                                                           \nLog-likelihood = -229.2587\nNo. of observations = 400\nAIC value = 470.5175",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Logistic Regression</span>",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "ord-log-reg-cont-categorical.html",
    "href": "ord-log-reg-cont-categorical.html",
    "title": "35  Continuous and Categorical",
    "section": "",
    "text": "35.1 Importing data\nIn this presentation, we analyse a dataset using ordinal logistic regression. We begin by reading the data and selecting our desired subset.\nCode\ndataF &lt;- \n    dget(\"C:/Dataset/anemia_data\") %&gt;% \n    select(sid, anemia_cat, community, fever, sex,\n           famsize, moccup2, foccup2, hosp_visit)\nWe then view a summary of the data\nCode\ndataF %&gt;% glimpse()\n\n\nRows: 476\nColumns: 9\n$ sid        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ anemia_cat &lt;fct&gt; Mild, Moderate, Normal, Severe, Mild, Moderate, Mild, Mild,…\n$ community  &lt;fct&gt; Kasei, Kasei, Kasei, Kasei, Kasei, Kasei, Kasei, Kasei, Kas…\n$ fever      &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Yes, Y…\n$ sex        &lt;fct&gt; Female, Female, Female, Male, Male, Female, Female, Male, F…\n$ famsize    &lt;dbl&gt; 4, 4, 2, 3, 5, 4, 9, 4, 4, 10, 3, 4, 3, 4, 5, 5, 6, 4, 4, 2…\n$ moccup2    &lt;fct&gt; Farmer, Farmer, Other, Other, Farmer, Farmer, Farmer, Other…\n$ foccup2    &lt;fct&gt; Farmer, Farmer, Other, Farmer, Farmer, Farmer, Farmer, Othe…\n$ hosp_visit &lt;fct&gt; No, No, No, Yes, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\nNote that the anemia_cat variable is an ordered factor variable. For completeness the single missing observation for the variable hosp_adm will be recoded to No.\nCode\ndataF &lt;- dataF %&gt;% \n    mutate(hosp_visit = forcats::fct_explicit_na(hosp_visit, na_level = \"No\"))\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `hosp_visit = forcats::fct_explicit_na(hosp_visit, na_level =\n  \"No\")`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n\nCode\nsummary(dataF)\n\n\n      sid           anemia_cat       community   fever         sex     \n Min.   :  1.0   Normal  : 92   Asuogya   : 61   No :160   Male  :252  \n 1st Qu.:119.8   Mild    :143   Sunkwae   : 62   Yes:316   Female:224  \n Median :240.5   Moderate:226   Dromankoma:229                         \n Mean   :240.2   Severe  : 15   Kasei     :124                         \n 3rd Qu.:360.2                                                         \n Max.   :501.0                                                         \n    famsize         moccup2      foccup2    hosp_visit\n Min.   : 0.000   Farmer:314   Farmer:355   No :320   \n 1st Qu.: 4.000   Other :162   Other :121   Yes:156   \n Median : 5.000                                       \n Mean   : 5.151                                       \n 3rd Qu.: 6.000                                       \n Max.   :11.000",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Ordinal Logistic Regression</span>",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Continuous and Categorical</span>"
    ]
  },
  {
    "objectID": "ord-log-reg-cont-categorical.html#model-specification",
    "href": "ord-log-reg-cont-categorical.html#model-specification",
    "title": "35  Continuous and Categorical",
    "section": "35.2 Model specification",
    "text": "35.2 Model specification\nNow we begin the ordinal regression by fixing the first model, the Null model.\n\n\nCode\nModel_0 &lt;- ordinal::clm(anemia_cat ~ 1, data = dataF, link = \"logit\")\nsummary(Model_0)\n\n\nformula: anemia_cat ~ 1\ndata:    dataF\n\n link  threshold nobs logLik  AIC     niter max.grad cond.H \n logit flexible  476  -543.39 1092.77 7(0)  2.15e-13 1.4e+01\n\nThreshold coefficients:\n                Estimate Std. Error z value\nNormal|Mild     -1.42885    0.11608 -12.310\nMild|Moderate   -0.02521    0.09168  -0.275\nModerate|Severe  3.42535    0.26237  13.056\n\n\nSubsequently, we introduce the fever variable as independent and express the results as OR with 95%CI\n\n\nCode\nModel_1 &lt;- ordinal::clm(anemia_cat ~ fever, data = dataF, link = \"logit\")\n\nbroom::tidy(Model_1, conf.int = TRUE, exponentiate = TRUE)%&gt;% \n    flextable::as_flextable() %&gt;% \n    flextable::colformat_double(\n        j = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\", \n              \"conf.low\", \"conf.high\"), \n        digits = 3)\n\n\ntermestimatestd.errorstatisticp.valueconf.lowconf.highcoef.typecharacternumericnumericnumericnumericnumericnumericcharacterNormal|Mild0.3060.163-7.2680.000interceptMild|Moderate1.2570.1521.5050.132interceptModerate|Severe40.1110.29312.6010.000interceptfeverYes1.4610.1812.0900.0371.0242.086locationn: 4\n\n\nResults indicate a significant association between fever and the degree of anaemia (OR=1.46, 95%CI: 1.02 to 2.09). Performing an ANOVA test to see if there exists a difference between the 2 models.\n\n\nCode\nanova(Model_0, Model_1)\n\n\n\n\nno.parAIClogLikLR.statdfPr(&gt;Chisq)\n\n31.09e+03-543        \n\n41.09e+03-5414.3710.0367\n\n\n\n\nThe results indicate adding fever to the Null model significantly improves the null model.\nNext, we add the community variable\n\n\nCode\nModel_2 &lt;- \n     ordinal::clm(anemia_cat ~ fever + community, data = dataF, link = \"logit\")\n\nbroom::tidy(Model_2, conf.int = TRUE, exponentiate = TRUE)%&gt;% \n    flextable::as_flextable() %&gt;% \n    flextable::colformat_double(\n        j = c(\"estimate\", \"std.error\", \"statistic\", \"p.value\", \n              \"conf.low\", \"conf.high\"), \n        digits = 3)\n\n\ntermestimatestd.errorstatisticp.valueconf.lowconf.highcoef.typecharacternumericnumericnumericnumericnumericnumericcharacterNormal|Mild0.1910.286-5.7890.000interceptMild|Moderate0.8080.274-0.7750.438interceptModerate|Severe27.0580.3678.9850.000interceptfeverYes1.3730.1831.7280.0840.9581.966locationcommunitySunkwae1.1790.3430.4790.6320.6022.314locationcommunityDromankoma0.6260.268-1.7470.0810.3681.054locationcommunityKasei0.4630.289-2.6590.0080.2610.813locationn: 7",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Ordinal Logistic Regression</span>",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Continuous and Categorical</span>"
    ]
  },
  {
    "objectID": "ord-log-reg-cont-categorical.html#checking-proportional-odds-assumption-for-the-model",
    "href": "ord-log-reg-cont-categorical.html#checking-proportional-odds-assumption-for-the-model",
    "title": "35  Continuous and Categorical",
    "section": "35.3 Checking proportional odds assumption for the model",
    "text": "35.3 Checking proportional odds assumption for the model\nHere we check the proportional odd assumption for our second model\n\n\nCode\nordinal::nominal_test(Model_2)\n\n\n\n\nDflogLikAICLRTPr(&gt;Chi)\n\n-5351.08e+03          \n\n2-5341.09e+030.3790.827  \n\n6-5251.08e+0318.7  0.00468\n\n\n\n\nThe significant p-value for the community variable indicates a breach of the proportional odd assumption",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Ordinal Logistic Regression</span>",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Continuous and Categorical</span>"
    ]
  },
  {
    "objectID": "ord-log-reg-cont-categorical.html#prediction",
    "href": "ord-log-reg-cont-categorical.html#prediction",
    "title": "35  Continuous and Categorical",
    "section": "35.4 Prediction",
    "text": "35.4 Prediction\nIn this section, we will use the model created above to predict an observation in a specific anaemia severity group. First, we begin by forming the prediction data we call newData.\n\n\nCode\nNewData &lt;- expand.grid(community = levels(dataF$community),\n                       fever = levels(dataF$fever))\nNewData\n\n\n\n\ncommunityfever\n\nAsuogyaNo\n\nSunkwaeNo\n\nDromankomaNo\n\nKaseiNo\n\nAsuogyaYes\n\nSunkwaeYes\n\nDromankomaYes\n\nKaseiYes\n\n\n\n\nWe now predict the probability that the specific predictor combination falls within the specific outcome category (anaemia category)\n\n\nCode\n(preds &lt;- predict(Model_2, newdata = NewData, type = \"prob\"))\n\n\n$fit\n     Normal      Mild  Moderate     Severe\n1 0.1601782 0.2868320 0.5173493 0.03564053\n2 0.1392894 0.2675468 0.5514246 0.04173921\n3 0.2335081 0.3300308 0.4138463 0.02261481\n4 0.2915487 0.3440404 0.3475709 0.01684010\n5 0.1220012 0.2486393 0.5810802 0.04827929\n6 0.1054658 0.2277287 0.6103913 0.05641416\n7 0.1816335 0.3030775 0.4845071 0.03078186\n8 0.2306604 0.3289444 0.4174245 0.02297070\n\n\nFor better visualisation, we bind the original data with the predictions\n\n\nCode\nbind_cols(NewData, preds$fit) %&gt;% \n    kableExtra::kbl(caption = \"Probabilities\", booktabs = T, digits = 3) %&gt;%\n    kableExtra::kable_classic(full_width = F, html_font = \"Cambria\") %&gt;% \n    kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nProbabilities\n\n\ncommunity\nfever\nNormal\nMild\nModerate\nSevere\n\n\n\n\nAsuogya\nNo\n0.160\n0.287\n0.517\n0.036\n\n\nSunkwae\nNo\n0.139\n0.268\n0.551\n0.042\n\n\nDromankoma\nNo\n0.234\n0.330\n0.414\n0.023\n\n\nKasei\nNo\n0.292\n0.344\n0.348\n0.017\n\n\nAsuogya\nYes\n0.122\n0.249\n0.581\n0.048\n\n\nSunkwae\nYes\n0.105\n0.228\n0.610\n0.056\n\n\nDromankoma\nYes\n0.182\n0.303\n0.485\n0.031\n\n\nKasei\nYes\n0.231\n0.329\n0.417\n0.023",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Ordinal Logistic Regression</span>",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Continuous and Categorical</span>"
    ]
  },
  {
    "objectID": "ord-log-reg-cont-categorical.html#visualising-the-model",
    "href": "ord-log-reg-cont-categorical.html#visualising-the-model",
    "title": "35  Continuous and Categorical",
    "section": "35.5 Visualising the model",
    "text": "35.5 Visualising the model\nBelow we visualize the model by using the MASS and effects packages. We begin by fitting the model again with polr function.\n\n\nCode\npol_model.1 &lt;- MASS::polr(anemia_cat ~ community, data = dataF)\npol_model.2 &lt;- MASS::polr(anemia_cat ~ fever*community, data = dataF)\n\n\nAnd then we visualise the probability of having various forms of anaemia giving one belonging to the various groups.\n\n\nCode\nM1 &lt;- effects::Effect(focal.predictors = \"community\", mod=pol_model.1)\n\n\n\nRe-fitting to get Hessian\n\n\nCode\nM2 &lt;- effects::Effect(focal.predictors = c(\"community\", \"fever\"), mod=pol_model.2)\n\n\n\nRe-fitting to get Hessian\n\n\nCode\nplot(M1)\n\n\n\n\n\n\n\n\n\nCode\nplot(M2)",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Ordinal Logistic Regression</span>",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Continuous and Categorical</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html",
    "href": "survival-analysis.html",
    "title": "37  Survival Analysis",
    "section": "",
    "text": "38 Reading and cleaning data\nCode\ndf_mbu &lt;-\n    readxl::read_xlsx(\"C:/Dataset/mbu.xlsx\") %&gt;% \n    janitor::clean_names() %&gt;% \n    select(\n        -c(pt_name, ip, diag1, diag2, diag3, address, \n              disweight, field1, del_date_time)) %&gt;% \n    filter(\n        (outcome %in% c(\"Died\", \"Discharged\")) & \n        (wt &lt; 8 & wt &gt;= 1) & \n        (age &lt; 29) & \n        (sex != \"Missing\") & \n        (place != \"Missing\") &\n        (apgar1 &lt; 11) &\n        (apgar2 &lt; 11) & \n        (!is.na(adm_date_time)) & \n        (!is.na(dis_date_time)) &\n        (del != \"Missing\")) %&gt;% \n    rename(apgar5 = apgar2,\n           del_mode = del) %&gt;% \n    mutate(gestage = ifelse(gestage &gt; 50 | gestage &lt; 26, NA, gestage),\n           place = ifelse(place == \"Self\", \"Home\", place),\n           place = ifelse(\n               place == \"Maternity Home\", \"Clinic/Hospital\", place),\n           across(c(sex, place), ~as.factor(.)),\n           place = fct_relevel(\n               place, c(\"KATH\", \"Clinic/Hospital\", \"Home\")),\n           died = outcome == \"Died\",\n           adm_dura_hrs = difftime(\n               dis_date_time, adm_date_time, units = \"hours\") %&gt;% \n               as.numeric(),\n           adm_dura_hrs = ifelse(\n               adm_dura_hrs &lt; 0 | adm_dura_hrs &gt; 1040, NA, adm_dura_hrs),\n           adm_year = format(adm_date_time, \"%Y\"),\n           outcome = factor(outcome, levels = c(\"Discharged\",\"Died\")),\n           del_mode = ifelse(\n               del_mode %in% c(\"Forceps\", \"Vacuum\"), \n               \"Assisted VD\", del_mode) %&gt;% \n               factor(levels = c(\"SVD\", \"C/S\", \"Assisted VD\"))) %&gt;% \n    select(-c(adm_date_time, dis_date_time)) %&gt;% \n    filter(adm_year != \"2012\" & !is.na(adm_dura_hrs))",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#weekly-survival",
    "href": "survival-analysis.html#weekly-survival",
    "title": "37  Survival Analysis",
    "section": "43.1 Weekly survival",
    "text": "43.1 Weekly survival\n\n\nCode\nsummary(km_by_sex, times = seq(0, 1040, by = 24*7))\n\n\nCall: survfit(formula = survival::Surv(event = died, time = adm_dura_hrs) ~ \n    sex, data = .)\n\n                sex=Female \n time n.risk n.event survival  std.err lower 95% CI upper 95% CI\n    0   3292       3    0.999 0.000526        0.998        1.000\n  168   1087     423    0.851 0.006897        0.838        0.865\n  336    348      33    0.803 0.010707        0.782        0.824\n  504    175      12    0.770 0.013998        0.743        0.797\n  672     83       9    0.709 0.023494        0.665        0.757\n  840     24      14    0.557 0.041019        0.483        0.644\n 1008      1       0    0.557 0.041019        0.483        0.644\n\n                sex=Male \n time n.risk n.event survival  std.err lower 95% CI upper 95% CI\n    0   4210       5    0.999 0.000531        0.998        1.000\n  168   1186     585    0.839 0.006366        0.827        0.851\n  336    296      49    0.777 0.010880        0.756        0.799\n  504    140      17    0.717 0.017530        0.683        0.752\n  672     79       6    0.672 0.024214        0.626        0.721\n  840     31       8    0.580 0.036905        0.512        0.657\n 1008      4       2    0.520 0.054000        0.425        0.638\n\n\n\n\nCode\nkm_by_sex %&gt;% \n    survminer::ggsurvplot(\n        data = df_mbu,\n        pval = T,\n        risk.table = T, \n        censor = F,\n        font.x = c(10, \"bold.italic\", \"red\"),\n        font.y = c(10, \"bold.italic\", \"darkred\"),\n        font.tickslab = c(10, \"plain\", \"darkgreen\"),\n        tables.theme = survminer::theme_cleantable(),\n        conf.int=T,\n        tables.height = 0.2,\n        break.x.by = 24*7,\n        legend.title = \"\",\n        surv.scale = \"percent\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#checking-the-model",
    "href": "survival-analysis.html#checking-the-model",
    "title": "37  Survival Analysis",
    "section": "47.1 Checking the model",
    "text": "47.1 Checking the model\n\n\nCode\nperformance::performance(cox_1)\n\n\nResponse residuals not available to calculate mean square error. (R)MSE\n  is probably not reliable.\n\n\nWarning: Can't calculate weighted residuals from model.\n\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | Nagelkerke's R2 |  RMSE | Sigma\n-------------------------------------------------------------------\n18651.333 | 18651.375 | 18734.408 |           0.139 | 0.413 | 0.000\n\n\nCode\nperformance::performance(cox_2)\n\n\nResponse residuals not available to calculate mean square error. (R)MSE\n  is probably not reliable.\n\n\nWarning: Can't calculate weighted residuals from model.\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | Nagelkerke's R2 |  RMSE | Sigma\n----------------------------------------------------------------\n9244.289 | 9244.365 | 9328.463 |           0.135 | 0.377 | 0.000",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#compare-two-multivariate-models-obviously-second-is-better-than-first",
    "href": "survival-analysis.html#compare-two-multivariate-models-obviously-second-is-better-than-first",
    "title": "37  Survival Analysis",
    "section": "47.2 Compare two multivariate models: Obviously second is better than first",
    "text": "47.2 Compare two multivariate models: Obviously second is better than first\n\n\nCode\nperformance::compare_performance(cox_1, cox_2)\n\n\nWhen comparing models, please note that probably not all models were fit\n  from same data.\n\n\n# Comparison of Model Performance Indices\n\nName  | Model |   AIC (weights) |  AICc (weights) |   BIC (weights) | Nagelkerke's R2 |  RMSE | Sigma\n-----------------------------------------------------------------------------------------------------\ncox_1 | coxph | 18651.3 (&lt;.001) | 18651.4 (&lt;.001) | 18734.4 (&lt;.001) |           0.139 | 0.413 | 0.000\ncox_2 | coxph |  9244.3 (&gt;.999) |  9244.4 (&gt;.999) |  9328.5 (&gt;.999) |           0.135 | 0.377 | 0.000",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#checking-for-proportional-hazard-assumption",
    "href": "survival-analysis.html#checking-for-proportional-hazard-assumption",
    "title": "37  Survival Analysis",
    "section": "47.3 Checking for proportional hazard assumption",
    "text": "47.3 Checking for proportional hazard assumption\nSignificant p-values indicates Proportional Hazard assumption violated\n\n\nCode\ncox_1 %&gt;% survival::cox.zph()\n\n\n            chisq df       p\nwt        0.00917  1  0.9237\nage       8.38652  1  0.0038\nsex       2.54003  1  0.1110\nplace     6.94463  2  0.0310\napgar1   30.05277  1 4.2e-08\napgar5   38.45559  1 5.6e-10\ndel_mode  2.09712  2  0.3504\nadm_year 10.39820  3  0.0155\nGLOBAL   72.90917 12 9.1e-11\n\n\nResiduals falling outside the standard error margins indicates Proportional Hazard assumption violated. These violations will have to be dealt with.\n\n\nCode\ncox_1 %&gt;% survival::cox.zph() %&gt;% survminer::ggcoxzph()\n\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#without-gestational-age",
    "href": "survival-analysis.html#without-gestational-age",
    "title": "37  Survival Analysis",
    "section": "48.1 Without gestational age",
    "text": "48.1 Without gestational age\n\n\nCode\ncox_1A &lt;-\n    df_mbu %&gt;% \n    select(-c(outcome, gestage)) %&gt;%\n    gtsummary::tbl_uvregression(\n        method = survival::coxph, \n        y = survival::Surv(event = died, time = adm_dura_hrs),\n        exponentiate = TRUE, \n        pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3)) %&gt;% \n    gtsummary::bold_labels() %&gt;% \n    gtsummary::bold_p()\n\ncox_1B &lt;- \n    df_mbu %&gt;% \n    select(-c(outcome, gestage)) %&gt;% \n    survival::coxph(\n        survival::Surv(\n            event = died, time = adm_dura_hrs) ~ ., data = .) %&gt;% \n    gtsummary::tbl_regression(\n        exponentiate = TRUE,\n        pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3)) %&gt;% \n    gtsummary::bold_labels() %&gt;% \n    gtsummary::bold_p()\n\ngtsummary::tbl_merge(\n    tbls = list(cox_1A, cox_1B),\n    tab_spanner = c(\"Univariate\", \"Multivariate\")) %&gt;% \n    gtsummary::modify_caption(\n        \"Combined Univariate and Multivariate Cox regression with sex\")\n\n\n\n\n\n\nCombined Univariate and Multivariate Cox regression with sex\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nUnivariate\n\n\nMultivariate\n\n\n\nN\nHR\n1\n95% CI\n1\np-value\nHR\n1\n95% CI\n1\np-value\n\n\n\n\nWeight in kgs\n7,502\n0.66\n0.61, 0.70\n&lt;0.001\n0.70\n0.66, 0.75\n&lt;0.001\n\n\nAge in days\n7,502\n1.00\n0.96, 1.05\n0.839\n1.01\n0.97, 1.06\n0.558\n\n\nSex\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Female\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    Male\n\n\n1.13\n1.01, 1.27\n0.034\n1.17\n1.04, 1.31\n0.010\n\n\nPlace of delivery\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    KATH\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    Clinic/Hospital\n\n\n2.29\n2.03, 2.57\n&lt;0.001\n1.85\n1.64, 2.09\n&lt;0.001\n\n\n    Home\n\n\n1.57\n0.89, 2.78\n0.122\n1.39\n0.78, 2.47\n0.261\n\n\nFirst minute APGAR\n7,502\n0.72\n0.70, 0.74\n&lt;0.001\n0.96\n0.90, 1.01\n0.113\n\n\nFifth minute APGAR\n7,502\n0.66\n0.64, 0.68\n&lt;0.001\n0.69\n0.66, 0.74\n&lt;0.001\n\n\nMode of delivery\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    SVD\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    C/S\n\n\n0.64\n0.57, 0.72\n&lt;0.001\n1.20\n1.06, 1.36\n0.005\n\n\n    Assisted VD\n\n\n0.53\n0.33, 0.88\n0.013\n0.92\n0.56, 1.51\n0.738\n\n\nYear of admission\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    2013\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    2014\n\n\n0.77\n0.67, 0.89\n&lt;0.001\n0.74\n0.64, 0.85\n&lt;0.001\n\n\n    2015\n\n\n0.72\n0.61, 0.84\n&lt;0.001\n0.82\n0.69, 0.96\n0.017\n\n\n    2016\n\n\n0.84\n0.70, 1.01\n0.063\n0.93\n0.77, 1.12\n0.464\n\n\n\n1\nHR = Hazard Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#with-gestational-age",
    "href": "survival-analysis.html#with-gestational-age",
    "title": "37  Survival Analysis",
    "section": "48.2 With gestational age",
    "text": "48.2 With gestational age\n\n\nCode\ncox_2A &lt;-\n    df_mbu %&gt;% \n    select(-c(outcome)) %&gt;%\n    gtsummary::tbl_uvregression(\n        method = survival::coxph, \n        y = survival::Surv(event = died, time = adm_dura_hrs),\n        exponentiate = TRUE, \n        pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3)) %&gt;% \n    gtsummary::bold_labels() %&gt;% \n    gtsummary::bold_p()\n\ncox_2B &lt;- \n    df_mbu %&gt;% \n    select(-c(outcome)) %&gt;% \n    survival::coxph(\n        survival::Surv(\n            event = died, time = adm_dura_hrs) ~ ., data = .) %&gt;% \n    gtsummary::tbl_regression(\n        pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3),\n        exponentiate = TRUE) %&gt;% \n    gtsummary::bold_labels() %&gt;% \n    gtsummary::bold_p()\n\ngtsummary::tbl_merge(\n    tbls = list(cox_2A, cox_2B),\n    tab_spanner = c(\"Univariate\", \"Multivariate\")) %&gt;% \n    gtsummary::modify_caption(\n        \"Combined Univariate and Multivariate Cox regression with sex\")\n\n\n\n\n\n\nCombined Univariate and Multivariate Cox regression with sex\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nUnivariate\n\n\nMultivariate\n\n\n\nN\nHR\n1\n95% CI\n1\np-value\nHR\n1\n95% CI\n1\np-value\n\n\n\n\nWeight in kgs\n7,502\n0.66\n0.61, 0.70\n&lt;0.001\n0.69\n0.59, 0.81\n&lt;0.001\n\n\nAge in days\n7,502\n1.00\n0.96, 1.05\n0.839\n1.03\n0.99, 1.08\n0.163\n\n\nSex\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Female\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    Male\n\n\n1.13\n1.01, 1.27\n0.034\n1.09\n0.93, 1.28\n0.299\n\n\nPlace of delivery\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    KATH\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    Clinic/Hospital\n\n\n2.29\n2.03, 2.57\n&lt;0.001\n1.60\n1.34, 1.93\n&lt;0.001\n\n\n    Home\n\n\n1.57\n0.89, 2.78\n0.122\n1.29\n0.64, 2.61\n0.475\n\n\nFirst minute APGAR\n7,502\n0.72\n0.70, 0.74\n&lt;0.001\n0.97\n0.90, 1.05\n0.457\n\n\nFifth minute APGAR\n7,502\n0.66\n0.64, 0.68\n&lt;0.001\n0.67\n0.62, 0.73\n&lt;0.001\n\n\nMode of delivery\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    SVD\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    C/S\n\n\n0.64\n0.57, 0.72\n&lt;0.001\n1.22\n1.03, 1.45\n0.025\n\n\n    Assisted VD\n\n\n0.53\n0.33, 0.88\n0.013\n1.36\n0.74, 2.51\n0.324\n\n\nGestational age\n4,793\n0.90\n0.88, 0.91\n&lt;0.001\n0.97\n0.94, 1.00\n0.048\n\n\nYear of admission\n7,502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    2013\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    2014\n\n\n0.77\n0.67, 0.89\n&lt;0.001\n0.74\n0.59, 0.92\n0.007\n\n\n    2015\n\n\n0.72\n0.61, 0.84\n&lt;0.001\n0.90\n0.71, 1.14\n0.376\n\n\n    2016\n\n\n0.84\n0.70, 1.01\n0.063\n1.04\n0.80, 1.34\n0.787\n\n\n\n1\nHR = Hazard Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#plotting-coefficients",
    "href": "survival-analysis.html#plotting-coefficients",
    "title": "37  Survival Analysis",
    "section": "48.3 Plotting coefficients",
    "text": "48.3 Plotting coefficients\n\n\nCode\ndf_mbu %&gt;% \n    select(-c(outcome)) %&gt;% \n    survival::coxph(\n        survival::Surv(event = died, time = adm_dura_hrs) ~ ., \n        data = .) %&gt;% \n    sjPlot::plot_model(\n        value.offset = 0.4,show.values = T, show.p = T) +\n    geom_hline(yintercept = 1, alpha =.5, color = 'grey45') +\n    labs(\n        title = \"Coefficients Plot of Hazard Ratios\", \n        y = \"Hazard ratio (95%CI)\") +\n    theme_minimal() +\n    scale_y_log10(limits = c(0.4, 5))\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Survival Analysis</span>",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "missing-data.html",
    "href": "missing-data.html",
    "title": "39  Visualization",
    "section": "",
    "text": "39.1 Importing data\nCode\nlibrary(tidyverse)\ndf_hpt &lt;- \n    readxl::read_xlsx(path = \"C:/Dataset/hptdata.xlsx\") %&gt;% \n    janitor::clean_names() %&gt;% \n    mutate(\n        sex = factor(sex),\n        educ = factor(\n            educ, \n            levels = c(\"None\", \"Primary\", \"JHS/Form 4\", \"SHS/Secondary\", \"Tertiary\")\n            ),\n        hpt = case_when(\n            (syst1 &gt;= 140 | syst2 &gt;= 140 | diast1 &gt;= 90 | diast2 &gt;= 90) ~ \"Yes\",\n            (syst1 &lt; 140 & syst2 &lt; 140 & diast1 &lt; 90 & diast2 &lt; 90) ~ \"No\"\n            ) %&gt;% factor()\n        )",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Missiing Data</span>",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "missing-data.html#labelling-data",
    "href": "missing-data.html#labelling-data",
    "title": "39  Visualization",
    "section": "39.2 Labelling data",
    "text": "39.2 Labelling data\n\n\nCode\ndf_hpt &lt;- \n    df_hpt %&gt;% \n    labelled::set_variable_labels(\n        sid = \"Study ID\",\n        ageyrs = \"Age (years)\",\n        sex = \"Sex\",\n        educ = \"Educational level\",\n        wgt = \"Body Weight\",\n        waist = \"Waist circumference (cm)\",\n        hgt = \"Height (cm)\",\n        syst1 = \"Systolic BP 1st\",\n        diast1 = \"Diastolic BP 1st\",\n        syst2 = \"Systolic BP 2nd\",\n        diast2 = \"Diastolic BP 2nd\",\n        recdate = \"Record date\",\n        hpt = \"Hypertension\"\n    )",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Missiing Data</span>",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "missing-data.html#summarizing-data",
    "href": "missing-data.html#summarizing-data",
    "title": "39  Visualization",
    "section": "39.3 Summarizing data",
    "text": "39.3 Summarizing data\n\n\nCode\ndf_hpt %&gt;% \n    summarytools::dfSummary(graph.col = F, labels.col = F)\n\n\nData Frame Summary  \ndf_hpt  \nDimensions: 250 x 13  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------\nNo   Variable            Stats / Values              Freqs (% of Valid)    Valid      Missing  \n---- ------------------- --------------------------- --------------------- ---------- ---------\n1    sid                 1. H001                       1 ( 0.4%)           250        0        \n     [character]         2. H002                       1 ( 0.4%)           (100.0%)   (0.0%)   \n                         3. H003                       1 ( 0.4%)                               \n                         4. H004                       1 ( 0.4%)                               \n                         5. H005                       1 ( 0.4%)                               \n                         6. H006                       1 ( 0.4%)                               \n                         7. H007                       1 ( 0.4%)                               \n                         8. H008                       1 ( 0.4%)                               \n                         9. H009                       1 ( 0.4%)                               \n                         10. H010                      1 ( 0.4%)                               \n                         [ 240 others ]              240 (96.0%)                               \n\n2    ageyrs              Mean (sd) : 51.4 (18.7)     61 distinct values    250        0        \n     [numeric]           min &lt; med &lt; max:                                  (100.0%)   (0.0%)   \n                         18 &lt; 51 &lt; 94                                                          \n                         IQR (CV) : 28 (0.4)                                                   \n\n3    sex                 1. Female                   170 (68.8%)           247        3        \n     [factor]            2. Male                      77 (31.2%)           (98.8%)    (1.2%)   \n\n4    educ                1. None                     112 (45.0%)           249        1        \n     [factor]            2. Primary                   34 (13.7%)           (99.6%)    (0.4%)   \n                         3. JHS/Form 4                84 (33.7%)                               \n                         4. SHS/Secondary             15 ( 6.0%)                               \n                         5. Tertiary                   4 ( 1.6%)                               \n\n5    wgt                 Mean (sd) : 59.8 (12.8)     182 distinct values   246        4        \n     [numeric]           min &lt; med &lt; max:                                  (98.4%)    (1.6%)   \n                         35.5 &lt; 57.6 &lt; 105.2                                                   \n                         IQR (CV) : 17.3 (0.2)                                                 \n\n6    waist               Mean (sd) : 83 (11)         49 distinct values    249        1        \n     [numeric]           min &lt; med &lt; max:                                  (99.6%)    (0.4%)   \n                         64 &lt; 81 &lt; 114                                                         \n                         IQR (CV) : 15 (0.1)                                                   \n\n7    hgt                 Mean (sd) : 160.7 (12)      47 distinct values    248        2        \n     [numeric]           min &lt; med &lt; max:                                  (99.2%)    (0.8%)   \n                         67 &lt; 161 &lt; 184                                                        \n                         IQR (CV) : 10.5 (0.1)                                                 \n\n8    syst1               Mean (sd) : 132.3 (27.8)    98 distinct values    250        0        \n     [numeric]           min &lt; med &lt; max:                                  (100.0%)   (0.0%)   \n                         74 &lt; 128 &lt; 225                                                        \n                         IQR (CV) : 35.8 (0.2)                                                 \n\n9    diast1              Mean (sd) : 78.2 (16.4)     64 distinct values    247        3        \n     [numeric]           min &lt; med &lt; max:                                  (98.8%)    (1.2%)   \n                         49 &lt; 76 &lt; 160                                                         \n                         IQR (CV) : 20 (0.2)                                                   \n\n10   syst2               Mean (sd) : 131.6 (27.2)    94 distinct values    250        0        \n     [numeric]           min &lt; med &lt; max:                                  (100.0%)   (0.0%)   \n                         84 &lt; 127 &lt; 232                                                        \n                         IQR (CV) : 36 (0.2)                                                   \n\n11   diast2              Mean (sd) : 77.9 (15.8)     63 distinct values    247        3        \n     [numeric]           min &lt; med &lt; max:                                  (98.8%)    (1.2%)   \n                         50 &lt; 76 &lt; 150                                                         \n                         IQR (CV) : 18.5 (0.2)                                                 \n\n12   recdate             min : 2012-02-12            250 distinct values   250        0        \n     [POSIXct, POSIXt]   med : 2012-06-15 12:00:00                         (100.0%)   (0.0%)   \n                         max : 2012-10-18                                                      \n                         range : 8m 6d                                                         \n\n13   hpt                 1. No                       146 (58.6%)           249        1        \n     [factor]            2. Yes                      103 (41.4%)           (99.6%)    (0.4%)   \n-----------------------------------------------------------------------------------------------",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Missiing Data</span>",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "missing-data.html#viewing-missing-data-pattern",
    "href": "missing-data.html#viewing-missing-data-pattern",
    "title": "39  Visualization",
    "section": "39.4 Viewing missing data pattern",
    "text": "39.4 Viewing missing data pattern\n\n\nCode\ndf_hpt %&gt;% mice::md.pattern(rotate.names = T)\n\n\n    sid ageyrs syst1 syst2 recdate educ waist hpt hgt sex diast1 diast2 wgt   \n237   1      1     1     1       1    1     1   1   1   1      1      1   1  0\n1     1      1     1     1       1    1     1   1   1   1      1      1   0  1\n2     1      1     1     1       1    1     1   1   1   1      1      0   1  1\n1     1      1     1     1       1    1     1   1   1   1      1      0   0  2\n2     1      1     1     1       1    1     1   1   1   1      0      1   0  2\n2     1      1     1     1       1    1     1   1   1   0      1      1   1  1\n1     1      1     1     1       1    1     1   1   0   1      1      1   1  1\n1     1      1     1     1       1    1     1   1   0   0      1      1   1  2\n1     1      1     1     1       1    1     1   0   1   1      0      1   1  2\n1     1      1     1     1       1    1     0   1   1   1      1      1   1  1\n1     1      1     1     1       1    0     1   1   1   1      1      1   1  1\n      0      0     0     0       0    1     1   1   2   3      3      3   4 18\n\n\n\n\n\n\n\n\nFigure 39.1: Missing data pattern\n\n\n\n\n\n\n\nCode\ndf_hpt %&gt;% \n    visdat::vis_miss()\n\ndf_hpt %&gt;% visdat::vis_dat(palette = \"cb_safe\")\n\n\n\n\n\n\n\n\nFigure 39.2: Missing data pattern 2\n\n\n\n\n\n\n\n\n\n\n\nFigure 39.3: Missing data pattern 2",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Missiing Data</span>",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "missing-data.html#regression-with-mice-imputation",
    "href": "missing-data.html#regression-with-mice-imputation",
    "title": "39  Visualization",
    "section": "39.5 Regression with mice imputation",
    "text": "39.5 Regression with mice imputation\n\n\nCode\ngtsummary::theme_gtsummary_compact()\n\n# Input the data\nimputed_data &lt;- \n    df_hpt %&gt;% \n    select(hpt, ageyrs, sex, waist, educ, wgt, hgt) %&gt;% \n    mice::mice(maxit = 20, m = 5,printFlag = F)\n\n# Visualize the 3rd og the 5 data sets created\nmice::complete(imputed_data, 4) %&gt;% \n    head() %&gt;% \n    gt::gt() %&gt;% \n    gt::opt_stylize(style = 3, color = \"blue\", add_row_striping = TRUE)\n\n\n\n\n\n\n\n\nhpt\nageyrs\nsex\nwaist\neduc\nwgt\nhgt\n\n\n\n\nYes\n84\nFemale\n83\nJHS/Form 4\n53.7\n153\n\n\nYes\n70\nFemale\n80\nNone\n56.8\n149\n\n\nYes\n55\nFemale\n113\nNone\n97.6\n162\n\n\nNo\n57\nMale\n76\nJHS/Form 4\n59.2\n172\n\n\nNo\n54\nMale\n78\nJHS/Form 4\n58.2\n168\n\n\nNo\n19\nFemale\n71\nSHS/Secondary\n54.9\n162\n\n\n\n\n\n\n\nCode\n# Create univariate table for original data set\ntbl1 &lt;- \n    df_hpt %&gt;% \n    gtsummary::tbl_uvregression(\n        include = c(ageyrs, sex, waist, educ, wgt, hgt),\n        y = hpt,\n        method = glm,\n        method.args = family(binomial),\n        exponentiate=TRUE,\n        pvalue_fun = function(x) gtsummary::style_pvalue(x, digits = 3)\n        ) %&gt;% \n    gtsummary::bold_labels() %&gt;% \n    gtsummary::bold_p()\ntbl1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nAge (years)\n249\n1.06\n1.04, 1.08\n&lt;0.001\n\n\nSex\n246\n\n\n\n\n\n\n\n\n    Female\n\n\n—\n—\n\n\n\n\n    Male\n\n\n1.09\n0.63, 1.87\n0.765\n\n\nWaist circumference (cm)\n248\n1.02\n1.00, 1.04\n0.099\n\n\nEducational level\n248\n\n\n\n\n\n\n\n\n    None\n\n\n—\n—\n\n\n\n\n    Primary\n\n\n0.23\n0.09, 0.56\n0.002\n\n\n    JHS/Form 4\n\n\n0.43\n0.24, 0.77\n0.005\n\n\n    SHS/Secondary\n\n\n0.32\n0.08, 0.98\n0.060\n\n\n    Tertiary\n\n\n2.60\n0.32, 53.4\n0.414\n\n\nBody Weight\n245\n0.98\n0.96, 1.00\n0.120\n\n\nHeight (cm)\n247\n0.98\n0.96, 1.00\n0.103\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Build the model\nimputed_model &lt;- \n    with(imputed_data, \n         glm(hpt ~ ageyrs + sex + waist + educ + wgt + hgt, \n             family = \"binomial\")\n         )\n\n# present beautiful table with gtsummary\ntbl2 &lt;- \n    imputed_model %&gt;% \n    gtsummary::tbl_regression(\n        exponentiate=TRUE,\n        pvalue_fun = function(x) gtsummary::style_pvalue(x, digits = 3)\n        ) %&gt;% \n    gtsummary::bold_labels() %&gt;% \n    gtsummary::bold_p()\ntbl2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nAge (years)\n1.06\n1.04, 1.09\n&lt;0.001\n\n\nSex\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n1.30\n0.61, 2.78\n0.500\n\n\nWaist circumference (cm)\n1.03\n0.98, 1.09\n0.230\n\n\nEducational level\n\n\n\n\n\n\n\n\n    None\n—\n—\n\n\n\n\n    Primary\n0.50\n0.18, 1.41\n0.191\n\n\n    JHS/Form 4\n0.87\n0.42, 1.81\n0.715\n\n\n    SHS/Secondary\n0.88\n0.22, 3.56\n0.854\n\n\n    Tertiary\n11.4\n0.91, 143\n0.059\n\n\nBody Weight\n1.00\n0.95, 1.05\n0.866\n\n\nHeight (cm)\n0.99\n0.96, 1.02\n0.424\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nCode\n# Combine tables\ngtsummary::tbl_merge(\n    tbls = list(tbl1, tbl2), \n    tab_spanner = c(\"**Univariate**\", \"**Multivariate**\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nUnivariate\n\n\nMultivariate\n\n\n\nN\nOR\n1\n95% CI\n1\np-value\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nAge (years)\n249\n1.06\n1.04, 1.08\n&lt;0.001\n1.06\n1.04, 1.09\n&lt;0.001\n\n\nSex\n246\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Female\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    Male\n\n\n1.09\n0.63, 1.87\n0.765\n1.30\n0.61, 2.78\n0.500\n\n\nWaist circumference (cm)\n248\n1.02\n1.00, 1.04\n0.099\n1.03\n0.98, 1.09\n0.230\n\n\nEducational level\n248\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    None\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    Primary\n\n\n0.23\n0.09, 0.56\n0.002\n0.50\n0.18, 1.41\n0.191\n\n\n    JHS/Form 4\n\n\n0.43\n0.24, 0.77\n0.005\n0.87\n0.42, 1.81\n0.715\n\n\n    SHS/Secondary\n\n\n0.32\n0.08, 0.98\n0.060\n0.88\n0.22, 3.56\n0.854\n\n\n    Tertiary\n\n\n2.60\n0.32, 53.4\n0.414\n11.4\n0.91, 143\n0.059\n\n\nBody Weight\n245\n0.98\n0.96, 1.00\n0.120\n1.00\n0.95, 1.05\n0.866\n\n\nHeight (cm)\n247\n0.98\n0.96, 1.00\n0.103\n0.99\n0.96, 1.02\n0.424\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Missiing Data</span>",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "missing-data.html#inputation-with-variable-selection-using-missranger-package",
    "href": "missing-data.html#inputation-with-variable-selection-using-missranger-package",
    "title": "39  Visualization",
    "section": "39.6 Inputation with variable selection using missRanger package",
    "text": "39.6 Inputation with variable selection using missRanger package\nTable 39.1 is the first table we are drawing\n\n\nCode\n# Create an empty list\nkk &lt;- list()\n\n\n# Create a loop for running multiple (10) imputations  and selecting the best variables\nfor(i in 1:10){\n    df_temp &lt;- \n        df_hpt %&gt;% \n        select(hpt, ageyrs, sex, waist, educ, wgt, hgt)\n\n    df_imp &lt;-  \n        df_temp %&gt;% \n        missRanger::missRanger(\n            formula = . ~ ., seed = i, num.trees = 1000,verbose = F) \n\n    model.x &lt;- \n        glm(hpt ~ ageyrs + sex + waist + educ + wgt + hgt, \n            family = \"binomial\", data = df_imp)\n\n    kk[[i]]&lt;-\n        MASS::stepAIC(model.x, direction = \"both\", trace = FALSE,) %&gt;% \n        broom::tidy(exponentiate = T) %&gt;% \n        pull(term)\n}\n\n# Tabulate selected variables. Chosen : ageyrs and waist\nunlist(kk) %&gt;% table()\n\n\n.\n(Intercept)      ageyrs       waist \n         10          10          10 \n\n\nCode\ndf_for_reg &lt;-\n    df_hpt %&gt;% \n    select(ageyrs, waist, hpt)\n\n# Univariate regression\ntbl1 &lt;-\n    df_temp %&gt;% \n    gtsummary::tbl_uvregression(\n        method = glm,\n        y = hpt,\n        method.args = family(binomial),\n        exponentiate = TRUE,\n        pvalue_fun = function(x) gtsummary::style_pvalue(x, digits = 3)\n        ) %&gt;% \n    gtsummary::bold_p()\n\n# Multivariate regression \ntbl2 &lt;-\n    df_for_reg %&gt;%\n    mice::mice(m = 10, seed = 200, printFlag = F)%&gt;%\n    with(glm(hpt ~ ageyrs + waist, family = \"binomial\")\n        ) %&gt;%\n    gtsummary::tbl_regression(\n        exponentiate = T,\n        pvalue_fun = function(x) gtsummary::style_pvalue(x, digits = 3)\n        ) %&gt;%\n    gtsummary::bold_labels() %&gt;%\n    gtsummary::bold_p()\n\n# Merge tables into one\ntbl_all &lt;-\n    gtsummary::tbl_merge(\n    tbls = list(tbl1, tbl2),\n    tab_spanner = c(\"**Univariate**\", \"**Multivariate**\")\n    )\n\ntbl_all\n\n\n\n\nTable 39.1: The first table is here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nUnivariate\n\n\nMultivariate\n\n\n\nN\nOR\n1\n95% CI\n1\np-value\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nAge (years)\n249\n1.06\n1.04, 1.08\n&lt;0.001\n1.06\n1.04, 1.08\n&lt;0.001\n\n\nSex\n246\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Female\n\n\n—\n—\n\n\n\n\n\n\n\n\n\n\n    Male\n\n\n1.09\n0.63, 1.87\n0.765\n\n\n\n\n\n\n\n\nWaist circumference (cm)\n248\n1.02\n1.00, 1.04\n0.099\n1.02\n1.00, 1.05\n0.098\n\n\nEducational level\n248\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    None\n\n\n—\n—\n\n\n\n\n\n\n\n\n\n\n    Primary\n\n\n0.23\n0.09, 0.56\n0.002\n\n\n\n\n\n\n\n\n    JHS/Form 4\n\n\n0.43\n0.24, 0.77\n0.005\n\n\n\n\n\n\n\n\n    SHS/Secondary\n\n\n0.32\n0.08, 0.98\n0.060\n\n\n\n\n\n\n\n\n    Tertiary\n\n\n2.60\n0.32, 53.4\n0.414\n\n\n\n\n\n\n\n\nBody Weight\n245\n0.98\n0.96, 1.00\n0.120\n\n\n\n\n\n\n\n\nHeight (cm)\n247\n0.98\n0.96, 1.00\n0.103\n\n\n\n\n\n\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\nFigure 39.4 is a depiction of the figures that may show on the table every time there is an indication for such\n\n\nCode\ndf_hpt %&gt;% \n    select(syst1, ageyrs, sex) %&gt;% \n    drop_na(syst1, ageyrs, sex) %&gt;% \n    ggplot(aes(x = ageyrs, y = syst1, color = sex)) + \n    geom_point()+\n    geom_smooth(formula = y~x, method = \"loess\", se = F)+\n    labs(\n        x = \"Age (years)\",\n        y = \"Systolic Blood Pressure (mmHg)\"\n        ) +\n    ggthemes::theme_clean()+\n    scale_color_manual(\n        name = \"Sex\", \n        labels = c(\"Female\",\"Male\"), \n        values = c(\"red\", \"#0043E0\")\n        )\n\n\n\n\n\n\n\n\nFigure 39.4: Relationship between systolic blood pressure and age\n\n\n\n\n\n\n\nCode\nlm1 &lt;- \n    glm(syst1 ~ sex+ ageyrs + waist + hgt, data = df_hpt, family = \"gaussian\")\nlm1 %&gt;% performance::check_heteroscedasticity()\n\n\nOK: Error variance appears to be homoscedastic (p = 0.136).\n\n\nCode\nlm1 %&gt;% performance::check_normality() %&gt;% plot()\n\n\nThere's no formal statistical test for normality for generalized linear\n  model.\n  Instead, please use `simulate_residuals()` and `check_residuals()` to\n  check for uniformity of residuals.\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% performance::check_posterior_predictions()\n\n\nWarning: 'performance::check_posterior_predictions' is deprecated.\nUse 'check_predictions()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% performance::check_outliers() %&gt;% plot()\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% performance::check_collinearity() %&gt;% plot() \n\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n    sjPlot::plot_model(type = \"eff\", term = c(\"ageyrs[all]\", \"sex\"))+ \n    theme_bw()\n\n\n\n\n\n\n\n\n\nCode\nlm1 %&gt;% \n    sjPlot::tab_model(\n        p.style = \"numeric_stars\", \n        show.reflvl = T, \n        show.intercept = F\n        )\n\n\n\n\n\n \nSystolic BP 1st\n\n\nPredictors\nEstimates\nCI\np\n\n\nAge (years)\n0.78 ***\n0.62 – 0.94\n&lt;0.001\n\n\nWaist circumference (cm)\n0.06 \n-0.21 – 0.34\n0.649\n\n\nHeight (cm)\n0.05 \n-0.22 – 0.31\n0.731\n\n\nFemale\nReference\n\n\n\n\nMale\n8.29 *\n1.52 – 15.05\n0.016\n\n\nObservations\n245\n\n\nR2\n0.298\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n\n\nCode\nlm2 &lt;- \n    glm(waist ~ sex* ageyrs, data = df_hpt, family = \"gaussian\")\nlm2 %&gt;% performance::check_heteroscedasticity()\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.002).\n\n\nCode\nlm2 %&gt;% performance::check_normality() %&gt;% plot()\n\n\nThere's no formal statistical test for normality for generalized linear\n  model.\n  Instead, please use `simulate_residuals()` and `check_residuals()` to\n  check for uniformity of residuals.\n\n\n\n\n\n\n\n\n\nCode\nlm2 %&gt;% performance::check_posterior_predictions()\n\n\nWarning: 'performance::check_posterior_predictions' is deprecated.\nUse 'check_predictions()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\nCode\nlm2 %&gt;% performance::check_outliers() %&gt;% plot()\n\n\n\n\n\n\n\n\n\nCode\nlm2 %&gt;% performance::check_collinearity() %&gt;% plot() \n\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\n\nCode\nlm2 %&gt;% \n    sjPlot::plot_model(type = \"eff\", term = c(\"sex\", \"ageyrs[20, 50, 80]\"))+ \n    theme_bw()\n\n\n\n\n\n\n\n\n\nCode\nlm2 %&gt;% \n    sjPlot::tab_model(\n        p.style = \"numeric_stars\", \n        show.reflvl = T, \n        show.intercept = F\n        )\n\n\n\n\n\n \nWaist circumference (cm)\n\n\nPredictors\nEstimates\nCI\np\n\n\nAge (years)\n-0.00 \n-0.09 – 0.09\n0.945\n\n\nFemale\nReference\n\n\n\n\nMale\n-8.26 \n-16.69 – 0.16\n0.055\n\n\nsexMale:ageyrs\n0.09 \n-0.06 – 0.25\n0.243\n\n\nObservations\n246\n\n\nR2\n0.030\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Missiing Data</span>",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "qr-single-continuous.html",
    "href": "qr-single-continuous.html",
    "title": "47  Single Continuous",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\ndf_data1 &lt;- read_csv(\"C:/Dataset/data1.csv\") %&gt;% drop_na()\n\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_quantile(formula = y ~ x)+\n    theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_quantile(formula = y ~ x, quantiles = 0.5)+\n    theme_bw()\n\n\n\n\n\n\n\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_quantile(formula = y ~ x, quantiles = c(0.025, 0.975),)+\n    theme_bw()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>GQuantile Regression</span>",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Single Continuous</span>"
    ]
  },
  {
    "objectID": "pkg-dplyr.html",
    "href": "pkg-dplyr.html",
    "title": "49  dplyr Package",
    "section": "",
    "text": "50 Creating function to configrure tables\nCode\ntbl_style &lt;- function(df){\n    df %&gt;% \n        gt::gt() %&gt;% \n        gt::tab_options(\n            table.font.size = 14, \n            table.font.names = \"serif\", \n            data_row.padding = gt::px(2)\n        ) %&gt;% \n        gt::opt_stylize(style = 5)\n}",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>`dplyr` Package</span>"
    ]
  },
  {
    "objectID": "pkg-dplyr.html#arrange",
    "href": "pkg-dplyr.html#arrange",
    "title": "49  dplyr Package",
    "section": "53.1 arrange",
    "text": "53.1 arrange\n\n\nCode\ndat %&gt;% \n    arrange(name, desc(day))\n\n\n# A tibble: 5 × 5\n  name     day month  year bp    \n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n1 Akosua    21    12  2010 110/76\n2 Ama       12     5  2020 120/80\n3 Kwame     14     2  2019 132/66\n4 Yaa       19     8  2000 117/77\n5 Yaw       13     3  1982 144/98",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>`dplyr` Package</span>"
    ]
  },
  {
    "objectID": "pkg-dplyr.html#unite",
    "href": "pkg-dplyr.html#unite",
    "title": "49  dplyr Package",
    "section": "53.2 unite()",
    "text": "53.2 unite()\n\n\nCode\ndat %&gt;% \n    unite(col = \"dob\", c(day, month, year), sep=\"/\") %&gt;% \n    tbl_style()\n\n\n\n\n\n\n\n\nname\ndob\nbp\n\n\n\n\nAma\n12/5/2020\n120/80\n\n\nKwame\n14/2/2019\n132/66\n\n\nAkosua\n21/12/2010\n110/76\n\n\nYaw\n13/3/1982\n144/98\n\n\nYaa\n19/8/2000\n117/77",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>`dplyr` Package</span>"
    ]
  },
  {
    "objectID": "pkg-dplyr.html#seperate",
    "href": "pkg-dplyr.html#seperate",
    "title": "49  dplyr Package",
    "section": "53.3 seperate()",
    "text": "53.3 seperate()\n\n\nCode\ndat %&gt;% \n    separate(col = bp, into = c(\"sbp\", \"dbp\"), sep = \"/\") %&gt;% \n    tbl_style()\n\n\n\n\n\n\n\n\nname\nday\nmonth\nyear\nsbp\ndbp\n\n\n\n\nAma\n12\n5\n2020\n120\n80\n\n\nKwame\n14\n2\n2019\n132\n66\n\n\nAkosua\n21\n12\n2010\n110\n76\n\n\nYaw\n13\n3\n1982\n144\n98\n\n\nYaa\n19\n8\n2000\n117\n77\n\n\n\n\n\n\n\n\n\nCode\ndat %&gt;% \n    separate(col = bp, into = c(\"sbp\", \"dbp\"), sep = \"/\") %&gt;% \n    unite(col = \"dob\", c(day, month, year), sep=\"/\") %&gt;% \n    mutate(dob_new = lubridate::dmy(dob)) %&gt;% \n    tbl_style()\n\n\n\n\n\n\n\n\nname\ndob\nsbp\ndbp\ndob_new\n\n\n\n\nAma\n12/5/2020\n120\n80\n2020-05-12\n\n\nKwame\n14/2/2019\n132\n66\n2019-02-14\n\n\nAkosua\n21/12/2010\n110\n76\n2010-12-21\n\n\nYaw\n13/3/1982\n144\n98\n1982-03-13\n\n\nYaa\n19/8/2000\n117\n77\n2000-08-19",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>`dplyr` Package</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html",
    "href": "pkg-gtsummary.html",
    "title": "50  gtsummary",
    "section": "",
    "text": "51 Reading in the data\nCode\ndf_cint_all &lt;- dget(\"C:/Dataset/cint_data_clean\")\ndf_mbu &lt;- readxl::read_xlsx(\"C:/Dataset/mbu.xlsx\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#table-1",
    "href": "pkg-gtsummary.html#table-1",
    "title": "50  gtsummary",
    "section": "55.1 Table 1",
    "text": "55.1 Table 1\n\n\nCode\ndf_cint %&gt;% \n    tbl_summary(\n        by = sex,                    # aggregate table by sex\n        missing_text = \"(Missing)\",  # Label missing data as such\n        type = sbp ~ \"continuous2\",  # Report sbp with 2 or more statistics\n        statistic = list(sbp ~ c(\"{mean},({sd})\",\n                                 \"({min},{max})\"),\n                         bmicat ~ \"{n}/{N} ({p}%)\",\n                         bulb_0 ~ c(\"{mean} ({sd})\")),\n        label = bmi ~ \"BMI (Kg/m sq.)\",   # To modify labels\n        digits = dbp ~ 1) %&gt;%         # Force dbp to have one decimal\n    add_overall(last = T) %&gt;%           # Add overall column\n    modify_spanning_header(all_stat_cols() ~ \"**Sex of Participants**\") %&gt;% \n    add_p(pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3)) %&gt;%    # Adds p-value column\n    add_q(method = \"fdr\",                   #Add p-value for multiple comparison\n          pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3)) %&gt;% \n    add_stat_label() %&gt;%        # Add specific stats to each variable\n    add_n() %&gt;%                     # Add valid observation to each variable\n    bold_p() %&gt;%       # Bold significant p-values\n    add_significance_stars()   # add significance stars\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\n\nSex of Participants\n\np-value\n1\nq-value\n2\n\n\nFemale\nN = 554\nMale\nN = 158\nOverall\nN = 712\n\n\n\n\nBulb diameter at time 0, Mean (SD)\n712\n0.90 (0.22)\n0.93 (0.20)\n0.91 (0.22)\n0.035*\n0.082\n\n\nBulb diameter at 12 months, Median (Q1, Q3)\n363\n0.83 (0.70, 0.95)\n0.85 (0.68, 0.98)\n0.83 (0.70, 0.95)\n0.683\n0.683\n\n\n    (Missing)\n\n\n262\n87\n349\n\n\n\n\n\n\nBMI (Kg/m sq.), Median (Q1, Q3)\n712\n27.1 (23.2, 31.2)\n23.0 (21.0, 25.6)\n26.1 (22.4, 30.1)\n&lt;0.001***\n&lt;0.001\n\n\nSystolic Blood Pressure\n712\n\n\n\n\n\n\n0.242\n0.423\n\n\n    Mean,(SD)\n\n\n125,(24)\n127,(24)\n125,(24)\n\n\n\n\n\n\n    (Min,Max)\n\n\n(66,231)\n(82,199)\n(66,231)\n\n\n\n\n\n\nDiastolic Blood Pressure, Median (Q1, Q3)\n712\n79.0 (70.0, 88.0)\n78.0 (69.0, 90.0)\n79.0 (70.0, 89.0)\n0.563\n0.657\n\n\nCategorized BMI, n/N (%)\n711\n\n\n\n\n\n\n&lt;0.001***\n&lt;0.001\n\n\n    Normal\n\n\n208/553 (38%)\n110/158 (70%)\n318/711 (45%)\n\n\n\n\n\n\n    High\n\n\n345/553 (62%)\n48/158 (30%)\n393/711 (55%)\n\n\n\n\n\n\n    (Missing)\n\n\n1\n0\n1\n\n\n\n\n\n\nHypertension present, n (%)\n712\n314 (57%)\n95 (60%)\n409 (57%)\n0.439\n0.615\n\n\n\n1\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n2\nFalse discovery rate correction for multiple testing",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#table-2",
    "href": "pkg-gtsummary.html#table-2",
    "title": "50  gtsummary",
    "section": "55.2 Table 2",
    "text": "55.2 Table 2\n\n\nCode\ndf_mbu_clean %&gt;% \n    tbl_summary(by = died) %&gt;% \n    add_overall(last = T) %&gt;%\n    add_p() %&gt;% \n    modify_spanning_header(all_stat_cols() ~ \"**Mortality**\") %&gt;% \n    modify_caption(caption = \"**Table 2**: MBU data by outcome\") %&gt;% \n    bold_labels()\n\n\n\n\n\n\nTable 2: MBU data by outcome\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nMortality\n\np-value\n2\n\n\nNo\nN = 4,257\n1\nYes\nN = 727\n1\nOverall\nN = 4,984\n1\n\n\n\n\nWeight (kgs)\n2.80 (2.00, 3.30)\n1.60 (1.10, 2.70)\n2.70 (1.90, 3.30)\n&lt;0.001\n\n\nAge (days)\n0.00 (0.00, 1.00)\n0.00 (0.00, 1.00)\n0.00 (0.00, 1.00)\n0.008\n\n\nSex\n\n\n\n\n\n\n0.6\n\n\n    Female\n1,887 (44%)\n329 (45%)\n2,216 (44%)\n\n\n\n\n    Male\n2,370 (56%)\n398 (55%)\n2,768 (56%)\n\n\n\n\nPlace of birth\n\n\n\n\n\n\n&lt;0.001\n\n\n    Clinic/Hospital\n601 (14%)\n195 (27%)\n796 (16%)\n\n\n\n\n    Home\n55 (1.3%)\n9 (1.2%)\n64 (1.3%)\n\n\n\n\n    KATH\n3,594 (84%)\n520 (72%)\n4,114 (83%)\n\n\n\n\n    Maternity Home\n7 (0.2%)\n3 (0.4%)\n10 (0.2%)\n\n\n\n\nAPGAR (min 1)\n6.00 (4.00, 7.00)\n4.00 (2.00, 6.00)\n6.00 (4.00, 7.00)\n&lt;0.001\n\n\nMode of delivery\n\n\n\n\n\n\n&lt;0.001\n\n\n    C/S\n2,344 (55%)\n318 (44%)\n2,662 (53%)\n\n\n\n\n    SVD\n1,790 (42%)\n398 (55%)\n2,188 (44%)\n\n\n\n\n    Vacuum\n123 (2.9%)\n11 (1.5%)\n134 (2.7%)\n\n\n\n\nGestational Age\n38.0 (35.0, 40.0)\n34.0 (29.0, 39.0)\n38.0 (34.0, 40.0)\n&lt;0.001\n\n\nAPGAR (min 5)\n8.00 (7.00, 9.00)\n6.00 (5.00, 7.00)\n8.00 (7.00, 9.00)\n&lt;0.001\n\n\nAdmission duration\n5 (3, 8)\n1 (0, 4)\n5 (2, 8)\n&lt;0.001\n\n\n\n1\nMedian (Q1, Q3); n (%)\n\n\n2\nWilcoxon rank sum test; Pearson’s Chi-squared test; Fisher’s exact test",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#table-3",
    "href": "pkg-gtsummary.html#table-3",
    "title": "50  gtsummary",
    "section": "55.3 Table 3",
    "text": "55.3 Table 3\nThis is table 3 of many😉\n1\n\n\nCode\ndf_cint %&gt;% \n    tbl_summary(\n        by = sex,                    # aggregate table by sex\n        statistic = all_continuous() ~ \"{mean} ({sd})\") %&gt;% # Stats touse for all continuous variables\n    add_stat_label(label = all_continuous()~ \"Mean(StD)\") %&gt;% # Label to give statistics\n    add_difference() %&gt;%    # Add a difeference, ci and p-value column\n    modify_spanning_header(all_stat_cols()~ \"**Sex**\") %&gt;%  # Add spanning header\n    modify_caption(\"**Table 1. Patient Characteristics**\") %&gt;% # Add table title\n    italicize_levels() %&gt;%  # Italics for the levels\n    bold_labels()   # Bold fo rhe labels\n\n\nThe following errors were returned during `modify_caption()`:\n✖ For variable `hpt` (`sex`) and \"estimate\", \"statistic\", \"p.value\",\n  \"parameter\", \"conf.low\", and \"conf.high\" statistics: Expecting `variable` to\n  be either &lt;logical&gt; or &lt;numeric/integer&gt; coded as 0 and 1.\n\n\n\n\n\n\nTable 1. Patient Characteristics\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nSex\n\nDifference\n1\n95% CI\n1,2\np-value\n1\n\n\nFemale\nN = 554\nMale\nN = 158\n\n\n\n\nBulb diameter at time 0, Mean(StD)\n0.90 (0.22)\n0.93 (0.20)\n-0.03\n-0.07, 0.01\n0.095\n\n\nBulb diameter at 12 months, Mean(StD)\n0.85 (0.23)\n0.87 (0.29)\n-0.02\n-0.10, 0.05\n0.5\n\n\n    Unknown\n262\n87\n\n\n\n\n\n\n\n\nBody Mass Index, Mean(StD)\n27.4 (5.7)\n23.7 (4.0)\n3.7\n3.0, 4.5\n&lt;0.001\n\n\nSystolic Blood Pressure, Mean(StD)\n125 (24)\n127 (24)\n-2.5\n-6.8, 1.8\n0.3\n\n\nDiastolic Blood Pressure, Mean(StD)\n80 (14)\n79 (15)\n0.35\n-2.3, 3.0\n0.8\n\n\nCategorized BMI, n (%)\n\n\n\n\n0.68\n0.50, 0.86\n\n\n\n\n    Normal\n208 (38%)\n110 (70%)\n\n\n\n\n\n\n\n\n    High\n345 (62%)\n48 (30%)\n\n\n\n\n\n\n\n\n    Unknown\n1\n0\n\n\n\n\n\n\n\n\nHypertension present, n (%)\n314 (57%)\n95 (60%)\n\n\n\n\n\n\n\n\n\n1\nWelch Two Sample t-test; Standardized Mean Difference\n\n\n2\nCI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#stratified-table",
    "href": "pkg-gtsummary.html#stratified-table",
    "title": "50  gtsummary",
    "section": "55.4 Stratified table",
    "text": "55.4 Stratified table\n\n\nCode\ndf_cint %&gt;% \n    drop_na(bmicat) %&gt;% \n    tbl_strata(\n        strata = bmicat, ~.x %&gt;%      # Add a strata to the table\n            tbl_summary(\n                by = sex\n            ) %&gt;% \n            add_p()      # P value for each strata\n    ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNormal\n\n\nHigh\n\n\n\nFemale\nN = 208\n1\nMale\nN = 110\n1\np-value\n2\nFemale\nN = 345\n1\nMale\nN = 48\n1\np-value\n2\n\n\n\n\nBulb diameter at time 0\n0.85 (0.75, 0.99)\n0.88 (0.78, 1.03)\n0.074\n0.85 (0.75, 1.03)\n0.91 (0.80, 1.05)\n0.13\n\n\nBulb diameter at 12 months\n0.80 (0.70, 0.90)\n0.84 (0.68, 0.98)\n0.2\n0.85 (0.70, 0.95)\n0.88 (0.68, 0.95)\n0.9\n\n\n    Unknown\n97\n60\n\n\n164\n27\n\n\n\n\nBody Mass Index\n22.22 (20.09, 23.51)\n21.85 (20.31, 23.11)\n0.4\n29.8 (27.5, 33.3)\n27.0 (25.9, 30.9)\n&lt;0.001\n\n\nSystolic Blood Pressure\n118 (100, 133)\n120 (107, 132)\n0.3\n123 (111, 140)\n136 (122, 158)\n&lt;0.001\n\n\nDiastolic Blood Pressure\n76 (67, 84)\n74 (66, 84)\n0.7\n80 (71, 90)\n85 (77, 95)\n0.035\n\n\nHypertension present\n97 (47%)\n57 (52%)\n0.4\n216 (63%)\n38 (79%)\n0.025\n\n\n\n1\nMedian (Q1, Q3); n (%)\n\n\n2\nWilcoxon rank sum test; Pearson’s Chi-squared test",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#specifying-tables-tests",
    "href": "pkg-gtsummary.html#specifying-tables-tests",
    "title": "50  gtsummary",
    "section": "55.5 Specifying tables tests",
    "text": "55.5 Specifying tables tests\n\n\nCode\ndf_cint %&gt;% \n    tbl_summary(by = sex,\n                statistic = list(all_continuous() ~ \"{mean} ({sd})\")\n    ) %&gt;% \n    add_p(\n        test = list(\n            all_continuous()~ \"t.test\",     # Specify T test for all continuous variables\n            all_categorical() ~ \"fisher.test\"     # Specify fisher's text for all categorical variables\n        )\n    ) %&gt;% \n    separate_p_footnotes()     # Specific p-value labelled\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nFemale\nN = 554\n1\nMale\nN = 158\n1\np-value\n\n\n\n\nBulb diameter at time 0\n0.90 (0.22)\n0.93 (0.20)\n0.0952\n\n\nBulb diameter at 12 months\n0.85 (0.23)\n0.87 (0.29)\n0.52\n\n\n    Unknown\n262\n87\n\n\n\n\nBody Mass Index\n27.4 (5.7)\n23.7 (4.0)\n&lt;0.0012\n\n\nSystolic Blood Pressure\n125 (24)\n127 (24)\n0.32\n\n\nDiastolic Blood Pressure\n80 (14)\n79 (15)\n0.82\n\n\nCategorized BMI\n\n\n\n\n&lt;0.0013\n\n\n    Normal\n208 (38%)\n110 (70%)\n\n\n\n\n    High\n345 (62%)\n48 (30%)\n\n\n\n\n    Unknown\n1\n0\n\n\n\n\nHypertension present\n314 (57%)\n95 (60%)\n0.53\n\n\n\n1\nMean (SD); n (%)\n\n\n2\nWelch Two Sample t-test\n\n\n3\nFisher’s exact test",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#customised-tables",
    "href": "pkg-gtsummary.html#customised-tables",
    "title": "50  gtsummary",
    "section": "55.6 Customised tables",
    "text": "55.6 Customised tables\n\n\nCode\ndf_cint %&gt;% \n    select(bmi, hpt, bulb_0, sbp, bmicat) %&gt;% \n    tbl_summary(\n        by = hpt,\n        statistic = list(\n            bmi ~ \"{mean} ({sd})\",\n            all_categorical() ~ \"{n} ({p})\",\n            sbp ~ \"{median} ({p25}, {p75})\",\n            bulb_0 ~ \"{mean} ({sd})\"),\n         missing_text = \"(Missing\",\n        digits = list(all_categorical() ~ c(0, 1))\n    ) %&gt;% \n    add_stat_label(label = list(\n        bmi = \"Mean(SD)\",\n        bulb_0 = \"Mean(SD)\",\n        all_categorical() ~ \"n(%)\",\n        sbp = \"Median(IQR)\"\n    )) %&gt;% \n    add_p(\n        test = list(\n            bmi ~ \"t.test\",\n            bulb_0 ~ \"t.test\",\n            all_categorical() ~ \"fisher.test\",\n            sbp ~ \"wilcox.test\"\n        )\n    ) %&gt;% \n    separate_p_footnotes()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nNo\nN = 303\nYes\nN = 409\np-value\n\n\n\n\nBody Mass Index, Mean(SD)\n25.3 (5.4)\n27.5 (5.5)\n&lt;0.0011\n\n\nBulb diameter at time 0, Mean(SD)\n0.88 (0.22)\n0.92 (0.22)\n0.0131\n\n\nSystolic Blood Pressure, Median(IQR)\n108 (98, 113)\n134 (125, 150)\n&lt;0.0012\n\n\nCategorized BMI, n(%)\n\n\n\n\n&lt;0.0013\n\n\n    Normal\n164 (54.1)\n154 (37.7)\n\n\n\n\n    High\n139 (45.9)\n254 (62.3)\n\n\n\n\n    (Missing\n0\n1\n\n\n\n\n\n1\nWelch Two Sample t-test\n\n\n2\nWilcoxon rank sum test\n\n\n3\nFisher’s exact test",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#creating-data-for-the-paired-table",
    "href": "pkg-gtsummary.html#creating-data-for-the-paired-table",
    "title": "50  gtsummary",
    "section": "55.7 Creating data for the paired table",
    "text": "55.7 Creating data for the paired table\n\n\nCode\ndf_paired_cint &lt;- \n    df_cint_all %&gt;% \n    select(sid, cart, cca_0, cca_12, ica_0, ica_12) %&gt;%\n    mutate(\n        ica_12 = case_when(ica_12 &gt; median(ica_12, na.rm=T) ~ \"High\",\n                               ica_12 &lt;= median(ica_12, na.rm=T) ~ \"Low\") %&gt;% \n            factor(),\n        ica_0 = case_when(ica_0 &gt; median(ica_0, na.rm=T) ~ \"High\",\n                               ica_0 &lt;= median(ica_0, na.rm=T) ~ \"Low\") %&gt;% \n            factor()\n    ) %&gt;% \n    select(sid, cart, cca_0, cca_12, ica_0, ica_12)\n\ndf_A &lt;-\n    df_paired_cint %&gt;% \n    pivot_longer(cols =  c(cca_0, cca_12), \n                 names_to = c(\"cca\", \"period\"), \n                 names_sep = \"_\", \n                 values_to  = \"cca_measure\") %&gt;% \n    select(sid, cart, period, cca_measure)\n\ndf_paired_long &lt;-\n    df_paired_cint %&gt;%\n    pivot_longer(cols =  c(ica_0, ica_12), \n                 names_to = c(\"ica\", \"period\"), \n                 names_sep = \"_\", \n                 values_to  = \"ica_measure\") %&gt;% \n    select(sid, cart, period, ica_measure) %&gt;% \n    full_join(df_A, by = c(\"sid\", \"period\", \"cart\"))",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#paired-table",
    "href": "pkg-gtsummary.html#paired-table",
    "title": "50  gtsummary",
    "section": "55.8 Paired table",
    "text": "55.8 Paired table\n\n\nCode\ndf_paired_long %&gt;% \n    mutate(period = case_when(period == \"0\" ~ \"Month 0\",\n                              period == \"12\" ~ \"Month 12\")) %&gt;% \n    filter(complete.cases(.)) %&gt;% \n    group_by(sid) %&gt;% \n    filter(n()==2) %&gt;% \n    ungroup() %&gt;%\n    tbl_strata(strata = cart, ~.x %&gt;%\n        tbl_summary(by = period, \n                    include = -sid,\n                    statistic = list(cca_measure ~ \"{mean} ({sd})\",\n                                     ica_measure ~ \"{n} ({p})\"),\n                    label = list(ica_measure = \"ICA\",\n                                 cca_measure = \"CCA(mm)\"),\n                    digits = list(all_categorical() ~ c(0, 1)))%&gt;% \n            add_p(test = list(ica_measure ~ \"mcnemar.test\",\n                              cca_measure ~ \"paired.t.test\"),\n                              group = sid,\n                  pvalue_fun = ~ gtsummary::style_pvalue(.x, digits = 3)) %&gt;% \n            separate_p_footnotes() %&gt;% \n            add_stat_label(label = list(ica_measure = \"n(%)\",\n                                        cca_measure = \"mean(SD)\"))\n    ) %&gt;% \n    modify_caption(\"**Table 2: Comparative month 0 and 12 measures**\")\n\n\n\n\n\n\nTable 2: Comparative month 0 and 12 measures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNaive\n\n\ncART\n\n\nCONTROL\n\n\n\nMonth 0\nN = 86\nMonth 12\nN = 86\np-value\nMonth 0\nN = 237\nMonth 12\nN = 237\np-value\nMonth 0\nN = 40\nMonth 12\nN = 40\np-value\n\n\n\n\nICA, n(%)\n\n\n\n\n0.7281\n\n\n\n\n0.7591\n\n\n\n\n0.0021\n\n\n    High\n32 (37.2)\n29 (33.7)\n\n\n154 (65.0)\n150 (63.3)\n\n\n14 (35.0)\n1 (2.5)\n\n\n\n\n    Low\n54 (62.8)\n57 (66.3)\n\n\n83 (35.0)\n87 (36.7)\n\n\n26 (65.0)\n39 (97.5)\n\n\n\n\nCCA(mm), mean(SD)\n0.83 (0.13)\n0.77 (0.14)\n0.0022\n0.93 (0.17)\n0.88 (0.16)\n&lt;0.0012\n0.84 (0.13)\n0.70 (0.11)\n&lt;0.0012\n\n\n\n1\nMcNemar’s Chi-squared test with continuity correction\n\n\n2\nPaired t-test",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#multivariate-regression",
    "href": "pkg-gtsummary.html#multivariate-regression",
    "title": "50  gtsummary",
    "section": "57.1 Multivariate regression",
    "text": "57.1 Multivariate regression\n\n57.1.1 Multiple liner regreession\n\n\nCode\ndf_cint %&gt;% \n    lm(sbp ~ ., data = .) %&gt;% \n    tbl_regression(pvalue_fun = function(x) style_pvalue(x, digits = 3)) %&gt;% \n    modify_header(update = list(estimate ~ \"**Estimate**\",\n                                label ~ \"**Variable**\")) %&gt;% \n    modify_caption(caption = \"**Table XI:** Multivariate linear regression\") %&gt;%\n    bold_labels() %&gt;% \n    as_gt() %&gt;% \n    gt::tab_options(\n        table.font.size = 14,\n        table.font.names = \"Times New Roman\",\n        data_row.padding = 1\n    ) \n\n\n\n\n\n\nTable XI: Multivariate linear regression\n\n\n\n\n\n\n\n\nVariable\nEstimate\n95% CI\n1\np-value\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n-0.25\n-3.6, 3.1\n0.883\n\n\nBulb diameter at time 0\n4.6\n-1.2, 10\n0.122\n\n\nBulb diameter at 12 months\n7.4\n1.7, 13\n0.011\n\n\nBody Mass Index\n0.05\n-0.32, 0.43\n0.781\n\n\nDiastolic Blood Pressure\n0.92\n0.79, 1.0\n&lt;0.001\n\n\nCategorized BMI\n\n\n\n\n\n\n\n\n    Normal\n—\n—\n\n\n\n\n    High\n0.19\n-4.0, 4.3\n0.930\n\n\nHypertension present\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n17\n13, 20\n&lt;0.001\n\n\n\n1\nCI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n57.1.2 Multiple variable logistic regression\n\n\nCode\ndf_mbu_clean %&gt;% \n    select(-dura_adm) %&gt;% \n    glm(died ~ ., data = ., family = binomial) %&gt;% \n    tbl_regression(exponentiate = T) %&gt;% \n    add_n(location = \"level\") %&gt;% \n    add_nevent(location = \"level\") %&gt;% \n    add_global_p() %&gt;% \n    add_q() %&gt;% \n    add_significance_stars(\n        hide_p = FALSE,\n        hide_ci = FALSE,\n        hide_se = TRUE) %&gt;% \n    add_vif() %&gt;% \n    modify_header(label = \"**Predictor**\") %&gt;% \n    modify_caption(caption = \"**Table 5: Highly customised logistic regression**\") %&gt;% \n    modify_footnote(ci = \"CI = My 95%CI\", abbreviation = TRUE) %&gt;% \n    sort_p() %&gt;% \n    bold_p(t=0.1, q=TRUE) %&gt;% \n    bold_labels() %&gt;% \n    italicize_levels()\n\n\nWarning: Use of the \"ci\" column was deprecated in gtsummary v2.0, and the column will\neventually be removed from the tables.\n! Review `?deprecated_ci_column()` for details on how to update your code.\nℹ The \"ci\" column has been replaced by the merged \"conf.low\" and \"conf.high\"\n  columns (merged with `modify_column_merge()`).\nℹ In most cases, a simple update from `ci = 'a new label'` to `conf.low = 'a\n  new label'` is sufficient.\n\n\n\n\n\n\nTable 5: Highly customised logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nN\nEvent N\nOR\n1,2\n95% CI\n2\np-value\nq-value\n3\nGVIF\n2\nAdjusted GVIF\n4,2\n\n\n\n\nAPGAR (min 5)\n4,984\n727\n0.57***\n0.51, 0.63\n&lt;0.001\n&lt;0.001\n3.8\n2.0\n\n\nWeight (kgs)\n4,984\n727\n0.51***\n0.43, 0.62\n&lt;0.001\n&lt;0.001\n2.9\n1.7\n\n\nPlace of birth\n\n\n\n\n\n***\n\n\n&lt;0.001\n&lt;0.001\n1.1\n1.0\n\n\n    Clinic/Hospital\n796\n195\n—\n—\n\n\n\n\n\n\n\n\n\n\n    Home\n64\n9\n0.66\n0.27, 1.48\n\n\n\n\n\n\n\n\n\n\n    KATH\n4,114\n520\n0.56\n0.45, 0.70\n\n\n\n\n\n\n\n\n\n\n    Maternity Home\n10\n3\n1.68\n0.30, 8.14\n\n\n\n\n\n\n\n\n\n\nGestational Age\n4,984\n727\n0.94***\n0.91, 0.97\n&lt;0.001\n0.001\n2.9\n1.7\n\n\nAge (days)\n4,984\n727\n1.05*\n1.00, 1.09\n0.032\n0.052\n1.0\n1.0\n\n\nMode of delivery\n\n\n\n\n\n*\n\n\n0.045\n0.060\n1.2\n1.0\n\n\n    C/S\n2,662\n318\n—\n—\n\n\n\n\n\n\n\n\n\n\n    SVD\n2,188\n398\n0.78\n0.64, 0.95\n\n\n\n\n\n\n\n\n\n\n    Vacuum\n134\n11\n1.00\n0.48, 1.90\n\n\n\n\n\n\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n0.095\n0.11\n1.0\n1.0\n\n\n    Female\n2,216\n329\n—\n—\n\n\n\n\n\n\n\n\n\n\n    Male\n2,768\n398\n1.17\n0.97, 1.41\n\n\n\n\n\n\n\n\n\n\nAPGAR (min 1)\n4,984\n727\n1.00\n0.92, 1.10\n&gt;0.9\n&gt;0.9\n3.9\n2.0\n\n\n\n1\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n2\nOR = Odds Ratio, CI = Confidence Interval, GVIF = Generalized Variance Inflation Factor\n\n\n3\nFalse discovery rate correction for multiple testing\n\n\n4\nGVIF2\n\n\n\n\n\n\n\n\n\n\n57.1.3 Cox proportional hazard regression\n\n\nCode\ndf_mbu_clean %&gt;% \n    coxph(Surv(dura_adm, died == \"Yes\")~., data = .) %&gt;% \n    tbl_regression(exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR\n1\n95% CI\n1\np-value\n\n\n\n\nWeight (kgs)\n0.63\n0.54, 0.74\n&lt;0.001\n\n\nAge (days)\n1.04\n1.01, 1.07\n0.021\n\n\nSex\n\n\n\n\n\n\n\n\n    Female\n—\n—\n\n\n\n\n    Male\n1.11\n0.95, 1.28\n0.2\n\n\nPlace of birth\n\n\n\n\n\n\n\n\n    Clinic/Hospital\n—\n—\n\n\n\n\n    Home\n0.55\n0.28, 1.08\n0.083\n\n\n    KATH\n0.66\n0.55, 0.78\n&lt;0.001\n\n\n    Maternity Home\n1.08\n0.34, 3.39\n0.9\n\n\nAPGAR (min 1)\n0.96\n0.90, 1.03\n0.3\n\n\nMode of delivery\n\n\n\n\n\n\n\n\n    C/S\n—\n—\n\n\n\n\n    SVD\n0.77\n0.66, 0.90\n0.001\n\n\n    Vacuum\n0.96\n0.52, 1.77\n&gt;0.9\n\n\nGestational Age\n0.97\n0.94, 1.00\n0.031\n\n\nAPGAR (min 5)\n0.68\n0.64, 0.74\n&lt;0.001\n\n\n\n1\nHR = Hazard Ratio, CI = Confidence Interval",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#merging",
    "href": "pkg-gtsummary.html#merging",
    "title": "50  gtsummary",
    "section": "58.1 Merging",
    "text": "58.1 Merging",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#stacking",
    "href": "pkg-gtsummary.html#stacking",
    "title": "50  gtsummary",
    "section": "58.2 Stacking",
    "text": "58.2 Stacking",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#multiple-comparison-table",
    "href": "pkg-gtsummary.html#multiple-comparison-table",
    "title": "50  gtsummary",
    "section": "58.3 Multiple comparison table",
    "text": "58.3 Multiple comparison table\n\n\nCode\nlibrary(titanic)\nlibrary(plotrix) #has a std.error function\n\n# create smaller version of the dataset\ndf &lt;- \n  titanic_train %&gt;%\n  select(Sex, Embarked, Age, Fare) %&gt;%\n  filter(Embarked != \"\") # deleting empty Embarked status\n\n# first, write a little function to get the 2-way ANOVA p-values in a table\n# function to get 2-way ANOVA p-values in tibble\ntwoway_p &lt;- function(variable) {\n  paste(variable, \"~ Sex * Embarked\") %&gt;%\n    as.formula() %&gt;%\n    aov(data = df) %&gt;% \n    broom::tidy() %&gt;%\n    select(term, p.value) %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    pivot_wider(names_from = term, values_from = p.value) %&gt;%\n    mutate(\n      variable = .env$variable,\n      row_type = \"label\"\n    )\n}\n\n# add all results to a single table (will be merged with gtsummary table in next step)\ntwoway_results &lt;-\n  bind_rows(\n    twoway_p(\"Age\"),\n    twoway_p(\"Fare\")\n  )\ntwoway_results\n\n\n# A tibble: 2 × 5\n           Sex Embarked `Sex:Embarked` variable row_type\n         &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1 0.00823      3.97e- 1         0.611  Age      label   \n2 0.0000000191 4.27e-16         0.0958 Fare     label   \n\n\nCode\ntbl &lt;-\n  # first build a stratified `tbl_summary()` table to get summary stats by two variables\n  df %&gt;%\n  tbl_strata(\n    strata =  Sex,\n    .tbl_fun =\n      ~.x %&gt;%\n      tbl_summary(\n        by = Embarked,\n        missing = \"no\",\n        statistic = all_continuous() ~ \"{mean} ({std.error})\",\n        digits = everything() ~ 1\n      ) %&gt;%\n      modify_header(all_stat_cols() ~ \"**{level}**\")\n  ) %&gt;%\n  # merge the 2way ANOVA results into tbl_summary table\n  modify_table_body(\n    ~.x %&gt;%\n      left_join(\n        twoway_results,\n        by = c(\"variable\", \"row_type\")\n      )\n  ) %&gt;%\n  # by default the new columns are hidden, add a header to unhide them\n  modify_header(list(\n    Sex ~ \"**Sex**\", \n    Embarked ~ \"**Embarked**\", \n    `Sex:Embarked` ~ \"**Sex * Embarked**\"\n  )) %&gt;%\n  # adding spanning header to analysis results\n  modify_spanning_header(c(Sex, Embarked, `Sex:Embarked`) ~ \"**Two-way ANOVA p-values**\") %&gt;%\n  # format the p-values with a pvalue formatting function\n  modify_fmt_fun(c(Sex, Embarked, `Sex:Embarked`) ~ style_pvalue) %&gt;%\n  # update the footnote to be nicer looking\n  modify_footnote(all_stat_cols() ~ \"Mean (SE)\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-gtsummary.html#footnotes",
    "href": "pkg-gtsummary.html#footnotes",
    "title": "50  gtsummary",
    "section": "",
    "text": "Need to insert this k3k3↩︎\n1/(2*df)↩︎",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>`gtsummary`</span>"
    ]
  },
  {
    "objectID": "pkg-nlme.html",
    "href": "pkg-nlme.html",
    "title": "51  nlme",
    "section": "",
    "text": "Code\ncint &lt;- dget(\"C:/Dataset/cint_data_11042021\")\n\n\nSince this is a large dataset we start off by selecting the variables we need, convert it to the long format, and generate the “Time” variable\n\n\nCode\ndf &lt;-\n    cint %&gt;% \n    select(\n        sid, cca_0, cca_12, sex, ageyrs, resid, physical, \n        hpt_old, cart) %&gt;% \n    gather(period, cca, cca_0:cca_12) %&gt;% \n    mutate(\n        Time = ifelse(\n            period == \"cca_0\", 0, \n            ifelse(period == \"cca_12\", 1, NA)))\n\n\nHere we begin to pick up abnormal or suspicious data escpecially in the cca. One way is to determine the changes with time\n\n\nCode\ndf2 &lt;- \n    df %&gt;% \n    arrange(sid, period) %&gt;%\n    group_by(sid) %&gt;% \n    mutate(\n        cca_prev = lag(cca, order_by = sid),\n        cca_diff = cca - cca_prev) \n\ndf2 %&gt;% \n    arrange(sid, period) %&gt;% \n    head(10) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsid\nsex\nageyrs\nresid\nphysical\nhpt_old\ncart\nperiod\ncca\nTime\ncca_prev\ncca_diff\n\n\n\n\n1\nFemale\n58\nUrban\nYes\nNo\nNaive\ncca_0\n0.925\n0\nNA\nNA\n\n\n1\nFemale\n58\nUrban\nYes\nNo\nNaive\ncca_12\n1.000\n1\n0.925\n0.075\n\n\n2\nFemale\n48\nUrban\nYes\nNo\nNaive\ncca_0\n0.975\n0\nNA\nNA\n\n\n2\nFemale\n48\nUrban\nYes\nNo\nNaive\ncca_12\n0.700\n1\n0.975\n-0.275\n\n\n3\nFemale\n40\nUrban\nYes\nNo\ncART\ncca_0\n0.850\n0\nNA\nNA\n\n\n3\nFemale\n40\nUrban\nYes\nNo\ncART\ncca_12\n0.925\n1\n0.850\n0.075\n\n\n4\nMale\n38\nUrban\nNo\nNo\nNaive\ncca_0\n0.875\n0\nNA\nNA\n\n\n4\nMale\n38\nUrban\nNo\nNo\nNaive\ncca_12\n0.750\n1\n0.875\n-0.125\n\n\n5\nFemale\n49\nUrban\nNo\nYes\ncART\ncca_0\n1.600\n0\nNA\nNA\n\n\n5\nFemale\n49\nUrban\nNo\nYes\ncART\ncca_12\n1.225\n1\n1.600\n-0.375\n\n\n\n\n\nAnd then plot the differences\n\n\nCode\ndf2 %&gt;% \n    na.omit() %&gt;% \n    ggplot(aes(x = cca_diff)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nComment: Generally there was decrease in the cca values over the two time periods. Next we summarise and visualise the data\n\n\nCode\ndf %&gt;% \n    na.omit() %&gt;% \n    ggplot(aes(x = Time, y = cca, group = sid))+\n    geom_jitter(show.legend = F, width = .01) +\n    stat_smooth(\n        formula = y ~x, \n        method = \"lm\", \n        se = FALSE, \n        col = \"grey\") +\n    theme_light()\n\n\n\n\n\n\n\n\n\nNext the UNCONDITIONAL NULL MODEL to evaluate clustering, thereby finding out if we need to even do multilevel analysis\n\n\nCode\nn0 &lt;- \n    nlme::gls(\n        cca ~ 1, data = df, na.action = na.omit, method = \"ML\")\n\nn0 %&gt;% \n    broom.mixed::tidy(conf.int=T) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.8520116\n0.004918\n173.2448\n0\n0.8423726\n0.8616507\n\n\n\n\n\n\n\nCode\nn1 &lt;- \n    nlme::lme(\n        cca ~ 1, random = ~ 1 | sid, \n        data = df, na.action = na.omit, method = \"ML\")\n\nn0 %&gt;% broom.mixed::tidy(conf.int=T) %&gt;% kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.8520116\n0.004918\n173.2448\n0\n0.8423726\n0.8616507\n\n\n\n\n\n\n\nCode\nn1 %&gt;% reghelper::ICC()\n\n\n[1] 0.3150857\n\n\nCode\nanova(n0, n1)\n\n\n   Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nn0     1  2 -869.6521 -859.6919 436.8260                        \nn1     2  3 -909.9268 -894.9866 457.9634 1 vs 2 42.27476  &lt;.0001\n\n\nSince ICC &gt; 0.05 and also the random effects CI does not include 0 there exist significant clustering to suggest we do multilevel modeling. Next we test the UNCONDITIONAL SLOPE MODEL using the FIXED slope\n\n\nCode\nn2 &lt;- \n    nlme::lme(\n        cca ~ Time, random = ~ 1 | sid, \n        data = df, \n        na.action = na.omit, \n        method = \"ML\")\n\nn2 %&gt;% \n    broom.mixed::tidy(conf.int=T) %&gt;% \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\ndf\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nfixed\nNA\n(Intercept)\n0.8629565\n0.0060113\n711\n143.554679\n0e+00\n0.8511653\n0.8747476\n\n\nfixed\nNA\nTime\n-0.0447315\n0.0086710\n362\n-5.158735\n4e-07\n-0.0617676\n-0.0276955\n\n\nran_pars\nsid\nsd_(Intercept)\n0.0979221\nNA\nNA\nNA\nNA\n0.0857260\n0.1118532\n\n\nran_pars\nResidual\nsd_Observation\n0.1268559\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\nn2 %&gt;% reghelper::ICC()\n\n\n[1] 0.3733763\n\n\nCode\nanova(n0, n1, n2)\n\n\n   Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nn0     1  2 -869.6521 -859.6919 436.8260                        \nn1     2  3 -909.9268 -894.9866 457.9634 1 vs 2 42.27476  &lt;.0001\nn2     3  4 -932.4301 -912.5098 470.2150 2 vs 3 24.50329  &lt;.0001\n\n\nNext we test the UNCONDITIONAL SLOPE MODEL using the RANDOM slope\n\n\nCode\nn3 &lt;- \n    nlme::lme(\n        cca ~ Time, \n        random = ~ Time | sid, \n        data = df, \n        na.action = na.omit, \n        method = \"ML\")\n\n\nn3 %&gt;% summary()\n\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: df \n        AIC       BIC   logLik\n  -929.0894 -899.2089 470.5447\n\nRandom effects:\n Formula: ~Time | sid\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.14902004 (Intr)\nTime        0.16324990 -0.51 \nResidual    0.05371762       \n\nFixed effects:  cca ~ Time \n                 Value   Std.Error  DF   t-value p-value\n(Intercept)  0.8629565 0.005942057 711 145.22857       0\nTime        -0.0452827 0.008766665 362  -5.16533       0\n Correlation: \n     (Intr)\nTime -0.413\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-0.85549105 -0.24307511 -0.02773692  0.23248407  1.46909846 \n\nNumber of Observations: 1075\nNumber of Groups: 712 \n\n\nCode\nn3 %&gt;% nlme::intervals(which = \"fixed\")\n\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                  lower        est.       upper\n(Intercept)  0.85130124  0.86295646  0.87461168\nTime        -0.06250667 -0.04528273 -0.02805879\n\n\nCode\nn3 %&gt;% reghelper::ICC()\n\n\n[1] 0.9442325\n\n\nCode\nanova(n0, n1, n2, n3)\n\n\n   Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nn0     1  2 -869.6521 -859.6919 436.8260                        \nn1     2  3 -909.9268 -894.9866 457.9634 1 vs 2 42.27476  &lt;.0001\nn2     3  4 -932.4301 -912.5098 470.2150 2 vs 3 24.50329  &lt;.0001\nn3     4  6 -929.0894 -899.2089 470.5447 3 vs 4  0.65926  0.7192\n\n\nNext we test the FULL MODEL using the RANDOM slope\n\n\nCode\nn4 &lt;- \n    nlme::lme(\n        cca ~ Time + hpt_old + sex + scale(ageyrs) + cart, \n        random = ~ 1 | sid, \n        data = df, \n        method = \"ML\", \n        na.action = na.omit)\n\nn4 %&gt;% \n    broom.mixed::tidy() %&gt;% \n    kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\ndf\nstatistic\np.value\n\n\n\n\nfixed\nNA\n(Intercept)\n0.8224719\n0.0101076\n706\n81.371922\n0.0000000\n\n\nfixed\nNA\nTime\n-0.0650204\n0.0087880\n362\n-7.398737\n0.0000000\n\n\nfixed\nNA\nhpt_oldYes\n0.0577155\n0.0145760\n706\n3.959637\n0.0000827\n\n\nfixed\nNA\nsexMale\n0.0290863\n0.0121127\n706\n2.401295\n0.0165947\n\n\nfixed\nNA\nscale(ageyrs)\n0.0280193\n0.0053049\n706\n5.281768\n0.0000002\n\n\nfixed\nNA\ncartcART\n0.0886504\n0.0119744\n706\n7.403327\n0.0000000\n\n\nfixed\nNA\ncartCONTROL\n-0.0159617\n0.0130719\n706\n-1.221074\n0.2224656\n\n\nran_pars\nsid\nsd_(Intercept)\n0.0745001\nNA\nNA\nNA\nNA\n\n\nran_pars\nResidual\nsd_Observation\n0.1259536\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\nn4 %&gt;% reghelper::ICC()\n\n\n[1] 0.2591817\n\n\nCode\nanova(n0, n1, n2, n3, n4) \n\n\n   Model df        AIC        BIC   logLik   Test   L.Ratio p-value\nn0     1  2  -869.6521  -859.6919 436.8260                         \nn1     2  3  -909.9268  -894.9866 457.9634 1 vs 2  42.27476  &lt;.0001\nn2     3  4  -932.4301  -912.5098 470.2150 2 vs 3  24.50329  &lt;.0001\nn3     4  6  -929.0894  -899.2089 470.5447 3 vs 4   0.65926  0.7192\nn4     5  9 -1088.4835 -1043.6628 553.2418 4 vs 5 165.39417  &lt;.0001",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Specific Packages</span>",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>`nlme`</span>"
    ]
  },
  {
    "objectID": "ss-matched-case-control.html",
    "href": "ss-matched-case-control.html",
    "title": "52  Matched Case-Control",
    "section": "",
    "text": "52.1 Objective\nTo determine if being HIV positive (exposure) is associated with malnutrition (outcome) in children on admission at a specialized nutritional rehabilitation center. A researcher intends to conduct a matched case-control study he selects children with malnutrition and matches them to controls of children without malnutrition at a ratio of 1:2.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Matched Case-Control</span>"
    ]
  },
  {
    "objectID": "ss-matched-case-control.html#hypothesis",
    "href": "ss-matched-case-control.html#hypothesis",
    "title": "52  Matched Case-Control",
    "section": "52.2 Hypothesis",
    "text": "52.2 Hypothesis\n\\(H_0\\): There is no association between being malnourished and being HIV positive",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Matched Case-Control</span>"
    ]
  },
  {
    "objectID": "ss-matched-case-control.html#formula",
    "href": "ss-matched-case-control.html#formula",
    "title": "52  Matched Case-Control",
    "section": "52.3 Formula",
    "text": "52.3 Formula\nWe use the formula below by Wang and Ji (2020).\n\\[\nn = (\\frac{Z_{\\alpha/2} + Z_\\beta}{P_1 - P_2})^2 \\times (1 + \\frac{1}{k})\n\\]\nWhere:\n\n\\(n\\) is the number of matched pairs needed\n\\(Z_{\\alpha/2}\\) is the critical value of the standard normal distribution at the desired significance level such that a 95% confidence level will correspond to 1.96\n\\(Z_{\\beta}\\) is the critical value of the standard normal distribution at the desired power such that an 80% power will correspond to a value of 0.84.\n\\(P_1\\) is the proportion of exposure in cases. This can be obtained from similar prior studies\n\\(P_0\\) is the proportion of exposure in controls, This can be obtained from similar prior studies\n\\(k\\) is the number of controls per case such that for a 1:1 match \\(k\\) =1",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Matched Case-Control</span>"
    ]
  },
  {
    "objectID": "ss-matched-case-control.html#determination",
    "href": "ss-matched-case-control.html#determination",
    "title": "52  Matched Case-Control",
    "section": "52.4 Determination",
    "text": "52.4 Determination\nAssuming in the literature the proportion of HIV positive in non-malnourished children ( \\(P_0\\)) was 0.25 and that for malnourished children (\\(P_1\\)) was 0.3. Also, the investigator decides the use a 95% confidence interval and a power of 80%. The sample size is determined as below\n\\[\nn = (\\frac{1.96 + 0.84}{0.3 - 0.25})^2 \\times (1 + \\frac{1}{2})\n\\]\nThus a minimum total of 4710 study subjects will be included. This will consist of 3140 controls and 1570 cases.\n\n\n\n\nWang, Xiaofeng, and Xinge Ji. 2020. “Sample Size Estimation in Clinical Research.” Chest 158 (1): S12–20. https://doi.org/10.1016/j.chest.2020.03.010.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Matched Case-Control</span>"
    ]
  },
  {
    "objectID": "ss-comparing-two-means.html",
    "href": "ss-comparing-two-means.html",
    "title": "53  Comparing two means",
    "section": "",
    "text": "53.1 Pre-requisite",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Comparing two means</span>"
    ]
  },
  {
    "objectID": "ss-comparing-two-means.html#pre-requisite",
    "href": "ss-comparing-two-means.html#pre-requisite",
    "title": "53  Comparing two means",
    "section": "",
    "text": "Mean and standard deviation in group 1 (\\(\\mu_1\\), \\(\\sigma_1\\))\nMean and standard deviation in group 2 (\\(\\mu_2\\), \\(\\sigma_2\\))\nAllowable Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) errors allowable\nThe ratio between the two groups (\\(r =\\frac{n_1}{n_2}\\))",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Comparing two means</span>"
    ]
  },
  {
    "objectID": "ss-comparing-two-means.html#preamble",
    "href": "ss-comparing-two-means.html#preamble",
    "title": "53  Comparing two means",
    "section": "53.2 Preamble",
    "text": "53.2 Preamble\nA researcher set out to determine the minimum sample size required for a study designed to determine if there is a significant difference in the weight of males and females in a community. To do this he researched and found out from a previous study:\n\nThe mean and SD of weight in males were 3.5kg and 1\nThe mean and SD of weight in females were 3kg and 1.2\nType I and II error rates of 0.05 and 0.2 were chosen.\nThe researcher decides based on availability to recruit twice the number of males compared to females",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Comparing two means</span>"
    ]
  },
  {
    "objectID": "ss-comparing-two-means.html#formula",
    "href": "ss-comparing-two-means.html#formula",
    "title": "53  Comparing two means",
    "section": "53.3 Formula",
    "text": "53.3 Formula\nWe use the formula1\n\\[\nn_1 = r*n_2 \\;\\text{ and }\\; n_2=\\left(1+\\frac{1}{r}\\right) \\left(\\sigma\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\mu_A-\\mu_B}\\right)^2\n\\]",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Comparing two means</span>"
    ]
  },
  {
    "objectID": "ss-comparing-two-means.html#calculation",
    "href": "ss-comparing-two-means.html#calculation",
    "title": "53  Comparing two means",
    "section": "53.4 Calculation",
    "text": "53.4 Calculation\nInputting into the formula we have\n\n\nCode\nmu1 = 3.5\nmu2 = 3\nr = 2\nsd = (1.2+1*2)/3\nalpha = 0.05\nbeta  = 0.20\n\nn2 &lt;- (1+1/r)*(sd*(qnorm(1-alpha/2)+qnorm(1-beta))/(mu1-mu2))^2\n\nn1 &lt;- r*n2\n\nglue::glue(\n    \"The number of Males and Females required are \", \n    {ceiling(n1)}, \" and \", {ceiling(n2)}, \" respectively.\")\n\n\nThe number of Males and Females required are 108 and 54 respectively.\n\n\nCode\nglue::glue(\"Total sample size = \", {ceiling(n2)+ceiling(n1)})\n\n\nTotal sample size = 162",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Comparing two means</span>"
    ]
  },
  {
    "objectID": "ss-comparing-two-means.html#footnotes",
    "href": "ss-comparing-two-means.html#footnotes",
    "title": "53  Comparing two means",
    "section": "",
    "text": "Chow S, Shao J, Wang H. 2008. Sample Size Calculations in Clinical Research. 2nd Ed. Chapman & Hall/CRC Biostatistics Series. page 58↩︎",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Comparing two means</span>"
    ]
  },
  {
    "objectID": "ss-single-mean-to-reference-value.html",
    "href": "ss-single-mean-to-reference-value.html",
    "title": "54  Mean Difference - One sample",
    "section": "",
    "text": "Testing to see if a mean is equal to a reference value\n$$ n=\\left(\\sigma\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\mu-\\mu_0}\\right)^2\n$$1-\\beta= \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right) \\quad ,\\quad z=\\frac{\\mu-\\mu_0}{\\sigma/\\sqrt{n}} $\n\n$n$ is sample size\n$\\sigma$ is standard deviation\n$\\Phi$ is the standard Normal distribution function\n$\\Phi^{-1}$ is the standard Normal quantile function\n$\\alpha$ is Type I error\n$\\beta$ is Type II error, meaning $1-\\beta$ is power\n\n\n\nCode\nmu=2\nmu0=1.5\nsd=1\nalpha=0.05\nbeta=0.20\n(n=(sd*(qnorm(1-alpha/2)+qnorm(1-beta))/(mu-mu0))^2)\n\n\n[1] 31.39552\n\n\nCode\nceiling(n)# 32\n\n\n[1] 32\n\n\nCode\nz=(mu-mu0)/sd*sqrt(n)\n(Power=pnorm(z-qnorm(1-alpha/2))+pnorm(-z-qnorm(1-alpha/2)))\n\n\n[1] 0.800001",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Sample Size</span>",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Mean Difference - One sample</span>"
    ]
  },
  {
    "objectID": "gr-histogram.html",
    "href": "gr-histogram.html",
    "title": "55  Histogram",
    "section": "",
    "text": "55.0.1 Read in Data\nWe begin by importing the data into R Studio and then summarizing it.\n\n\nCode\ndf_histo &lt;- \n    readstata13::read.dta13(\"C:/Dataset/olivia_data_wide.dta\") %&gt;% \n    select(hb1, hb2, hb3, hb4)\n\ndf_histo %&gt;% \n    summarytools::dfSummary(labels.col = F, graph.col = F) \n\n\nData Frame Summary  \ndf_histo  \nDimensions: 350 x 4  \nDuplicates: 3  \n\n-----------------------------------------------------------------------------------\nNo   Variable    Stats / Values           Freqs (% of Valid)   Valid      Missing  \n---- ----------- ------------------------ -------------------- ---------- ---------\n1    hb1         Mean (sd) : 11.3 (1.2)   57 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                              (100.0%)   (0.0%)   \n                 8.3 &lt; 11.3 &lt; 16.6                                                 \n                 IQR (CV) : 1.8 (0.1)                                              \n\n2    hb2         Mean (sd) : 11.2 (1.3)   63 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                              (100.0%)   (0.0%)   \n                 6.1 &lt; 11 &lt; 15.6                                                   \n                 IQR (CV) : 1.8 (0.1)                                              \n\n3    hb3         Mean (sd) : 11.1 (1.2)   57 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                              (100.0%)   (0.0%)   \n                 8 &lt; 11.1 &lt; 15.2                                                   \n                 IQR (CV) : 1.8 (0.1)                                              \n\n4    hb4         Mean (sd) : 11.8 (2.5)   89 distinct values   350        0        \n     [numeric]   min &lt; med &lt; max:                              (100.0%)   (0.0%)   \n                 3.5 &lt; 11.5 &lt; 24.4                                                 \n                 IQR (CV) : 2.4 (0.2)                                              \n-----------------------------------------------------------------------------------\n\n\n\n\n55.0.2 Simple histogram\n\n\nCode\ndf_histo %&gt;% \n    ggplot(aes(x = hb1)) +\n    geom_histogram(\n        col = \"red\", \n        fill = \"snow1\", \n        bins = 12) +\n    labs(\n        x = \"Hemoglobin (mg/dl)\", \n        y = \"Frequency\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 55.1: Distribution of the first hemoglobins concentration\n\n\n\n\n\n\n\n55.0.3 Histogram with normal curve\n\n\nCode\ndf_histo %&gt;% \n    ggplot(\n        aes(x = hb1)) + \n    geom_histogram(\n        aes(y = after_stat(density)),\n        breaks = seq(7.5, 17.5, by = 1), \n        colour = \"blue\", \n        fill = \"white\") +\n    stat_function(\n        fun = dnorm, \n        args = list(mean = mean(df_histo$hb1), sd = sd(df_histo$hb1)),\n        color = 'red')+\n    labs(\n        x = \"Hemoglobin (mg/dl)\", \n        y = \"Density\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 55.2: Distribution of the first hemoglobins concentration\n\n\n\n\n\n\n\n55.0.4 Panel histogram\n\n\nCode\ndf_temp &lt;- \n    df_histo %&gt;% \n    pivot_longer(cols = c(hb1, hb2, hb3, hb4)) %&gt;% \n    drop_na(value) %&gt;% \n    mutate(\n        name = factor(\n            name, \n            levels = c(\"hb1\", \"hb2\", \"hb3\", \"hb4\"),\n            labels = c(\"First HB\", \"Second HB\", \"Third HB\", \"Fourth HB\")))\n\ndf_temp %&gt;% \n    ggplot(\n        aes(x = value)) + \n    geom_histogram(\n        aes(y = after_stat(density)),\n        breaks = seq(7.55, 17.5, by = 1), \n        colour = \"blue\", \n        fill = \"white\", \n        bins = 10) +\n    stat_function(\n        fun = dnorm, \n        args = list(\n            mean = mean(df_temp$value), sd = sd(df_temp$value)),\n        color = 'red')+\n    labs(\n        x = \"Hemoglobin (mg/dl)\", \n        y = \"Density\") +\n    theme_bw()+\n    facet_wrap(\n        facets = .~name)+\n    theme(\n        text = element_text(family = \"serif\"),\n        strip.text = element_text(face = \"bold\", color = \"white\"),\n        strip.background = element_rect(fill = \"#4C4CBD\"),\n        plot.title = element_text(face = 'bold'))\n\n\n\n\n\n\n\n\nFigure 55.3: Distribution of the first hemoglobins concentration",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Histogram</span>"
    ]
  },
  {
    "objectID": "gr-densityplot.html",
    "href": "gr-densityplot.html",
    "title": "56  Density Plot",
    "section": "",
    "text": "56.0.1 Import dataset\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\ndf_bp &lt;- \n    readstata13::read.dta13(\"C:/Dataset/BP.dta\") %&gt;% \n    select(sex, sbp, dbp, saltadd) %&gt;% \n    pivot_longer(\n        cols = c(sbp, dbp), \n        values_to = \"pressure\", \n        names_to = \"bp_type\")\n\ndataF &lt;- readstata13::read.dta13(\"C:/Dataset/olivia_data_wide.dta\")\n\n\n\n\n56.0.2 Primary density plot\n\n\nCode\ndf_bp %&gt;% \n    filter(bp_type == \"sbp\") %&gt;% \n    ggplot(aes(x = pressure)) +\n    geom_density(\n        color = \"blue\", \n        fill = 'red', \n        linetype = \"dashed\", \n        alpha = 0.2) +\n    theme_light()\n\n\n\n\n\n\n\n\nFigure 56.1: Density plot of the Systolic blood pressures\n\n\n\n\n\n\n\n56.0.3 Density with separate colors\n\n\nCode\ndf_bp %&gt;% \n    filter(bp_type == \"sbp\") %&gt;% \n    drop_na(saltadd) %&gt;% \n    ggplot(aes(x = pressure, color = saltadd, fill = saltadd)) +\n    geom_density(\n        linetype = \"dashed\", alpha = 0.2) +\n    theme_light()+\n    scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nFigure 56.2: Density plot of the Systolic blood pressures\n\n\n\n\n\n\n\n56.0.4 Densityplot with facets\n\n\nCode\ndf_bp %&gt;% \n    drop_na(saltadd) %&gt;% \n    ggplot(aes(x = pressure, color = saltadd)) +\n    geom_density(aes(fill = saltadd), linetype = \"dashed\", alpha = 0.2) +\n    theme_light()+\n    scale_color_brewer(palette = \"Dark2\") +\n    facet_grid(bp_type ~ sex)\n\n\n\n\n\n\n\n\nFigure 56.3: Density plot of the Systolic blood pressures\n\n\n\n\n\n\n\n56.0.5 ggridges plot\n\n\nCode\ndf_bp %&gt;% \n    drop_na(saltadd) %&gt;% \n    ggplot(aes(x = pressure, fill = bp_type)) +\n    ggridges::geom_density_ridges(aes(y = saltadd), alpha = .3) +\n    labs(x = \"Pressure\", \n         y = \"Salt added to diet\") +\n    ggridges::theme_ridges(font_size = 12) +\n    scale_fill_discrete(\n        name = \"Blood Pressure Type\", \n        labels = c(\"sbp\" = \"Systolic\", \"dbp\" = \"Diastolic\")) +\n    theme(legend.position = \"right\")\n\n\nPicking joint bandwidth of 11.5\n\n\n\n\n\n\n\n\nFigure 56.4: Comparative Systolic and Diastolic Blood Pressue for salt addiotin and sex\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(mcv1, mcv2, mcv3, mcv4, mcv5, agecat, id) %&gt;%\n    pivot_longer(cols = mcv1:mcv5, names_to = \"Time\", values_to = \"MCV\") %&gt;% \n    ggplot(aes(x = MCV, fill = Time)) +\n    ggridges::geom_density_ridges(aes(y = agecat), alpha = .5) +\n    labs(x = \"MCV\", \n         y = \"Age Group Category (years)\",\n         title = \"Sequential changes in MCV over the study duration per age category\") +\n    ggridges::theme_ridges() +\n    scale_fill_discrete(name = \"Measure\", \n                        labels = c(\"mcv1\" = \"First\", \n                                   \"mcv2\" = \"Second\", \n                                   \"mcv3\" = \"Third\",\n                                   \"mcv4\" = \"Fourth\",\n                                   \"mcv5\" = \"Fifth\")) +\n    theme(legend.position = \"right\")\n\n\nPicking joint bandwidth of 3.72",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Density Plot</span>"
    ]
  },
  {
    "objectID": "gr-boxplot.html",
    "href": "gr-boxplot.html",
    "title": "57  Boxplot",
    "section": "",
    "text": "First we read in the data\n\n\nCode\nrm(list = ls(all = TRUE))\ndat &lt;- \n    foreign::read.dta(\"C:/Dataset/bea_organ_damage_28122013.dta\")\ndataF &lt;- \n    readstata13::read.dta13(\"C:/Dataset/olivia_data_wide.dta\")\n\n\nWarning in readstata13::read.dta13(\"C:/Dataset/olivia_data_wide.dta\"): \n   Factor codes of type double or float detected in variables\n\n   anemia1, anemia2, anemia3, anemia4, anemia5\n\n   No labels have been assigned.\n   Set option 'nonint.factors = TRUE' to assign labels anyway.\n\n\n\n58 Boxplot\nNext we select three variables for plotting, keep only the complete cases and then store the ggplot() into an object called BP.\n\n\nCode\nBP &lt;- \n  dat %&gt;% \n  select(q12weight, q2idtype, q3sex) %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(x = q2idtype, y = q12weight, fill = q3sex))\n\n\nNext we draw our boxplot with axis labels, title, axes format, and color specification.\n\n\nCode\nBP + \n  geom_boxplot() +\n    theme_test() +\n    labs(title=\"My Boxplot\", x=\"Case or Control\", y=\"Weight (hgs)\") + \n    theme(plot.title = element_text(size=15, face=\"bold\"), \n          axis.text.x = element_text(size=12), \n          axis.text.y = element_text(size=12),\n          axis.title.x = element_text(size=13),\n          axis.title.y = element_text(size=13)) + \n  scale_color_discrete(name = \"Sex\")\n\n\n\n\n\n\n\n\n\nNewt we set up a similar boxplot but this time use the color option for the ggplot() and not the fill option.\n\n\nCode\nBP &lt;- \n  dat %&gt;% \n  select(q12weight, q2idtype, q3sex) %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(x = q2idtype, y = q12weight, color = q3sex))\n\n\n\n\nCode\nBP + \n  geom_boxplot() +\n    theme_light() +\n    labs(title = \"My Boxplot\", \n         x = \"Case or Control\", \n         y = \"Weight (hgs)\") + \n    theme(plot.title=element_text(size=15, face=\"bold\"), \n          axis.text.x=element_text(size=12), \n          axis.text.y=element_text(size=12),\n          axis.title.x=element_text(size=13),\n          axis.title.y=element_text(size=13)) + \n  scale_color_discrete(name=\"Sex\")\n\n\n\n\n\n\n\n\n\nCode\nrm(BP, dat)\n\n\nHere we use a different dataset to draw the next boxplot\n\n\nCode\ndf1 &lt;- read.csv(\"C:/Dataset/booking1.csv\")\nglimpse(df1)\n\n\nRows: 50\nColumns: 4\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n$ sex    &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Female\", \"Female\", \"…\n$ weight &lt;int&gt; 4, 10, 8, 14, 16, 9, 23, 14, 7, 13, 16, 10, 7, 13, 13, 10, 17, …\n$ height &lt;int&gt; 68, 81, 78, 90, 106, 85, 120, 98, 69, 88, 89, 81, 79, 93, 109, …\n\n\nNext we plot two boxplots on one graph\n\n\nCode\ndf1 %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(x = sex)) + \n    geom_boxplot(aes(y = weight, color=\"red\")) + \n    geom_boxplot(aes(y = height, color = \"steelblue\")) + \n  labs(color = \"Anthropometrics\") +\n  scale_color_manual(labels = c(\"Weight\",\"Height\"),\n                     values = c(\"red\", \"steelblue\"))\n\n\n\n\n\n\n\n\n\nCode\nrm(df1)\n\n\nWe then use the ToothGrowth data for for the next few boxplots\n\n\nCode\ndata(ToothGrowth)\nToothGrowth &lt;-\n  ToothGrowth %&gt;% \n  mutate(dose = factor(dose))\nglimpse(ToothGrowth)\n\n\nRows: 60\nColumns: 3\n$ len  &lt;dbl&gt; 4.2, 11.5, 7.3, 5.8, 6.4, 10.0, 11.2, 11.2, 5.2, 7.0, 16.5, 16.5,…\n$ supp &lt;fct&gt; VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, V…\n$ dose &lt;fct&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 1, 1, 1, 1, …\n\n\nAnd then we form the ggplot object\n\n\nCode\np &lt;- \n  ToothGrowth %&gt;% \n  ggplot(aes(x = dose, y = len))\n\n\nOther renditions of the boxplot is as shown below. First rotated one\n\n\nCode\np + geom_boxplot() + coord_flip()     # Axis rotated\n\n\n\n\n\n\n\n\n\nNotched boxplot\n\n\nCode\np + geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\nCustomizaton of the outlier\n\n\nCode\np + geom_boxplot(outlier.colour=\"red\", \n                 outlier.shape=8, \n                 outlier.size=4)\n\n\n\n\n\n\n\n\n\nWe add a statistic to the plot here\n\n\nCode\np + geom_boxplot() + \n    stat_summary(fun = mean, \n                 geom = \"point\", \n                 shape = 18, \n                 size = 4, \n                 col = \"red\")\n\n\n\n\n\n\n\n\n\nAnd then limit the categories the x axis\n\n\nCode\np + \n  geom_boxplot() + \n  scale_x_discrete(limits=c(\"0.5\", \"2\"))\n\n\nWarning: Removed 20 rows containing missing values or values outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nNext a boxplot with a superimposed dotplot\n\n\nCode\np + \n  geom_boxplot() + \n  geom_dotplot(binaxis='y', \n               stackdir='center', \n               dotsize=0.5, \n               binwidth = 1,\n               col = \"red\",\n               fill = \"red\")\n\n\n\n\n\n\n\n\n\nAnd a boxplot with superimposed jittered points\n\n\nCode\np + \n  geom_boxplot() +\n  geom_jitter(shape=16, position=position_jitter(0.2))\n\n\n\n\n\n\n\n\n\nCode\nrm(p)\n\n\nNext we manually set out own color scale\n\n\nCode\nP &lt;- \n  ToothGrowth %&gt;% \n  ggplot(aes(factor(dose), len, color=dose))\n\n\n\n\nCode\nP + \n    geom_boxplot() +\n  scale_color_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\"))\n\n\n\n\n\n\n\n\n\nAnd use one of the color scales\n\n\nCode\nP + \n    geom_boxplot() +\n    scale_color_brewer(palette=\"Dark2\") +\n    scale_fill_brewer(palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nChange the legend position\n\n\nCode\nP + \n  geom_boxplot() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nAnd remove legend\n\n\nCode\nP + \n  geom_boxplot() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nChange the order of items in the legend\n\n\nCode\nP + \n  geom_boxplot() +\n  scale_x_discrete(limits=c(\"2\", \"0.5\", \"1\"))\n\n\n\n\n\n\n\n\n\nBox plot with multiple groups: Change box plot colors by groups\n\n\nCode\nggplot(ToothGrowth, aes(x=dose, y=len, fill=supp)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nCode\nrm(P)\n\n\nChange the position of the boxes\n\n\nCode\nP &lt;- \n  ggplot(ToothGrowth, aes(x=dose, y=len, fill=supp)) + \n    geom_boxplot(position=position_dodge(1))\nP\n\n\n\n\n\n\n\n\n\nAnd then we add dots\n\n\nCode\nP + geom_dotplot(binaxis='y', \n                 stackdir='center', \n                 position=position_dodge(1),\n                 binwidth = 1)\n\n\n\n\n\n\n\n\n\nCustomized\n\n\nCode\ndataF %&gt;% \n    select(mcv1, mcv2, mcv3, mcv4, mcv5, agecat, id) %&gt;%\n    pivot_longer(cols = mcv1:mcv5, names_to = \"Time\", values_to = \"MCV\") %&gt;% \n    ggplot(aes(x = Time, y = MCV, col = Time), fill = \"snow1\") +\n    geom_boxplot(\n        outlier.color = 'black', \n        outlier.shape = 23, \n        outlier.fill = \"steelblue2\", \n        outlier.size = 2) +\n    stat_summary(\n        aes(fill=Time), \n        fun.data = mean_se, \n        geom = \"pointrange\", \n        size=0.5, \n        shape =23, \n        color = \"black\", \n        show.legend = F) +\n    scale_color_manual(\n        name = \"Measure\", \n        values = c(\"red\", \"yellow\", \"green\", \"violet\", \"brown\"),\n        labels = c(\"First\",\"Second\", \"Third\", \"Fourth\", \"Fifth\")) +\n    scale_x_discrete(\n        labels =c(\n            \"mcv1\" = \"First MCV\", \n            \"mcv2\" = \"Second MCV\", \n            \"mcv3\" = \"Third MCV\", \n            \"mcv4\" = \"Fourth MCV\", \n            \"mcv5\" = \"Fifth MCV\")) +\n    labs(title = \"Distribution of MCVs over the review periods\") +\n    theme(\n        plot.title = element_text(\n            family = \"serif\", \n            face = \"bold.italic\", \n            size = 14, \n            colour = \"steelblue4\", \n            hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(mcv1, mcv2, mcv3, mcv4, mcv5, agecat, id) %&gt;%\n    pivot_longer(cols = mcv1:mcv5, names_to = \"Time\", values_to = \"MCV\") %&gt;% \n    ggplot(aes(x = Time, y = MCV, col = Time), fill = \"snow1\") +\n    geom_boxplot(outlier.color = \"white\", outlier.alpha = 0) +\n    geom_jitter(width =.2, alpha = .2, col=1) +\n    labs(\n        x = \"Time of Sample taking\", \n        y = \"Mean Corpuscular Volume\",\n        title = \"Sequential changes in MCV over the study duration\") +\n    theme_bw() +\n    scale_x_discrete(\n        labels = c(\n            \"mcv1\" = \"First MCV\", \n            \"mcv2\" = \"Second MCV\", \n            \"mcv3\" = \"Third MCV\",\n            \"mcv4\" = \"Fourth MCV\",\n            \"mcv5\" = \"Fifth MCV\"))\n\n\n\n\n\n\n\n\nFigure 58.1: Sequential changes in MCV over the study duration\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(mcv1, mcv2, mcv3, mcv4, mcv5, agecat, id) %&gt;%\n    pivot_longer(cols = mcv1:mcv5, names_to = \"Time\", values_to = \"MCV\") %&gt;% \n    ggplot(aes(x = Time, y = MCV, col = Time), fill = \"snow1\") +\n    ggbeeswarm::geom_beeswarm() +\n    labs(x = \"Time of Sample taking\", \n         y = \"Mean Corpuscular Volume\",\n         title = \"Sequential changes in MCV over the study duration\") +\n    theme_bw() +\n    scale_x_discrete(\n        labels = c(\n            \"mcv1\" = \"First MCV\", \"mcv2\" = \"Second MCV\", \"mcv3\" = \"Third MCV\", \n            \"mcv4\" = \"Fourth MCV\", \"mcv5\" = \"Fifth MCV\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(hb1, hb2, hb3, hb4, hb5, agecat, id) %&gt;%\n    pivot_longer(cols = hb1:hb5, names_to = \"Time\", values_to = \"hb\") %&gt;% \n    ggplot(aes(x = Time, y = hb, color = Time)) +\n    geom_boxplot()+\n    scale_x_discrete(\n        name = NULL,\n        labels = c(\n            \"hb1\" = \"First HB\", \"hb2\" = \"Second HB\", \n            \"hb3\" = \"Third HB\", \"hb4\" = \"Fourth HB\", \n            \"hb5\" = \"Fifth HB\"))+\n    scale_y_continuous(\n        name = expression(paste('Hemoglobin Concentration (', mu, 'g/dl)')),\n        limits = c(0, 30), \n        breaks = seq(0, 30, 5), \n        expand = c(0,0))+\n    scale_color_discrete(\n        name = NULL, \n        labels = c(\n            \"hb1\" = \"First HB\", \"hb2\" = \"Second HB\", \n            \"hb3\" = \"Third HB\", \"hb4\" = \"Fourth HB\", \n            \"hb5\" = \"Fifth HB\"))+\n    guides(color=guide_legend(ncol=2,title = NULL))+\n    theme_classic()+\n    theme(\n        text = element_text(family = \"serif\", size = 13),\n        legend.background = element_rect(color = \"black\"),\n        legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.85),\n        legend.direction = \"horizontal\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Boxplot</span>"
    ]
  },
  {
    "objectID": "gr-barplot.html",
    "href": "gr-barplot.html",
    "title": "58  Barplot",
    "section": "",
    "text": "58.1 Basic barplot\nNext, we make the basic Barplot\nCode\np &lt;- \n  df %&gt;% \n  ggplot(aes(x = dose, y = len)) \n\np + geom_bar(stat=\"identity\")\nNext we flip the barplot horizontal, change the size of the bar width and change the theme\nCode\np + \n  geom_bar(\n      stat = \"identity\", \n      width = 0.8, \n      color = \"blue\", \n      fill = \"grey90\") +\n  coord_flip() +\n  theme_bw()\nNext we limit the observations to just two\nCode\np + \n  geom_bar(\n      stat = \"identity\", \n      width = 0.8, \n      color = \"black\", \n      fill = \"steelblue\") +\n  scale_x_discrete(limits=c(\"D0.5\", \"D2\"))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\nNext we put labels on the bars at the outside and inside\nCode\np +\n    geom_bar(stat=\"identity\", fill=\"steelblue\")+\n    geom_text(\n        aes(label=len), \n        vjust=-0.5, \n        size=4, \n        col = \"black\")+\n    theme_minimal()\nCode\n# Change barplot line colors by groups\np &lt;- \n    ggplot(df, aes(x = dose, y = len, color = dose)) + \n    geom_bar(stat = \"identity\", fill = \"white\") + \n    geom_text(aes(label=len), vjust=1.5, size=5, col = \"red\")\np",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Barplot</span>"
    ]
  },
  {
    "objectID": "gr-barplot.html#use-custom-color-palettes",
    "href": "gr-barplot.html#use-custom-color-palettes",
    "title": "58  Barplot",
    "section": "58.2 Use custom color palettes",
    "text": "58.2 Use custom color palettes\n\n\nCode\np + scale_color_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\"))\n\n\n\n\n\n\n\n\n\nUse brewer color palettes\n\n\nCode\np + scale_color_brewer(palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\np + scale_color_grey() + theme_classic()\n\n\n\n\n\n\n\n\n\nChange barplot fill colors by groups\n\n\nCode\n# \np &lt;- \n  ggplot(df, aes(x=dose, y=len, fill=dose)) +\n  geom_bar(stat=\"identity\")+theme_minimal()\np\n\n\n\n\n\n\n\n\n\nUse custom color palettes\n\n\nCode\np + scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\np + \n  scale_fill_brewer(palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nUse grey scale\n\n\nCode\np + scale_fill_grey()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x=dose, y=len, fill=dose))+\n    geom_bar(stat=\"identity\", color=\"black\")+\n    scale_fill_manual(values=c(\"#999999\", \"#E69F00\", \"#56B4E9\"))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nChange bar fill colors to blues\n\n\nCode\np + scale_fill_brewer(palette=\"Blues\")\n\n\n\n\n\n\n\n\n\nCode\n    p + theme(legend.position=\"top\")\n\n\n\n\n\n\n\n\n\nCode\n    p + theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\n\n\nCode\n    p + theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nChange position of bars\n\n\nCode\np + \n  scale_x_discrete(limits=c(\"D2\", \"D0.5\", \"D1\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndat &lt;- foreign::read.dta(\"C:/dataset/bea_organ_damage_28122013.dta\")\nBC &lt;- \n  dat %&gt;% \n  select(q2idtype, q3sex) %&gt;% \n  na.omit() %&gt;% \n  group_by(q2idtype, q3sex) %&gt;% \n  summarize(Freq = n()) %&gt;% \n  ggplot(aes(x=q2idtype, y=Freq, fill=q3sex))\n\n\n`summarise()` has grouped output by 'q2idtype'. You can override using the\n`.groups` argument.\n\n\nNext we draw the barplot using the economist theme from the ggthemes package\n\n\nCode\nBC +  \n  geom_bar(stat=\"identity\", position= position_dodge()) +\n    geom_text(aes(label=Freq), vjust=1.6, color=\"black\", \n              size=4, position = position_dodge(0.9)) +\n    scale_fill_brewer(palette=\"Reds\") + \n    labs(title=\"My Barplot\", x=\"Case or Control\", y=\"Frequency\") +\n    scale_color_discrete(name=\"Sex\") + \n    ggthemes::theme_stata()\n\n\n\n\n\n\n\n\n\nNext we plot a baroplot with error bars. To do that we first we form the ggplot object that we call BC.\n\n\nCode\nBC &lt;-\n  dat %&gt;% \n  select(Type = q2idtype, Sex = q3sex, q12weight) %&gt;% \n  na.omit() %&gt;% \n  group_by(Type, Sex) %&gt;% \n  summarize(Mean.wgt = mean(q12weight), SD.wgt = sd(q12weight)) %&gt;% \n  ggplot(aes(x=Type, y=Mean.wgt, fill=Sex))\n\n\n`summarise()` has grouped output by 'Type'. You can override using the\n`.groups` argument.\n\n\nAnd then plot the graph\n\n\nCode\nBC +  \n    geom_bar(stat=\"identity\", position=position_dodge()) + \n    geom_errorbar(\n        aes(ymin = Mean.wgt-SD.wgt, ymax = Mean.wgt+SD.wgt), \n        width=.2, \n        size=0., \n        position=position_dodge(.9)) +\n    labs(\n        title=\"Mean weight with error bars\", \n        x=\"Case or Control\", \n        y=\"Mean(kgs)\") +\n    scale_fill_brewer(palette=\"Paired\") + \n    ggthemes::theme_stata()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 &lt;- data.frame(supp=rep(c(\"VC\", \"OJ\"), each=3),\n                dose=rep(c(\"D0.5\", \"D1\", \"D2\"),2),\n                len=c(6.8, 15, 33, 4.2, 10, 29.5))\nhead(df2)\n\n\nTRUE   supp dose  len\nTRUE 1   VC D0.5  6.8\nTRUE 2   VC   D1 15.0\nTRUE 3   VC   D2 33.0\nTRUE 4   OJ D0.5  4.2\nTRUE 5   OJ   D1 10.0\nTRUE 6   OJ   D2 29.5\n\n\n\n\nCode\ndf2 %&gt;% \n  ggplot(aes(x=dose, y=len, fill=supp)) +\n  geom_bar(stat=\"identity\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf2 %&gt;% \n  ggplot(aes(x=dose, y=len, fill=supp)) + \n  geom_bar(stat=\"identity\", position=position_dodge())\n\n\n\n\n\n\n\n\n\nChange color manually\n\n\nCode\np &lt;- \n  ggplot(data=df2, aes(x=dose, y=len, fill=supp)) +\n  geom_bar(stat=\"identity\", color=\"black\", position=position_dodge()) +\n  scale_fill_manual(values=c('#999999','#E69F00')) +\n  theme_minimal()\np\n\n\n\n\n\n\n\n\n\nCreate some data\n\n\nCode\ndf_sorted &lt;- \n  tibble(supp = factor(rep(c(\"VC\", \"OJ\"), each=3)),\n         dose = rep(c(\"0.5\", \"1\", \"2\"),2),\n         len = c(6.8, 15, 33, 4.2, 10, 29.5)) %&gt;% \n  arrange(dose, supp) %&gt;% \n  group_by(dose) %&gt;% \n  mutate(label_ypos=cumsum(len))\ndf_sorted\n\n\n# A tibble: 6 × 4\n# Groups:   dose [3]\n  supp  dose    len label_ypos\n  &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 OJ    0.5     4.2        4.2\n2 VC    0.5     6.8       11  \n3 OJ    1      10         10  \n4 VC    1      15         25  \n5 OJ    2      29.5       29.5\n6 VC    2      33         62.5\n\n\n\n\nCode\ndf_sorted %&gt;% \n  ggplot(aes(x=dose, y=len, fill=supp)) +\n  geom_bar(stat=\"identity\")+\n  geom_text(aes(y=label_ypos, label=len), vjust=1.6, \n            color=\"white\", size=3.5)+\n  scale_fill_brewer(palette=\"Paired\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPlotting barplot with x-axis treated as continuous variable\n\n\nCode\n# \ndf_sorted %&gt;% \n  mutate(dose = as.numeric(dose)) %&gt;% \n  ggplot(aes(x=dose, y=len, fill=supp)) +\n  geom_bar(stat=\"identity\", position=position_dodge())+\n  scale_fill_brewer(palette=\"Paired\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Axis treated as discrete variable\ndf_sorted %&gt;% \n  mutate(dose = as.factor(dose)) %&gt;% \n  ggplot(aes(x=dose, y=len, fill=supp)) +\n  geom_bar(stat=\"identity\", position=position_dodge())+\n  scale_fill_brewer(palette=\"Paired\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nToothGrowth %&gt;% \n  mutate(dose = as.factor(dose)) %&gt;% \n  group_by(supp, dose) %&gt;% \n  summarise(sd = sd(len), len = mean(len), .groups = \"drop\") %&gt;% \n  ggplot(aes(x=dose, y=len, fill=supp)) + \n  geom_bar(stat=\"identity\", position=position_dodge()) +\n  geom_errorbar(aes(ymin=len-sd, ymax=len+sd), width=.2, \n                position=position_dodge(.9)) +\n  labs(title=\"Plot of length  per dose\", x=\"Dose (mg)\", y = \"Length\")+\n  scale_fill_brewer(palette=\"Paired\") + \n  theme_minimal()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Barplot</span>"
    ]
  },
  {
    "objectID": "gr-barplot.html#plotting-raw-data",
    "href": "gr-barplot.html#plotting-raw-data",
    "title": "58  Barplot",
    "section": "58.3 Plotting raw data",
    "text": "58.3 Plotting raw data\n\n\nCode\nbabies &lt;- \n    dget(\"babies_clean\") %&gt;% \n    select(apgar5cat, died)\n\nbabies %&gt;% \n    ggplot(aes(x = apgar5cat, fill = apgar5cat)) +\n    geom_bar(stat = 'count') +\n    scale_fill_grey(\n        name = \"APGAR Category\",\n        label = c(\"LOW\", \"MEDIUM\", \"HIGH\"))+\n    theme_bw()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Barplot</span>"
    ]
  },
  {
    "objectID": "gr-scatter.html",
    "href": "gr-scatter.html",
    "title": "59  Scatter Plot",
    "section": "",
    "text": "Code\ndat &lt;- foreign::read.dta(\"C:/Dataset/bea_organ_damage_28122013.dta\")\n\ndataF &lt;- readstata13::read.dta13(\"C:/Dataset/olivia_data_wide.dta\")\n\ndf1 &lt;- read.csv(\"C:\\\\Users\\\\Sbngu\\\\Dropbox\\\\Data for book\\\\booking1.csv\")\n\nSC &lt;- \n  dat %&gt;% \n  select(q12weight, q13waist, q3sex, q10what) %&gt;% \n  drop_na() %&gt;%\n  ggplot(aes(x = q12weight, y = q13waist, color = q3sex))\n\n\n\n\nCode\nSC + \n    geom_point(shape = \"diamond\", size = 2) +\n    labs(\n        x = \"Weight (kgs)\", \n        color = \"Gender\",\n        y = \"Waist Circumference (cms)\") +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n    theme(\n        axis.title.x = element_text(\n            vjust = 0, \n            size = 14, \n            color = \"blue\", \n            face = \"italic\"),\n        axis.title.y = element_text(\n          vjust = 2, \n          size = 14, \n          color = \"firebrick\", \n          face = \"bold\"),\n        axis.text = element_text(\n          color = \"dodgerblue\", size = 12),\n        axis.text.x = element_text(face = \"italic\"))\n\n\n\n\n\n\n\n\nFigure 59.1: Relationship between weight and waist circunference\n\n\n\n\n\n\n\nCode\nSC +    \n  geom_point(aes(shape = q3sex)) + \n    geom_smooth(method=lm, formula = y~x, se=F) + \n    theme_bw(base_family = \"serif\") +\n    labs(\n        x = \"Weight (kgs)\", \n        y = \"Waist Circumference (cms)\") + \n    theme(\n        plot.title=element_text(size=15, face=\"bold\"), \n        axis.text.x=element_text(size=12), \n        axis.text.y=element_text(size=12),\n        axis.title.x=element_text(size=13),\n        axis.title.y=element_text(size=13), \n        plot.background = element_rect(fill = \"grey90\"),\n        panel.background = element_rect(fill = \"snow1\")) +\n    scale_color_discrete(name=\"Sex\") +\n    scale_shape_discrete(name=\"Sex\")\n\n\n\n\n\n\n\n\nFigure 59.2: Relationship between weight and waist circunference\n\n\n\n\n\n\n\nCode\nSC +    \n    geom_point() + \n    geom_smooth(method=loess, se=TRUE, formula = y~x) + \n    theme_classic() +\n    labs(\n        x = \"Weight (kgs)\", \n        y = \"Waist Circumference (cms)\") + \n    theme(\n        plot.title=element_text(size=15, face=\"bold\"), \n        axis.text.x=element_text(size=12), \n        axis.text.y=element_text(size=12),\n        axis.title.x=element_text(size=13),\n        axis.title.y=element_text(size=13)) + \n  scale_color_discrete(name=\"Sex\")\n\n\n\n\n\n\n\n\nFigure 59.3: Relationship between weight and waist circunference\n\n\n\n\n\n\n\nCode\nSC +    \n  geom_point() + \n    geom_smooth(method=lm, formula = y ~ x, se=F) + \n    theme_minimal() +\n    labs(\n        x = \"Weight (kgs)\", \n        y = \"Waist Circumference (cms)\") + \n    theme(\n        plot.title=element_text(size=15, face=\"bold\"), \n        axis.text.x=element_text(size=12), \n        axis.text.y=element_text(size=12),\n        axis.title.x=element_text(size=13),\n        axis.title.y=element_text(size=13)) + \n    scale_color_discrete(name = \"Sex\") + \n    facet_wrap(~q10what)\n\n\n\n\n\n\n\n\nFigure 59.4: Relationship between weight and waist circunference\n\n\n\n\n\n\n\nCode\nSC + \n  geom_point(color = \"firebrick\") +\n  labs(\n      x = \"Weight (kgs)\", \n      y = \"Waist Circumference (cms)\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\n\n\n\n\nFigure 59.5: Relationship between weight and waist circunference\n\n\n\n\n\nLimit Axes Range\n\n\nCode\ndf1 %&gt;% \n    drop_na(weight, height) %&gt;% \n  ggplot(aes(x = weight, y = height)) +\n    geom_point(aes(color = sex, shape = sex)) + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = T) + \n    labs(x = \"Weight (kgs)\", y = \"Height (cm)\") +\n    theme_light() +\n    theme(\n        plot.title=element_text(size = 16, face = \"bold\"), \n        axis.text.x=element_text(size = 12), \n        axis.text.y=element_text(size = 12),\n        axis.title.x=element_text(size = 12, face = \"bold\"),\n        axis.title.y=element_text(size = 12, face = \"bold\")) + \n    scale_color_discrete(name=\"Gender\") + \n    scale_shape_discrete(name=\"Gender\")\n\n\n\n\n\n\n\n\nFigure 59.6: Plot of Weight versus height\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    mutate(\n        mcv_cat_1 = case_when(\n            mcv1 &lt; 80 ~ \"Microcyte\",\n            mcv1 &gt;= 80 & mcv1 &lt;= 90 ~ \"Normal\",\n            mcv1 &gt; 90 ~ \"Macrocyte\") %&gt;% \n            factor(levels = c(\"Microcyte\", \"Normal\", \"Macrocyte\"))) %&gt;% \n    ggplot(aes(x = hb1, y = hb2)) +\n    geom_point(aes(size = mcv_cat_1, col = mcv_cat_1), alpha = .5) +\n    geom_smooth(formula = y ~ x, method = \"lm\", color = \"black\") +\n    geom_vline(\n        xintercept = 10, \n        color = \"red\", \n        linewidth = 0.5, \n        linetype = \"dashed\") +\n    geom_hline(\n        yintercept = 10, \n        color = \"red\", \n        linewidth = 0.5, \n        linetype = \"dashed\") +\n    geom_abline(\n        intercept = 0, \n        slope = 1, \n        color = \"brown\", \n        linewidth = 0.5, \n        linetype = \"dashed\") +\n    labs(x = \"First Hemoglobin\", y = \"Second Hemoglobin\") +\n    theme_light()\n\n\n\n\n\n\n\n\nFigure 59.7: Relationship between first and second platelet counts showing possible outliers\n\n\n\n\n\n\n\nCode\ndataF.2 &lt;- \n    tibble(\n    lbls = c(\"Kofi\",\"Ama\", \"Yaw\",\"Sammy\", \"Abena\"),\n    hgts = c(176, 154, 136, 144, 165),\n    wgts = c(65, 76,48,77, 65))\n\ndataF.2 %&gt;% \n    ggplot(aes(x = hgts, y = wgts))+\n    geom_point() +\n    annotate(\n        \"text\", \n        x = dataF.2$hgts, \n        y = dataF.2$wgts, \n        label = dataF.2$lbls, \n        vjust = 1, col=1:5) +\n    labs(\n        title = \"Height vrs Weight\", \n        subtitle = \"Yes we can\", \n        caption = \"2020 Data\")+\n    ggthemes::theme_gdocs() +\n    scale_y_continuous(\n        name = \"Weight (kgs)\", \n        limits = c(40, 80), \n        breaks = c(45, 50, 55, 60, 65, 70, 75)) +\n    scale_x_continuous(\n        name = \"Height (kgs)\", \n        limits = c(130, 200), \n        breaks = seq(130, 190, 10)) +\n    geom_label(aes(label = lbls), col = \"grey\", nudge_y = 3)\n\n\n\n\n\n\n\n\nFigure 59.8: Height v. Weight\n\n\n\n\n\n\n\nCode\ndataG &lt;- \n    dataF %&gt;% \n    mutate(is_outlier = (plt1&lt;50 | plt2&lt;100 | plt2&gt;400 | plt1&gt;400)) \n\ndataG %&gt;% \n    ggplot(aes(x = plt1, y = plt2, col = is_outlier)) + \n    geom_point() +\n    labs(\n        x = \"First Platelet Count\",\n        y = \"Second Platelet Count\",\n        title = \"Relationship between first and second platelet counts showing possible outliers\")+\n    theme_bw()+\n    ggrepel::geom_label_repel(\n        data = filter(dataG, is_outlier == TRUE), \n        aes(label=id)) +\n    theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nFigure 59.9: Relationship between first and second platelet counts showing possible outliers\n\n\n\n\n\n\n\nCode\np1 &lt;-\n    dataF %&gt;% \n    ggplot(aes(x = hb1, y = hb2))+\n    geom_point(col = \"maroon\") +\n    geom_smooth(formula = y~x, method = \"lm\") +\n    theme_bw() +\n    labs(\n        x = \"First HgB measurement (g/dL)\",\n        y = \"Second HgB measurement (g/dL)\")\n\np2 &lt;- \n    dataF %&gt;% \n    ggplot() +\n    geom_histogram(aes(x=hb1),bins = 12, col=\"black\", fill = \"grey\") +\n    labs(x=NULL, y=NULL) +\n    theme_void() +\n    theme(\n        axis.ticks.y = element_blank(),\n        axis.ticks.x  = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank())\n\np3 &lt;- \n    dataF %&gt;% \n    ggplot() +\n    geom_histogram(aes(x=hb2),bins = 12, col=\"black\", fill = \"grey\") +\n    coord_flip()+\n    labs(x=NULL, y=NULL) +\n    theme_void() +\n    theme(\n        axis.ticks.y = element_blank(),\n          axis.ticks.x  = element_blank(),\n          axis.line.x = element_blank(),\n          axis.line.y = element_blank(),\n          axis.text.x = element_blank(),\n          axis.text.y = element_blank())\n\ncol1 &lt;- (p2/p1) + plot_layout(heights = c(1,4))\ncol2 &lt;- (plot_spacer()/p3) + plot_layout(heights = c(1,4))\n(col1 | col2) +  \n    plot_layout(widths = c(5,1)) +\n    plot_annotation(caption = \"Source: 2021 Data\")\n\n\n\n\n\n\n\n\nFigure 59.10: My special scatterplot with histograms of first and secon HgB\n\n\n\n\n\n\n\nCode\np1 &lt;-\n    dataF %&gt;% \n    ggplot(aes(x=hb1, y = hb2, col = fpreg)) +\n    geom_point() +\n    geom_density_2d(color = \"blue\")\n\np2 &lt;-\n    dataF %&gt;% \n    ggplot(aes(x=hb1, y = hb3, col = fpreg)) +\n    geom_point() +\n    geom_density_2d(color = \"blue\")\n\n(p1 + p2) +\n    plot_annotation(\n        title = \"My special title is here\",\n        subtitle = \"Yes it is here\",\n        caption = \"Why not!\",\n        theme = theme(\n            plot.title = element_text(family =\"serif\", colour = \"red\"),\n            plot.subtitle = element_text(\n                family = \"serif\", color = \"red\", face = \"italic\")),\n        tag_levels = \"A\") + \n    plot_layout(widths = c(1, 2),guides = \"collect\")\n\n\n\n\n\n\n\n\nFigure 59.11: Combining plots\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    mutate(hct3 = ifelse(hct3 &lt; 20, hct3 +40, hct3),\n           hct3 = ifelse(hct3 &gt; 60, hct3 - 20, hct3)) %&gt;% \n    ggplot(aes(x = hct3, y = hb3)) +\n    geom_point(color = \"grey45\") +\n    geom_smooth(aes(x = hct3, y = hb3, col =  \"Observed\"), \n                formula = y~x, method = \"lm\", se = F) + \n    geom_segment(aes(x = min(hct3), y = min(hct3/3), \n                     xend = max(hct3), yend = max(hct3/3), \n                     col =  \"Expected\"))+\n    labs(title = \"Relationship between the third HB and HCT measurements\",\n         subtitle = \"Comparison of observed  and expected regression line if HCT = 3*HB\",\n         x = \"Hematocrit (%)\", y = \"Hemoglobin (mg/dl)\", \n         color = \"Regression Line\") +\n    theme_classic()+\n    theme(plot.title = element_text(face = \"bold\"),\n          plot.subtitle = element_text(face = \"italic\"))\n\n\nWarning in geom_segment(aes(x = min(hct3), y = min(hct3/3), xend = max(hct3), : All aesthetics have length 1, but the data has 350 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    ggplot(size = 0.5) +\n    geom_point(aes(hb3, mcv3, color = mcv1, size = occup, shape = educ)) + \n    guides(color = guide_colorbar(title = \"First MCV\"),\n           shape = guide_legend(title = \"Educational level\"), \n           size = guide_legend(title = \"Occupation\"))\n\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;%\n    mutate(mari = fct_collapse(mari, \n                             \"Married\" = c(\"Married\",\"Cohabiting\"),\n                             \"Single\" = c(\"Widowed\", \"Divorced\"))) %&gt;% \n    ggplot()+\n    geom_point(aes(x = avehb, y = avehct, color = mari), show.legend = F) +\n    geom_smooth(aes(x = avehb, y = avehct), se=F, formula = y~x, \n                method = \"lm\", size = 1, alpha = .5, col = \"grey\")+\n    facet_wrap(~mari, nrow = 2, strip.position = \"left\") +\n    labs(y = \"Hematocrit (%)\", x = \"Hemoglobin (g/dl)\", \n         title = str_glue(\"Relationship between Blood hemoglobin and \",\n                          \"Hematocrit stratified by marital status\"))+\n    theme(panel.background = element_blank(),\n          panel.grid = element_blank(),\n          axis.line = element_line(),\n          strip.placement = \"outside\",\n          strip.background = element_rect(fill = \"#c1d3fe\", color = \"black\"),\n          strip.text = element_text(size = 10, face = \"bold\"),\n          text = element_text(family = \"serif\"),\n          axis.text = element_text(size = 10, face = \"bold\"),\n          axis.title = element_text(size = 10, face = \"bold.italic\"),\n          plot.title = element_text(face = \"bold\"))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Scatter Plot</span>"
    ]
  },
  {
    "objectID": "gr-line-plot.html",
    "href": "gr-line-plot.html",
    "title": "60  Line Graphs",
    "section": "",
    "text": "Code\ndataD &lt;- readxl::read_excel(\"C:/Dataset/rainfall.xlsx\")\n\nthe_year &lt;- 2001\ndataD %&gt;% \n    janitor::clean_names() %&gt;% \n    rename(date_1 = time) %&gt;% \n    arrange(date_1) %&gt;% \n    mutate(\n        year_1 = lubridate::year(date_1),\n        day = lubridate::day(date_1), \n        mth = lubridate::month(date_1),\n        the_year = year_1 == the_year) %&gt;%\n    group_by(year_1) %&gt;% \n    mutate(cum_rainfall = cumsum(rainfall)) %&gt;% \n    ungroup() %&gt;% \n    mutate(new_date = lubridate::ymd(str_glue(\"2000-{mth}-{day}\"))) %&gt;% \n    ggplot(\n        aes(x = new_date, y = cum_rainfall, \n            group = year_1, \n            size = the_year)) +\n    geom_line(aes(col = the_year)) +\n    labs(\n        y = \"cumulative Rainfall (mm)\")+\n    scale_x_date(name = NULL, date_breaks = \"1 month\", date_labels = \"%b\") +\n    scale_color_manual(\n        name = \"Year\", \n        labels = c(\"Others\", the_year), \n        values = c(\"grey\",\"red\"))+\n    scale_size_manual(breaks = c(F,T), values = c(0.5,0.7), guide = \"none\")+\n    scale_y_continuous(breaks = seq(0,1500, 250), expand = c(0,50)) +\n    theme_classic()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 60.1: Cumulative rainfall pattern in Kumasi, Ghana (2000 - 2004)\n\n\n\n\n\n\n\nCode\ndataF &lt;- \n    readstata13::read.dta13(\n        \"C:/Dataset/olivia_data_wide.dta\",\n        nonint.factors = TRUE)\n\ndataF %&gt;% \n    group_by(educ) %&gt;% \n    summarize(across(c(mcv1, mcv2, mcv3, mcv4, mcv5), mean)) %&gt;% \n    pivot_longer(col = mcv1:mcv5) %&gt;% \n    mutate(\n        illitrate = ifelse(educ==\"None\", \"Illitrate\",\"Educated\") %&gt;% \n            factor()) %&gt;% \n    ggplot(aes(x = name, y = value, col = illitrate, group = educ)) +\n    geom_line(aes(size=illitrate)) +\n    geom_point(size = 1.5)+\n    labs(title = \"Evolution of mean platelets count over the five measurements\",\n         y = \"Count\",x = NULL)+\n    scale_color_manual(name = \"Educational Status\", values = c(\"grey50\",\"red\"), \n                       label = c(\"Educated\",\"Illitrate\")) +\n    scale_size_manual(name = \"Educational Status\", values = c(0.5, 1)) +\n    scale_y_continuous(limits = c(70, 120)) +\n    theme_bw()+\n    theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Line Graphs</span>"
    ]
  },
  {
    "objectID": "gr-curvelinear-plot.html",
    "href": "gr-curvelinear-plot.html",
    "title": "61  Curvilinear Plots",
    "section": "",
    "text": "61.1 Linear\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)+\n    theme_bw()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Curvilinear Plots</span>"
    ]
  },
  {
    "objectID": "gr-curvelinear-plot.html#polynomial",
    "href": "gr-curvelinear-plot.html#polynomial",
    "title": "61  Curvilinear Plots",
    "section": "61.2 Polynomial",
    "text": "61.2 Polynomial\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = F)+\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"lm\", formula = y ~ poly(x, 3))+\n    theme_bw()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Curvilinear Plots</span>"
    ]
  },
  {
    "objectID": "gr-curvelinear-plot.html#loess",
    "href": "gr-curvelinear-plot.html#loess",
    "title": "61  Curvilinear Plots",
    "section": "61.3 Loess",
    "text": "61.3 Loess\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"loess\", formula = y ~ x)+\n    theme_bw()",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Curvilinear Plots</span>"
    ]
  },
  {
    "objectID": "gr-curvelinear-plot.html#splines",
    "href": "gr-curvelinear-plot.html#splines",
    "title": "61  Curvilinear Plots",
    "section": "61.4 Splines",
    "text": "61.4 Splines\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"gam\", formula = y ~ splines::ns(x, 2))+\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"gam\", formula = y ~ splines::ns(x, 3))+\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"gam\", formula = y ~ splines::bs(x, 3))+\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_data1 %&gt;% \n    ggplot(aes(x = weight, y = height)) +\n    geom_point()+\n    geom_smooth(method = \"lm\", formula = y ~ quantreg::qr(.5))+\n    theme_bw()\n\n\nWarning: Failed to fit group -1.\nCaused by error:\n! 'qr' is not an exported object from 'namespace:quantreg'",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Curvilinear Plots</span>"
    ]
  },
  {
    "objectID": "gr-forest-plot.html",
    "href": "gr-forest-plot.html",
    "title": "62  Forest Plot",
    "section": "",
    "text": "Forest plot example\n\n\nCode\nlabel &lt;- paste0(\"X\", 1:6)\nmean  &lt;- c(1.29,0.76,2.43,1.68,1.22,1.7) \nlower &lt;- c(0.84,0.50,1.58,1.1,0.8,1.11)\nupper &lt;- c(1.95,1.16,3.67,2.54,1.85,2.56)\n\ndf &lt;- data.frame(label, mean, lower, upper)\n\n# reverses the factor level ordering for labels after coord_flip()\ndf$label &lt;- factor(df$label, levels=rev(df$label))\n\nlibrary(ggplot2)\nfp &lt;- ggplot(data=df, aes(x=label, y=mean, ymin=lower, ymax=upper)) +\n        geom_pointrange() + \n        geom_hline(yintercept=1, lty=2) +  # add a dotted line at x=1 after flip\n        coord_flip() +  # flip coordinates (puts labels on y axis)\n        xlab(\"Label\") + ylab(\"Mean (95% CI)\") +\n        theme_bw()  # use a white background\nprint(fp)",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Forest Plot</span>"
    ]
  },
  {
    "objectID": "gr-population-pyramid.html",
    "href": "gr-population-pyramid.html",
    "title": "63  Population Pyramid",
    "section": "",
    "text": "63.0.0.1 Population Pyramid\n\n\nCode\nreadxl::read_xlsx(\n        \"C:/Dataset/ghana.xlsx\",\n        skip = 3, \n        sheet = \"2000 - 2020\", \n        range = \"A4:ARD262\") %&gt;% \n    select(ends_with(\"20\")) %&gt;% \n    filter(BTOTL_2020==29340248) %&gt;% \n    pivot_longer(cols = BTOTL_2020:F80PL_2020) %&gt;% \n    filter(!name %in% c(\"BTOTL_2020\", \"MTOTL_2020\", \"FTOTL_2020\") ) %&gt;% \n    filter(!str_detect(name, \"^B\")) %&gt;% \n    mutate(\n        sex = str_extract(name, \"^\\\\w\"),\n        agegrp = str_c(str_sub(name, 2, 3),\"-\", str_sub(name, 4,5)),\n        Population = ifelse(sex == \"F\", -value, value)) %&gt;% \n    ggplot(aes(x = agegrp, y = Population, fill = sex)) + \n    geom_bar(stat = \"identity\") + \n    labs(x = \"Age Grouping in Years\", y = NULL, fill = \"Gender\") +\n    scale_y_continuous(\n        breaks = seq(-2000000, 2000000, 500000), \n        labels = paste0(as.character(c(seq(2.0, 0.5, -0.5), \n        seq(0, 2.0, 0.5))), \"m\")) + \n    coord_flip() +\n    scale_fill_brewer(palette = \"Set1\", labels = c(\"Female\",\"Male\")) + \n    theme_classic()\n\n\n\n\n\n\n\n\nFigure 63.1: Population pyramid of Ghana (2020)",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Population Pyramid</span>"
    ]
  },
  {
    "objectID": "gr-miscellaneous.html",
    "href": "gr-miscellaneous.html",
    "title": "64  Miscellaneous",
    "section": "",
    "text": "Code\ndataF %&gt;% \n    ggplot(aes(x = hb1, y = hb2, color = educ, shape = educ)) +\n    geom_point() +\n    geom_smooth(formula = y ~ x, method = \"lm\") +\n    theme_light()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(starts_with(\"hb\"), fpreg) %&gt;% \n    GGally::ggpairs(columns = 1:5, aes(colour = fpreg)) + \n    scale_fill_brewer(palette = \"Set1\") +\n    scale_colour_brewer(palette = \"Set1\")\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    mutate(mcv_cat_1 = case_when(mcv1 &lt; 80 ~ \"Microcyte\",\n                                 mcv1 &gt;= 80 & mcv1 &lt;= 90 ~ \"Normal\",\n                                 mcv1 &gt; 90 ~ \"Macrocyte\") %&gt;% \n               factor(levels = c(\"Microcyte\", \"Normal\", \"Macrocyte\"))) %&gt;% \n    ggplot(aes(x = hb1, y = hb2)) +\n    geom_point(aes(size = mcv_cat_1, col = mcv_cat_1), alpha = .5) +\n    geom_smooth(formula = y ~ x, method = \"lm\", color = \"black\") +\n    geom_vline(xintercept = 10, color = \"red\", size = 0.5, linetype = \"dashed\") +\n    geom_hline(yintercept = 10, color = \"red\", size = 0.5, linetype = \"dashed\") +\n    geom_abline(intercept = 0, slope = 1, color = \"brown\", size = 0.5, \n                linetype = \"dashed\") +\n    labs(x = \"First Hemoglobin\", \n         y = \"Second Hemoglobin\", \n         title = \"Hemoglobin on Hemoglobin Distribution\") +\n    theme_light()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(mcv1, mcv2, mcv3, mcv4, mcv5, agecat, id) %&gt;%\n    pivot_longer(cols = mcv1:mcv5, names_to = \"Time\", values_to = \"MCV\") %&gt;% \n    ggplot(aes(x = Time, y = MCV, col = Time), fill = \"snow1\") +\n    geom_boxplot(outlier.color = \"white\", outlier.alpha = 0) +\n    geom_jitter(width =.2, alpha = .2, col=1) +\n    labs(x = \"Time of Sample taking\", \n         y = \"Mean Corpuscular Volume\",\n         title = \"Sequential changes in MCV over the study duration\") +\n    theme_bw() +\n    scale_x_discrete(labels =c(\"mcv1\" = \"First MCV\", \n                               \"mcv2\" = \"Second MCV\", \n                               \"mcv3\" = \"Third MCV\",\n                               \"mcv4\" = \"Fourth MCV\",\n                               \"mcv5\" = \"Fifth MCV\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    ggplot(aes(x = mcv1, y = mch1, size = mcv1)) +\n    geom_point(shape = 23, aes(fill = agecat), alpha =.2) +\n    scale_x_continuous(name = \"Initial Mean Corpuscular Volume\",\n                       breaks = seq(50, 130, 10),\n                       labels = c(\"50.0\",\"60.0\",\"70.0\", \"80.0\", \"90.0\", \"100.0\", \n                                  \"110.0\",\"120.0\", \"130.0\"), \n                       position = \"top\") +\n    scale_y_continuous(name = \"Mean Corpuscular Hemoglobin\", \n                       limits = c(15,45),\n                       breaks = c(15, 30, 45), \n                       labels = c(\"15.00\",\"30.00\",\"45.00\"),\n                       position = \"right\") +\n    scale_fill_manual(name = \"Age Category\", \n                      values = c(\"blue\", \"red\", \"green\", \"brown\")) +\n    scale_size_continuous(name = \"MCV\",\n                          range = c(1,4), \n                          limits = c(50, 140), \n                          breaks = c(80, 120, 140),\n                          labels = c(\"Microcyte\", \"Normocyte\", \"Macrocyte\"))\n\n\n\n\n\n\n\n\n\n\n64.0.0.1 Elipse to show grouping\n\n\nCode\ndataF %&gt;% \n    ggplot(size = 0.5) +\n    geom_point(aes(hb3, mcv3, color = mcv1, size = occup, shape = educ)) + \n    guides(color = guide_colorbar(title = \"First MCV\"),\n           shape = guide_legend(title = \"Educational level\"), \n           size = guide_legend(title = \"Occupation\"))\n\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.2 Histogram with density overlay\n\n\nCode\ndataF %&gt;% \n    ggplot(aes(x = hb1, y = ..density..)) +\n    geom_histogram(fill = \"skyblue\", col = \"black\", bins = 15)+\n    geom_density(aes(y = ..density..), col = \"red\", size= 1) +\n    labs(x = \"First HB\", y = \"Density\", title = \"Distribution of HgB\")+\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.3 Histogram with normal overlay\n\n\nCode\ndataF %&gt;% \n    ggplot(aes(x = hb2))+\n    geom_histogram(aes(y = ..density..), bins=10, fill = \"snow\", col = \"red\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = mean(dataF$hb2, na.rm=T), \n                              sd = sd(dataF$hb2)), col = \"blue\",\n                  size = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.4 Scatter plot by the various age groups - Facetting\n\n\nCode\nagecat_label &lt;- \n    c(\"Age: 10-19 years\", \"Age: 20-29 years\",\n      \"Age: 30-39 years\",\"Age: 40-49 years\")\n\nnames(agecat_label) &lt;- c(\"10-19\", \"20-29\", \"30-39\", \"50-59\")\n\ndataF %&gt;%\n    ggplot(aes(hb3, mcv3), size = 0.5) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y~x) +\n    labs(\n        title = \"Relationship between hemoglobin and mean corpuscular volume\",\n        x =  \"Hemoglobin (mg/dl)\",\n        y = \"Mean Corpuscular Volume (fl)\")+\n    theme_bw()+\n    facet_wrap(facets = .~agecat, labeller = labeller(agecat = agecat_label))+\n    theme(\n        text = element_text(family = \"serif\"),\n        strip.text = element_text(face = \"bold\"),\n        strip.background = element_rect(fill = \"white\"),\n        plot.title = element_text(face = 'bold'))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    ggplot(aes(hb3, mcv3), size = 0.5) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y~x) +\n    labs(\n        title = \"Relationship between hemoglobin and mean corpuscular volume\",\n        x =  \"Hemoglobin (mg/dl)\",\n        y = \"Mean Corpuscular Volume (fl)\")+\n    theme_bw() +\n    facet_grid(occup ~ agecat, labeller = labeller(agecat = agecat_label))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    ggplot(aes(hb3, mcv3), size = 0.5) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y~x) +\n    labs(\n        title = \"Relationship between hemoglobin and mean corpuscular volume\",\n        x =  \"Hemoglobin (mg/dl)\",\n        y = \"Mean Corpuscular Volume (fl)\")+\n    theme_bw() +\n    facet_wrap(c(\"occup\", \"agecat\"), nrow = 3, labeller = labeller(agecat = agecat_label))\n\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.5 Venn Diagram\n\n\nCode\ndf &lt;- \n    tibble(x=c(1,2,3), y=c(2,3.5,2), lab1 = c(\"A\",\"B\",\"C\"), counts = 1:3)\ndf %&gt;% \n    ggplot() +\n    ggforce::geom_circle(\n        aes(x0 = x, y0 = y, r = 1.4, fill = lab1), \n        alpha = .3, \n        size = 1, \n        colour = 'grey') +\n    coord_fixed() +\n    theme_void() +\n    scale_fill_manual(values = c('cornflowerblue', 'firebrick',  'gold')) +\n    scale_colour_manual(values = c('cornflowerblue', 'firebrick', 'gold'), \n                        guide = \"none\") +\n    labs(fill = NULL, title = \"My Venn Diagram I\") + \n     annotate(\"text\", \n              x = c(2, 3.5, 2.7, 1.2,   2,  2, 0.3), \n              y = c(4,   2, 2.7, 2.8, 2.5, 1.5, 1.8), \n              label = 1:7, size = 5, fontface = \"bold\") +\n    theme(\n        plot.title = element_text(\n            hjust = 0.5, \n            family = \"serif\", \n            face = \"bold\", \n            size = 16, \n            colour = \"red\"),\n          legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.6 Correlation Plot\n\n\nCode\ndataF %&gt;% \n    select(hb1:hb3, hb4, hb5, hct1:hct3, hct4, hct5) %&gt;% \n    cor() %&gt;% \n    ggcorrplot::ggcorrplot(hc.order = FALSE, \n           type = \"lower\", \n           lab = TRUE, \n           lab_size = 3, \n           method=\"square\", \n           colors = c(\"tomato2\", \"white\", \"springgreen3\"), \n           title=\"Correlogram of blood indices\", \n           ggtheme=theme_bw)\n\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.7 Plotting Prediction Interval\n\n\nCode\ndataLM &lt;- dataF %&gt;% select(hct4, hb4)\n\nlm(hb4 ~ hct4, data = dataLM) %&gt;% \n    predict(interval = \"predict\") %&gt;% \n    as_tibble() %&gt;% \n    bind_cols(dataLM) %&gt;% \n    ggplot(aes(x = hct4, y = hb4)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = y~x, se=T)+\n    geom_line(aes(y = lwr), col = \"coral2\", linetype = \"dashed\") +\n    geom_line(aes(y = upr), col = \"coral2\", linetype = \"dashed\") +\n    labs(title = \"Relationship between HB4 and HCT4 with fillted line, prediction and se intervals\",\n         x = \"HCT 4 (%)\", y = \"HB 4 (mg/dl)\", caption = \"Nurse Data 2015\")+\n    theme_bw()\n\n\nWarning in predict.lm(., interval = \"predict\"): predictions on current data refer to _future_ responses\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.8 Color Palletes\n\n\nCode\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    group_by(anemia1, agecat) %&gt;% \n    summarise_each(funs(Mean = mean, SD = sd, se=sd(.)/sqrt(n())), hb1:hb2) %&gt;% \n    mutate(Anemia.1 = case_when(anemia1 == 0 ~ \"No\",\n                                anemia1 == 1 ~ \"Yes\") %&gt;% as_factor()) %&gt;% \n    ggplot(aes(x=Anemia.1, y=hb1_Mean, fill = agecat)) +  \n    geom_errorbar(aes(ymin = hb1_Mean - 1.96*hb1_SD, ymax = hb1_Mean + 1.96*hb1_SD),\n                  position = position_dodge(0.9), width = 0.2, size = 0.8) + \n    geom_bar(stat = \"identity\", position = position_dodge(0.9), col = \"black\") +\n    labs(x = \"First Anemia Present\", y = \"Mean of First HgB (mg/dL)\",\n         title = \"Average initial HgB for first anemia and Age Categories\") +\n    theme_bw()+\n    scale_fill_brewer(name = \"Age Group\", palette = \"Dark2\",\n                      labels= c(\"10-19 yrs\", \"20-29 yrs\", \"30-39 yrs\", \"40-49 yrs\"))\n\n\nWarning: `summarise_each()` was deprecated in dplyr 0.7.0.\nℹ Please use `across()` instead.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.9 Adding text as labels, etc\n\n\nCode\ntemp &lt;-\n    dataF %&gt;% \n    group_by(educ) %&gt;% \n    summarize(across(c(hb1, hb2, hb3, hb4, hb5), mean)) \n\ntemp.2 &lt;- tibble(x = rep(5,4), y = temp$hb5, z = temp$educ)\n    \ntemp %&gt;% \n    pivot_longer(col = hb1:hb5, names_to = \"Period\", values_to = \"hgb\") %&gt;% \n    ggplot(aes(x = Period, y = hgb)) +\n    geom_line(aes(color = educ, group = educ), size = 1)+\n    geom_point(aes(color = educ, group = educ, shape = educ), size =2)+\n    labs(title = \"Average Hemoglobin for each educational level\", x=NULL)+\n    scale_y_continuous(name = \"Hemoglobin (mg/dL)\", limits = c(10,16)) +\n    scale_x_discrete(labels = c(\"hb1\" = \"First \\nMeasure\",\n                                \"hb2\" = \"Second \\nMeasure\",\n                                \"hb3\" = \"Third \\nMeasure\",\n                                \"hb4\" = \"Fourth \\nMeasure\",\n                                \"hb5\" = \"Fifth \\nMeasure\"))+\n    ggrepel::geom_label_repel(data = temp.2, aes(x = x, y = y, label = z))+\n    theme_bw()+\n    theme(legend.position = \"none\",\n          plot.title = element_text(family=\"serif\",colour = \"red\", \n                               hjust =0.5, face = \"bold\", size=16))\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.10 Highlighting one line\n\n\n64.0.0.11 Adding text to special observations\n\n\nCode\ndataG &lt;- \n    dataF %&gt;% \n    mutate(is_outlier = (plt1&lt;50 | plt2&lt;100 | plt2&gt;400 | plt1&gt;400)) \n\ndataG %&gt;% \n    ggplot(aes(x = plt1, y = plt2, col = is_outlier)) + \n    geom_point() +\n    labs(x = \"First Platelet Count\",\n         y = \"Second Platelet Count\",\n         title = \"Relationship between first and second platelet counts showing possible outliers\")+\n    theme_bw()+\n    ggrepel::geom_label_repel(data = filter(dataG, is_outlier == TRUE), \n                              aes(label=id)) +\n    theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.12 Regression line with prediction and regression lines\n\n\nCode\ndataH &lt;- \n    readxl::read_xlsx(\n        \"C:/Dataset/Red cell indices against ferritin.xlsx\"\n        ) %&gt;% \n    mutate(\n        lg.fer = log(Ferritin), \n        MCH = ifelse(is.na(MCH), median(MCH, na.rm=T), MCH)\n        )\n\npreds &lt;- \n    rbind(\n        predict(lm(lg.fer ~ RBC, data = dataH), interval = \"prediction\"),\n        predict(lm(lg.fer ~ HGB, data = dataH), interval = \"prediction\"), \n        predict(lm(lg.fer ~ HCT, data = dataH), interval = \"prediction\"),\n        predict(lm(lg.fer ~ MCV, data = dataH), interval = \"prediction\"), \n        predict(lm(lg.fer ~ MCH, data = dataH), interval = \"prediction\")\n        ) %&gt;% \n    as_tibble()\n\ndataH %&gt;% \n    pivot_longer(cols=RBC:MCH, names_to = \"bld.ind\") %&gt;% \n    mutate(\n        bld.ind = factor(bld.ind, levels = c(\"RBC\", \"HGB\", \"HCT\", \"MCV\", \"MCH\"))\n        ) %&gt;% \n    arrange(bld.ind) %&gt;% \n    bind_cols(preds) %&gt;% \n    ggplot(aes(x = value)) + \n    geom_point(aes(y = lg.fer)) +\n    geom_smooth(aes(y = lg.fer), se=T, method = \"lm\", formula = y~x) +\n    geom_line(aes(y = upr), col = \"red\", linetype = \"dashed\") +\n    geom_line(aes(y = lwr), col = \"red\", linetype = \"dashed\") +\n    facet_wrap(vars(bld.ind), nrow = 2, scales = \"free\") +\n    labs(\n        title = \"Blood indices with prediction lines (red), regression line (blue) and regression error\",\n        y = \"Log of serum ferritin concentration\",\n        x = NULL)\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.13 Follow-up plot - Highly Customised\n\n\nCode\ndataF %&gt;% \n    select(id, contains(\"mcv\")) %&gt;% \n    arrange() %&gt;% \n    pivot_longer(cols = c(mcv1:mcv5)) %&gt;% \n    mutate(tms = unclass(factor(name))) %&gt;% \n    ggplot(aes(x = tms, y = value, group = id, color = avemcv)) +\n    geom_line() +\n    labs(x = NULL, \n         title = \"Variations Of MCV Over The Five Review Periods\") +\n    scale_x_continuous(breaks = c(1:5), \n                       limits = c(1,5),\n                       labels = c(\"First\",\"Second\", \"Third\", \"Fourth\", \"Fifth\"))+\n    scale_y_continuous(name= \"Mean Corposcular Hemoglobin (fl) Measurement\",\n                       breaks = seq(50, 140, 10)) +\n    scale_color_viridis_c(breaks = seq(50, 140, 10)) +\n    theme(\n        plot.background = element_rect(fill = \"black\", colour = \"black\"),\n        panel.background = element_rect(fill = \"black\", color = \"grey\"),\n        panel.grid = element_blank(),\n        axis.text = element_text(colour = \"grey\", face = \"bold\", family = \"serif\"),\n        axis.ticks = element_line(colour = \"grey\"),\n        axis.title = element_text(colour = \"grey\", face = \"bold\", family = \"serif\"),\n        plot.title = element_text(colour = \"grey\",hjust = 0.5, face = \"bold\", family = \"serif\"),\n        legend.background = element_rect(fill = \"black\", colour = \"grey\"),\n        legend.title = element_blank(),\n        legend.text = element_text(colour = \"grey\", face = \"bold\", family = \"serif\"),\n        legend.key.height = unit(0.64, \"in\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.14 Barchart with CIs\n\n\nCode\ndataF %&gt;% \n    mutate(avehb_cat = case_when(avehb &lt; median(avehb) ~ \"Low HB\",\n                                 avehb &gt;= median(avehb) ~ \"High HB\") %&gt;% \n               factor(levels = c(\"Low HB\", \"High HB\"))) %&gt;% \n    select(starts_with(\"mcv\"), avehb_cat) %&gt;% \n    pivot_longer(cols = c(mcv1:mcv5), values_to = \"mcv\", names_to = \"measure\") %&gt;% \n    group_by(avehb_cat, measure) %&gt;% \n    mutate(mean_mcv = mean(mcv), \n           low_mcv = mean_mcv - 1.96*sd(mcv)/sqrt(n()),\n           high_mcv = mean_mcv + 1.96*sd(mcv)/sqrt(n())) %&gt;% \n    ggplot(aes(y = mean_mcv, x = measure, fill = avehb_cat)) +\n    geom_bar(stat = \"identity\", position = position_dodge(0.9)) +\n    geom_errorbar(aes(ymin = low_mcv, ymax = high_mcv), size = 0.8, \n                  width = 0.3, \n                  position = position_dodge(0.9)) +\n    labs(y = \"Mean Corpuscular Volume\",\n         x = NULL,\n         title = \"Variation in MCV per review period (95%CI)\",\n         caption = \"Source: Data One\")+\n    scale_fill_manual(name = \"HGB Status\", values = c(\"grey\", \"grey45\"))+\n    scale_x_discrete(labels = toupper(c(\"First\", \"Second\", \"Third\", \"Fourth\", \"Fifth\")))+\n    scale_y_continuous(limits = c(0,130), breaks = seq(0, 130, 10))+\n    theme(\n        panel.background = element_rect(colour = \"black\", fill = \"white\"),\n        plot.background  = element_rect(fill = \"grey\"),\n        plot.title = element_text(face = \"bold\", hjust = 0.5, family = \"serif\"),\n        axis.text = element_text(face = \"bold\", family = \"serif\"),\n        axis.title = element_text(family = \"serif\", face = \"bold\"),\n        plot.caption = element_text(family = \"serif\", face = \"bold\"),\n        legend.text = element_text(family = \"serif\", face = \"bold\"),\n        legend.title = element_text(family = \"serif\", face = \"bold\")\n        )\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.15 Lollipop Plot\n\n\nCode\ndataF %&gt;% \n    summarize(across(contains(c(\"mcv\",\"hb\", \"wbc\", \"mch\")),mean)) %&gt;% \n    pivot_longer(cols = mcv1:avemchc) %&gt;% \n    filter(!(name %in% c(\"avemcv\",\"avehb\", \"avewbc\", \"avemch\", \"avemchc\"))) %&gt;%\n    mutate(name2 = str_extract(name, \"^\\\\D*\"),\n           name = toupper(name)) %&gt;% \n    ggplot(aes(x = name, y = value, color = name2))+\n    geom_segment(aes(xend = name, yend = 0), show.legend = F) +\n    geom_point(size = 6, show.legend = F) +\n    geom_text(aes(label = round(value, 1)), col = \"black\", size =2)+\n    labs(title = \"Blood indices variability for each review period\",\n         y = \"Value\",\n         x = NULL)+\n    theme_light() +\n    theme(\n        axis.text.x = element_text(angle = 90),\n        plot.title  = element_text(hjust = 0.5, face = \"bold\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n64.0.0.16 Plots from the ggstatsplot package\n\n\nCode\ndataF %&gt;% \n    select(hb1, fpreg) %&gt;% \n    mutate(hb1 = round(hb1, 1)) %&gt;% \n    na.omit() %&gt;% \n    ggbetweenstats(\n      y=hb1, x=fpreg, \n      ggtheme = theme_light(),\n      bf.prior = F, \n      xlab = \"First Pregnancy\",\n      ylab = \"Hemoglobin\",\n      title = \"Relationship between first pregnancy and hemoglobin\",\n      outlier.tagging = T,\n      outlier.color = \"red\",\n      outlier.shape = 18)\n\n\nError in integrate(meta.t.like, lower = (lower - mean.delta)/scale.delta,  : \n  non-finite function value\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    ggscatterstats(x = mcv1, y = mcv2, ggtheme = theme_light())\n\n\nRegistered S3 method overwritten by 'ggside':\n  method from  \n  +.gg   GGally\n\n\n`stat_xsidebin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_ysidebin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    ggbarstats(y = educ, x = fpreg, \n               ggtheme = theme_light(),\n               legend.title = \"First \\nPregnancy\",\n               xlab = \"Educational Level\",\n               bf.message = F,\n               title = \"Relationship between educational level and first pregnancy\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n  gghistostats(\n    x = hb1,\n    ggtheme = theme_classic(), \n    normal.curve = T, \n    binwidth = 1,\n    normal.curve.args = list(size = 1, col = \"red\"),\n    bin.args = list(color = \"black\", fill = \"blue\", alpha = 0.1),\n    xlab = \"Hemoglobin\", \n    title = \"Distribution of First Hemoglobins\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm(hb1 ~ hb2 + hb3 + hb4 + hb5, data=dataF) %&gt;% \n    ggcoefstats(output = \"plot\",\n                exclude.intercept = T, \n                ggtheme = theme_light(), \n                color = \"red\") +\n    labs(y = \"Covariates\", \n         x = \"Estimates\", \n         title = \"Distribution of coefficient estimates (95% CI)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndataF %&gt;% \n    select(hb1, hb1, hb3, hb4, hb5, mcv1, mcv2, mcv3, mcv4, mcv5) %&gt;% \n    ggcorrmat(colors = c(\"red\",\"white\",\"blue\"), \n              ggtheme = theme_bw(),\n              matrix.type = \"lower\")\n\n\n\n\n\n\n\n\n\n\n\n65 Plots with Dates\n\n\nCode\ndataF %&gt;%\n    mutate(mari = fct_collapse(mari, \n                             \"Married\" = c(\"Married\",\"Cohabiting\"),\n                             \"Single\" = c(\"Widowed\", \"Divorced\"))) %&gt;% \n    ggplot()+\n    geom_point(aes(x = avehb, y = avehct, color = mari), show.legend = F) +\n    geom_smooth(aes(x = avehb, y = avehct), se=F, formula = y~x, \n                method = \"lm\", size = 1, alpha = .5, col = \"grey\")+\n    facet_wrap(~mari, nrow = 2, strip.position = \"left\") +\n    labs(y = \"Hematocrit (%)\", x = \"Hemoglobin (g/dl)\", \n         title = str_glue(\"Relationship between Blood hemoglobin and \",\n                          \"Hematocrit stratified by marital status\"))+\n    theme(panel.background = element_blank(),\n          panel.grid = element_blank(),\n          axis.line = element_line(),\n          strip.placement = \"outside\",\n          strip.background = element_rect(fill = \"#c1d3fe\", color = \"black\"),\n          strip.text = element_text(size = 10, face = \"bold\"),\n          text = element_text(family = \"serif\"),\n          axis.text = element_text(size = 10, face = \"bold\"),\n          axis.title = element_text(size = 10, face = \"bold.italic\"),\n          plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n65.0.0.1 Drawing free lines with legend\n\n\nCode\ndataF %&gt;% \n    mutate(hct3 = ifelse(hct3 &lt; 20, hct3 +40, hct3),\n           hct3 = ifelse(hct3 &gt; 60, hct3 - 20, hct3)) %&gt;% \n    ggplot(aes(x = hct3, y = hb3)) +\n    geom_point(color = \"grey45\") +\n    geom_smooth(aes(x = hct3, y = hb3, col =  \"Observed\"), \n                formula = y~x, method = \"lm\", se = F) + \n    geom_segment(aes(x = min(hct3), y = min(hct3/3), \n                     xend = max(hct3), yend = max(hct3/3), \n                     col =  \"Expected\"))+\n    labs(title = \"Relationship between the third HB and HCT measurements\",\n         subtitle = \"Comparison of observed  and expected regression line if HCT = 3*HB\",\n         x = \"Hematocrit (%)\", y = \"Hemoglobin (mg/dl)\", \n         color = \"Regression Line\") +\n    theme_classic()+\n    theme(plot.title = element_text(face = \"bold\"),\n          plot.subtitle = element_text(face = \"italic\"))\n\n\nWarning in geom_segment(aes(x = min(hct3), y = min(hct3/3), xend = max(hct3), : All aesthetics have length 1, but the data has 350 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "<span style='font-weight:bold; color: black;'>Graphics</span>",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Miscellaneous</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wang, Xiaofeng, and Xinge Ji. 2020. “Sample Size Estimation in\nClinical Research.” Chest 158 (1): S12–20. https://doi.org/10.1016/j.chest.2020.03.010.",
    "crumbs": [
      "References"
    ]
  }
]